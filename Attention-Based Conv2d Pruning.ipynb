{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 设置随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as fn\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现 Attention-Based Conv2d Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.1258e+00, -1.1524e+00, -2.5058e-01],\n",
       "           [-4.3388e-01,  8.4871e-01,  6.9201e-01],\n",
       "           [-3.1601e-01, -2.1152e+00,  3.2227e-01]],\n",
       " \n",
       "          [[-1.2633e+00,  3.4998e-01,  3.0813e-01],\n",
       "           [ 1.1984e-01,  1.2377e+00,  1.1168e+00],\n",
       "           [-2.4728e-01, -1.3527e+00, -1.6959e+00]],\n",
       " \n",
       "          [[ 5.6665e-01,  7.9351e-01,  5.9884e-01],\n",
       "           [-1.5551e+00, -3.4136e-01,  1.8530e+00],\n",
       "           [ 7.5019e-01, -5.8550e-01, -1.7340e-01]],\n",
       " \n",
       "          [[ 1.8348e-01,  1.3894e+00,  1.5863e+00],\n",
       "           [ 9.4630e-01, -8.4368e-01, -6.1358e-01],\n",
       "           [ 3.1593e-02, -4.9268e-01,  2.4841e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 4.3970e-01,  1.1241e-01,  6.4079e-01],\n",
       "           [ 4.4116e-01, -1.0231e-01,  7.9244e-01],\n",
       "           [-2.8967e-01,  5.2507e-02,  5.2286e-01]],\n",
       " \n",
       "          [[ 2.3022e+00, -1.4689e+00, -1.5867e+00],\n",
       "           [-6.7309e-01,  8.7283e-01,  1.0554e+00],\n",
       "           [ 1.7784e-01, -2.3034e-01, -3.9175e-01]],\n",
       " \n",
       "          [[ 5.4329e-01, -3.9516e-01, -4.4622e-01],\n",
       "           [ 7.4402e-01,  1.5210e+00,  3.4105e+00],\n",
       "           [-1.5312e+00, -1.2341e+00,  1.8197e+00]],\n",
       " \n",
       "          [[-5.5153e-01, -5.6925e-01,  9.1997e-01],\n",
       "           [ 1.1108e+00,  1.2899e+00, -1.4782e+00],\n",
       "           [ 2.5672e+00, -4.7312e-01,  3.3555e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.6293e+00, -5.4974e-01, -4.7983e-01],\n",
       "           [-4.9968e-01, -1.0670e+00,  1.1149e+00],\n",
       "           [-1.4067e-01,  8.0575e-01, -9.3348e-02]],\n",
       " \n",
       "          [[ 6.8705e-01, -8.3832e-01,  8.9182e-04],\n",
       "           [ 8.4189e-01, -4.0003e-01,  1.0395e+00],\n",
       "           [ 3.5815e-01, -2.4600e-01,  2.3025e+00]],\n",
       " \n",
       "          [[-1.8817e+00, -4.9727e-02, -1.0450e+00],\n",
       "           [-9.5650e-01,  3.3532e-02,  7.1009e-01],\n",
       "           [ 1.6459e+00, -1.3602e+00,  3.4457e-01]],\n",
       " \n",
       "          [[ 5.1987e-01, -2.6133e+00, -1.6965e+00],\n",
       "           [-2.2824e-01,  2.7995e-01,  2.4693e-01],\n",
       "           [ 7.6887e-02,  3.3801e-01,  4.5440e-01]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 9.9122e-01,  4.0964e-01,  1.0504e+00],\n",
       "           [-6.8717e-01, -9.5386e-01,  7.2710e-01],\n",
       "           [-7.6824e-01, -1.3224e+00,  4.8052e-01]],\n",
       " \n",
       "          [[-4.3049e-01,  3.6525e-02,  1.2693e-01],\n",
       "           [ 5.9660e-01, -2.3058e-01,  4.3881e-01],\n",
       "           [ 3.8105e-01,  1.7162e+00, -3.9212e-03]],\n",
       " \n",
       "          [[-2.0564e+00, -1.5919e+00, -2.4523e-01],\n",
       "           [ 3.0279e-01,  1.1842e+00,  6.7116e-01],\n",
       "           [-1.0036e+00,  4.7560e-01,  8.3765e-01]],\n",
       " \n",
       "          [[ 8.3736e-01, -7.9423e-01, -3.6217e-01],\n",
       "           [ 2.1609e+00,  3.1477e-01,  3.0974e+00],\n",
       "           [ 1.2121e-02,  8.0322e-01, -6.9618e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.0645e+00,  2.3837e-01,  1.3249e-02],\n",
       "           [-4.2369e-01, -9.2583e-02, -8.5188e-01],\n",
       "           [-5.4269e-02, -8.0253e-01, -1.1104e+00]],\n",
       " \n",
       "          [[ 2.1508e-01, -3.5944e-01,  5.7850e-01],\n",
       "           [ 7.8056e-01,  1.2963e+00, -7.9423e-01],\n",
       "           [-1.1428e+00,  4.9265e-02,  4.1032e-01]],\n",
       " \n",
       "          [[ 3.0642e-02, -6.2849e-02, -5.6990e-01],\n",
       "           [-1.2050e+00,  2.5428e-01, -6.3739e-02],\n",
       "           [ 1.8938e-01,  8.2531e-01, -7.0136e-01]],\n",
       " \n",
       "          [[ 7.8780e-01, -1.1025e+00,  1.2695e-01],\n",
       "           [ 1.8245e+00, -1.4463e-01, -1.7125e+00],\n",
       "           [ 3.6174e-01, -4.6252e-01, -2.1531e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 5.3696e-01,  5.8164e-01, -1.3250e+00],\n",
       "           [-1.3147e+00, -5.2480e-01, -1.3042e+00],\n",
       "           [-1.0938e+00, -1.7034e+00, -6.7298e-01]],\n",
       " \n",
       "          [[ 5.0530e-01,  1.4977e+00, -5.4544e-01],\n",
       "           [-1.3346e+00,  4.7449e-01,  4.8438e-01],\n",
       "           [ 4.3440e-01, -7.3347e-01,  4.5299e-01]],\n",
       " \n",
       "          [[ 3.2461e-01, -1.3075e+00, -6.4060e-01],\n",
       "           [-4.5010e-01,  7.7285e-01,  1.2818e+00],\n",
       "           [-4.8171e-01,  1.2247e+00, -4.3755e-01]],\n",
       " \n",
       "          [[ 3.7199e-01, -3.2037e-01, -1.0110e+00],\n",
       "           [-1.1993e+00,  2.1330e-01,  1.7954e+00],\n",
       "           [ 4.0927e-01,  1.3281e+00, -1.0375e+00]]]]),\n",
       " tensor([[[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]],\n",
       " \n",
       "          [[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]]]),\n",
       " torch.Size([64, 4, 3, 3]),\n",
       " torch.Size([64, 4, 3, 3]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(64, 4, 3, 3)\n",
    "z = torch.ones(64, 4, 3, 3)\n",
    "x, z, x.shape, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 将 Weight data 类型转换和求绝对值\n",
    "- x [N<sub>in</sub>, N<sub>out</sub>, kernel_size[0], kernel_size[1]]\n",
    "- A [N<sub>in</sub>, N<sub>out</sub>, kernel_size[0] * kernel_size[1]]\n",
    "- C = N<sub>in</sub>, H =  N<sub>out</sub>, W = kernel_size[0] * kernel_size[1]\n",
    "- A [C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.1258e+00, 1.1524e+00, 2.5058e-01,  ..., 3.1601e-01,\n",
       "           2.1152e+00, 3.2227e-01],\n",
       "          [1.2633e+00, 3.4998e-01, 3.0813e-01,  ..., 2.4728e-01,\n",
       "           1.3527e+00, 1.6959e+00],\n",
       "          [5.6665e-01, 7.9351e-01, 5.9884e-01,  ..., 7.5019e-01,\n",
       "           5.8550e-01, 1.7340e-01],\n",
       "          [1.8348e-01, 1.3894e+00, 1.5863e+00,  ..., 3.1593e-02,\n",
       "           4.9268e-01, 2.4841e-01]],\n",
       " \n",
       "         [[4.3970e-01, 1.1241e-01, 6.4079e-01,  ..., 2.8967e-01,\n",
       "           5.2507e-02, 5.2286e-01],\n",
       "          [2.3022e+00, 1.4689e+00, 1.5867e+00,  ..., 1.7784e-01,\n",
       "           2.3034e-01, 3.9175e-01],\n",
       "          [5.4329e-01, 3.9516e-01, 4.4622e-01,  ..., 1.5312e+00,\n",
       "           1.2341e+00, 1.8197e+00],\n",
       "          [5.5153e-01, 5.6925e-01, 9.1997e-01,  ..., 2.5672e+00,\n",
       "           4.7312e-01, 3.3555e-01]],\n",
       " \n",
       "         [[1.6293e+00, 5.4974e-01, 4.7983e-01,  ..., 1.4067e-01,\n",
       "           8.0575e-01, 9.3348e-02],\n",
       "          [6.8705e-01, 8.3832e-01, 8.9182e-04,  ..., 3.5815e-01,\n",
       "           2.4600e-01, 2.3025e+00],\n",
       "          [1.8817e+00, 4.9727e-02, 1.0450e+00,  ..., 1.6459e+00,\n",
       "           1.3602e+00, 3.4457e-01],\n",
       "          [5.1987e-01, 2.6133e+00, 1.6965e+00,  ..., 7.6887e-02,\n",
       "           3.3801e-01, 4.5440e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[9.9122e-01, 4.0964e-01, 1.0504e+00,  ..., 7.6824e-01,\n",
       "           1.3224e+00, 4.8052e-01],\n",
       "          [4.3049e-01, 3.6525e-02, 1.2693e-01,  ..., 3.8105e-01,\n",
       "           1.7162e+00, 3.9212e-03],\n",
       "          [2.0564e+00, 1.5919e+00, 2.4523e-01,  ..., 1.0036e+00,\n",
       "           4.7560e-01, 8.3765e-01],\n",
       "          [8.3736e-01, 7.9423e-01, 3.6217e-01,  ..., 1.2121e-02,\n",
       "           8.0322e-01, 6.9618e-01]],\n",
       " \n",
       "         [[1.0645e+00, 2.3837e-01, 1.3249e-02,  ..., 5.4269e-02,\n",
       "           8.0253e-01, 1.1104e+00],\n",
       "          [2.1508e-01, 3.5944e-01, 5.7850e-01,  ..., 1.1428e+00,\n",
       "           4.9265e-02, 4.1032e-01],\n",
       "          [3.0642e-02, 6.2849e-02, 5.6990e-01,  ..., 1.8938e-01,\n",
       "           8.2531e-01, 7.0136e-01],\n",
       "          [7.8780e-01, 1.1025e+00, 1.2695e-01,  ..., 3.6174e-01,\n",
       "           4.6252e-01, 2.1531e+00]],\n",
       " \n",
       "         [[5.3696e-01, 5.8164e-01, 1.3250e+00,  ..., 1.0938e+00,\n",
       "           1.7034e+00, 6.7298e-01],\n",
       "          [5.0530e-01, 1.4977e+00, 5.4544e-01,  ..., 4.3440e-01,\n",
       "           7.3347e-01, 4.5299e-01],\n",
       "          [3.2461e-01, 1.3075e+00, 6.4060e-01,  ..., 4.8171e-01,\n",
       "           1.2247e+00, 4.3755e-01],\n",
       "          [3.7199e-01, 3.2037e-01, 1.0110e+00,  ..., 4.0927e-01,\n",
       "           1.3281e+00, 1.0375e+00]]]),\n",
       " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " torch.Size([64, 4, 9]),\n",
       " torch.Size([64, 4, 9]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d11, d12, _, _ = x.shape\n",
    "d21, d22, _, _ = z.shape\n",
    "Ax = x.view(d11, d12, -1).abs()\n",
    "Az = z.view(d21, d22, -1).abs()\n",
    "Ax, Az, Ax.shape, Az.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 沿通道方向绝对值之和 \n",
    "F(A)=∑<sub>i=1</sub><sup>C</sup> |A<sub>i</sub>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[56.6199, 40.9421, 53.9351, 48.8748, 46.8690, 59.5128, 39.5230, 53.0775,\n",
       "          52.0177],\n",
       "         [57.4621, 50.7123, 49.4178, 51.8026, 54.0485, 59.0183, 42.7864, 40.8320,\n",
       "          47.9534],\n",
       "         [51.3775, 62.0132, 44.3578, 48.8647, 51.0990, 50.5940, 46.7558, 52.7277,\n",
       "          57.0936],\n",
       "         [58.7794, 51.1150, 49.7080, 53.7365, 50.6284, 46.9924, 57.3430, 52.9584,\n",
       "          49.6174]]),\n",
       " tensor([[64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
       "         [64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
       "         [64., 64., 64., 64., 64., 64., 64., 64., 64.],\n",
       "         [64., 64., 64., 64., 64., 64., 64., 64., 64.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c, h, w = Ax.shape\n",
    "FAx = torch.zeros(h, w)\n",
    "FAz = torch.zeros(h, w)\n",
    "for i in range(d11):\n",
    "    FAx.add_(torch.abs(Ax[i]))\n",
    "    FAz.add_(torch.abs(Az[i]))\n",
    "FAx, FAz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 计算 ||F(A)||<sup>2</sup>  二范数的平方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(308.5361), tensor(384.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FAx_s, FAz_s = torch.linalg.norm(FAx), torch.linalg.norm(FAz)\n",
    "FAx_s, FAz_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 计算 F(A) / ||F(A)||<sup>2</sup> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1835, 0.1327, 0.1748, 0.1584, 0.1519, 0.1929, 0.1281, 0.1720, 0.1686],\n",
       "         [0.1862, 0.1644, 0.1602, 0.1679, 0.1752, 0.1913, 0.1387, 0.1323, 0.1554],\n",
       "         [0.1665, 0.2010, 0.1438, 0.1584, 0.1656, 0.1640, 0.1515, 0.1709, 0.1850],\n",
       "         [0.1905, 0.1657, 0.1611, 0.1742, 0.1641, 0.1523, 0.1859, 0.1716, 0.1608]]),\n",
       " tensor([[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fx, Fz = FAx / FAx_s,  FAz / FAz_s\n",
    "Fx, Fz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 计算 F(A<sub>j</sub>) / ||F(A<sub>j</sub>)||<sup>2</sup>  和 gamma = ∑ | F(A) / ||F(A)||<sup>2</sup> - F(A<sub>j</sub>) / ||F(A<sub>j</sub>)||<sup>2</sup> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1258, 1.1524, 0.2506, 0.4339, 0.8487, 0.6920, 0.3160, 2.1152, 0.3223],\n",
      "        [1.2633, 0.3500, 0.3081, 0.1198, 1.2377, 1.1168, 0.2473, 1.3527, 1.6959],\n",
      "        [0.5667, 0.7935, 0.5988, 1.5551, 0.3414, 1.8530, 0.7502, 0.5855, 0.1734],\n",
      "        [0.1835, 1.3894, 1.5863, 0.9463, 0.8437, 0.6136, 0.0316, 0.4927, 0.2484]]) tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[55.4941, 39.7897, 53.6846, 48.4409, 46.0203, 58.8208, 39.2070, 50.9622,\n",
      "         51.6954],\n",
      "        [56.1988, 50.3623, 49.1097, 51.6828, 52.8109, 57.9015, 42.5391, 39.4794,\n",
      "         46.2575],\n",
      "        [50.8109, 61.2197, 43.7589, 47.3096, 50.7576, 48.7410, 46.0056, 52.1422,\n",
      "         56.9202],\n",
      "        [58.5959, 49.7256, 48.1216, 52.7902, 49.7847, 46.3789, 57.3114, 52.4657,\n",
      "         49.3689]]) tensor([[63., 63., 63., 63., 63., 63., 63., 63., 63.],\n",
      "        [63., 63., 63., 63., 63., 63., 63., 63., 63.],\n",
      "        [63., 63., 63., 63., 63., 63., 63., 63., 63.],\n",
      "        [63., 63., 63., 63., 63., 63., 63., 63., 63.]])\n",
      "tensor(303.8511) tensor(378.)\n",
      "tensor([[0.1826, 0.1310, 0.1767, 0.1594, 0.1515, 0.1936, 0.1290, 0.1677, 0.1701],\n",
      "        [0.1850, 0.1657, 0.1616, 0.1701, 0.1738, 0.1906, 0.1400, 0.1299, 0.1522],\n",
      "        [0.1672, 0.2015, 0.1440, 0.1557, 0.1670, 0.1604, 0.1514, 0.1716, 0.1873],\n",
      "        [0.1928, 0.1637, 0.1584, 0.1737, 0.1638, 0.1526, 0.1886, 0.1727, 0.1625]]) tensor([[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0546), tensor(0.))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ax0, Az0 = Ax[0], Az[0]\n",
    "print(Ax0, Az0)\n",
    "\n",
    "FAx0, FAz0 = FAx - Ax0, FAz - Az0\n",
    "print(FAx0, FAz0)\n",
    "\n",
    "FAx0_s, FAz0_s = torch.linalg.norm(FAx0), torch.linalg.norm(FAz0)\n",
    "print(FAx0_s, FAz0_s)\n",
    "\n",
    "Fx0, Fz0 = FAx0 / FAx0_s, FAz0 / FAz0_s\n",
    "print(Fx0, Fz0)\n",
    "\n",
    "gammax0, gammaz0 = (Fx - Fx0).abs().sum(), (Fz - Fz0).abs().sum()\n",
    "gammax0, gammaz0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0546, 0.0671, 0.0609, 0.0496, 0.0437, 0.0406, 0.0526, 0.0524, 0.0428,\n",
       "         0.0674, 0.0513, 0.0571, 0.0700, 0.0606, 0.0682, 0.0583, 0.0720, 0.0541,\n",
       "         0.0582, 0.0520, 0.0550, 0.0501, 0.0600, 0.0514, 0.0582, 0.0712, 0.0583,\n",
       "         0.0555, 0.0482, 0.0614, 0.0545, 0.0447, 0.0492, 0.0716, 0.0651, 0.0625,\n",
       "         0.0574, 0.0620, 0.0560, 0.0640, 0.0508, 0.0675, 0.0500, 0.0625, 0.0535,\n",
       "         0.0604, 0.0685, 0.0520, 0.0569, 0.0577, 0.0527, 0.0724, 0.0488, 0.0514,\n",
       "         0.0493, 0.0551, 0.0405, 0.0504, 0.0475, 0.0516, 0.0435, 0.0553, 0.0502,\n",
       "         0.0469]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammax_list, gammaz_list = torch.zeros(c), torch.zeros(c)\n",
    "for j in range(c):\n",
    "    Axj, Azj = Ax[j], Az[j]\n",
    "    FAxj, FAzj = FAx - Axj, FAz - Azj\n",
    "    FAxj_s, FAzj_s = torch.linalg.norm(FAxj), torch.linalg.norm(FAzj)\n",
    "    Fxj, Fzj = FAxj / FAxj_s, FAzj / FAzj_s\n",
    "    gammmax, gammmaz = (Fx - Fxj).abs().sum(), (Fz - Fzj).abs().sum()\n",
    "    gammax_list[j] = gammmax\n",
    "    gammaz_list[j] = gammmaz\n",
    "\n",
    "gammax_list, gammaz_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0724),\n",
       " tensor(51),\n",
       " tensor([[0.2082, 0.4852, 0.0437, 0.1160, 0.7978, 0.2425, 0.4761, 0.3796, 0.0114],\n",
       "         [0.4512, 1.0632, 0.2497, 0.0545, 0.8129, 0.9576, 1.2139, 0.5725, 0.0793],\n",
       "         [1.1229, 1.4157, 0.2296, 0.1575, 1.1857, 1.2910, 0.0035, 0.7711, 2.6715],\n",
       "         [1.9461, 2.2148, 1.6746, 0.2388, 1.9177, 0.6798, 2.5549, 1.3455, 1.9527]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortx, sortx_index = torch.sort(gammax_list, descending=True)\n",
    "sortz, sorty_index = torch.sort(gammaz_list, descending=True)\n",
    "maxx, maxx_index = sortx[0], sortx_index[0]\n",
    "maxx, maxx_index, Ax[maxx_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 测试 `linalg.norm` 与 `nn.functional.normalize` 的区别\n",
    "- 二范数：( |x<sub>1</sub>|<sup>2</sup> + |x<sub>2</sub>|<sup>2</sup> +...+|x<sub>n</sub>|<sup>2</sup>)<sup>1/2</sup>\n",
    "- `torch.norm` 已废弃，不建议使用\n",
    "- [linalg.norm](https://pytorch.org/docs/stable/linalg.html?highlight=norm#torch.linalg.norm) 在计算矩阵二范数时，没有开平方根\n",
    "- [nn.functional.normalize](https://pytorch.org/docs/stable/nn.functional.html?highlight=normalize#torch.nn.functional.normalize) 在计算矩阵二范数时，开了平方根\n",
    "- 即 `torch.sqrt(torch.linalg.norm(data))` 等价于 `fn.normalize(data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = torch.ones(2, 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.1 linalg.norm without square root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "norm_data = torch.linalg.norm(data)\n",
    "result1 = data / norm_data\n",
    "norm_data, result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2  linalg.norm square root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = torch.sqrt(torch.linalg.norm(data))  # 增加开根号\n",
    "result2 = data / norm_data\n",
    "norm_data, result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 manual functional.normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denom = data.norm(2, 1, False).clamp_min(1e-12)\n",
    "ret = data / denom\n",
    "denom, ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 auto functional.normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret2 = fn.normalize(result_z)\n",
    "ret2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
