{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Resnet: modules & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy\n",
    "\n",
    "class channel_selection(nn.Module):\n",
    "    \"\"\"\n",
    "    Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer.\n",
    "    The output shape of this layer is determined by the number of 1 in `self.indexes`.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels):\n",
    "        \"\"\"\n",
    "        Initialize the `indexes` with all one vector with the length same as the number of channels.\n",
    "        During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0.\n",
    "        \"\"\"\n",
    "        super(channel_selection, self).__init__()\n",
    "        self.indexes = nn.Parameter(torch.ones(num_channels))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "        ---------\n",
    "        input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer.\n",
    "        \"\"\"\n",
    "        selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy()))\n",
    "        if selected_index.size == 1:\n",
    "            selected_index = np.resize(selected_index, (1,)) \n",
    "        output = input_tensor[:, selected_index, :, :]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "preactivation resnet with bottleneck design.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, cfg, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.select = channel_selection(inplanes)\n",
    "        self.conv1 = nn.Conv2d(cfg[0], cfg[1], kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(cfg[1])\n",
    "        self.conv2 = nn.Conv2d(cfg[1], cfg[2], kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(cfg[2])\n",
    "        self.conv3 = nn.Conv2d(cfg[2], planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        # group1\n",
    "        out = self.bn1(x)\n",
    "        out = self.select(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        # group2\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # group3\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        # down sample\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "    def forward_bn(self, x):\n",
    "        bn_value = []\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        bn_value.append(out.clone())\n",
    "        out = self.select(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        bn_value.append(out.clone())\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn3(out)\n",
    "        bn_value.append(out.clone())\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        return out, bn_value\n",
    "\n",
    "    def mask_bn(self, index, cfg_mask):\n",
    "        if index == 0:\n",
    "            self.bn1.weight.data.mul_(cfg_mask)\n",
    "            self.bn1.bias.data.mul_(cfg_mask)\n",
    "        elif index == 1:\n",
    "            self.bn2.weight.data.mul_(cfg_mask)\n",
    "            self.bn2.bias.data.mul_(cfg_mask)\n",
    "        elif index == 2:\n",
    "            self.bn3.weight.data.mul_(cfg_mask)\n",
    "            self.bn3.bias.data.mul_(cfg_mask)\n",
    "        else:\n",
    "            raise ValueError(\"Index is not including.\")\n",
    "\n",
    "\n",
    "class resnet(nn.Module):\n",
    "    def __init__(self, depth=164, dataset='cifar10', cfg=None, conv_cfg=None):\n",
    "        \"\"\"\n",
    "        :param depth:\n",
    "            164 layers => 1 conv2d + 3 layers × 18 blocks (every layer)  × 3 conv2ds (every block)  + 1 avgPool2d\n",
    "            param n = (depth - 2) // 9:\n",
    "                n means how many blocks in every layer\n",
    "                9 = 3 layers × 3 conv2d (every block)\n",
    "        :param cfg:\n",
    "            if depth = 164, then len(cfg) = 164\n",
    "        :param conv_cfg:\n",
    "            layer block index examples (index starts at 1 & ≤ 18):\n",
    "                3 indexes / layer: [4, 9, 14] or [6, 12, 18]\n",
    "                2 indexes / layer: [9, 18]\n",
    "                1 index / layer: [18]\n",
    "\n",
    "        number of BatchNorm2d:\n",
    "            163 = 162 (3 layers × 18 Bottlenecks × 3 BatchNorm2ds) + 1 BatchNorm2d\n",
    "        \"\"\"\n",
    "        super(resnet, self).__init__()\n",
    "        assert (depth - 2) % 9 == 0, 'depth should be 9n+2'\n",
    "\n",
    "        n = (depth - 2) // 9  # depth = 164, n = 18\n",
    "        block = Bottleneck\n",
    "        self.block_cfg = conv_cfg\n",
    "        self.inplanes = 16\n",
    "\n",
    "        # model config\n",
    "        if cfg is None:\n",
    "            cfg = [[16, 16, 16], [64, 16, 16] * (n - 1),\n",
    "                   [64, 32, 32], [128, 32, 32] * (n - 1),\n",
    "                   [128, 64, 64], [256, 64, 64] * (n - 1), [256]]\n",
    "            cfg = [item for sub_list in cfg for item in sub_list]\n",
    "\n",
    "        # model feature\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 16, n, cfg=cfg[0: 3 * n])\n",
    "        self.layer2 = self._make_layer(block, 32, n, cfg=cfg[3 * n: 6 * n], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, n, cfg=cfg[6 * n:9 * n], stride=2)\n",
    "        self.bn = nn.BatchNorm2d(64 * block.expansion)\n",
    "        self.select = channel_selection(64 * block.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "\n",
    "        # model classifier\n",
    "        if dataset == 'cifar10':\n",
    "            num_classes = 10\n",
    "        elif dataset == 'cifar100':\n",
    "            num_classes = 100\n",
    "        else:\n",
    "            raise ValueError('Model `dataset` parameter is Error!')\n",
    "        self.fc = nn.Linear(cfg[-1], num_classes)\n",
    "\n",
    "        # model initialize weight\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(0.5)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, cfg, stride=1):\n",
    "        \"\"\"\n",
    "        :param block: Bottleneck item\n",
    "        :param planes: record the layer's output channel size\n",
    "                        planes * expansion = output_planes_size,\n",
    "                        like 16 * 4 = 128 (the first block output)\n",
    "        :param blocks: number of Bottleneck in layer\n",
    "        :param cfg:  channel config of all blocks\n",
    "                        3 cfg items / Bottleneck\n",
    "        :param stride: default = 1\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False))\n",
    "        layers = [block(self.inplanes, planes, cfg[0:3], stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, cfg[3 * i: 3 * (i + 1)]))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        block_value = []\n",
    "        if self.block_cfg:\n",
    "            for idx, item in enumerate(self.layer1):\n",
    "                x = item(x)\n",
    "                if idx + 1 in self.block_cfg:\n",
    "                    block_value.append(x)\n",
    "            for idx, item in enumerate(self.layer2):\n",
    "                x = item(x)\n",
    "                if idx + 1 in self.block_cfg:\n",
    "                    block_value.append(x)\n",
    "            for idx, item in enumerate(self.layer3):\n",
    "                x = item(x)\n",
    "                if idx + 1 in self.block_cfg:\n",
    "                    block_value.append(x)\n",
    "        else:\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.select(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y = self.fc(x)\n",
    "\n",
    "        if len(block_value):\n",
    "            return y, block_value\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'D:\\Project\\Gitee\\network-slimming\\logs\\ft_at_resnet164_cifar100_percent_0.6_seed_2\\model_best.pt'\n",
      "-> model cfg is loading...\n",
      " cfg: [5, 8, 13, 8, 9, 11, 13, 12, 15, 9, 10, 10, 9, 13, 13, 10, 14, 14, 9, 11, 11, 14, 14, 14, 8, 10, 12, 13, 12, 10, 9, 10, 12, 10, 11, 16, 2, 5, 7, 5, 8, 13, 11, 12, 13, 11, 12, 16, 2, 6, 8, 6, 10, 15, 7, 10, 29, 21, 26, 28, 24, 25, 24, 17, 25, 26, 26, 25, 25, 26, 27, 20, 22, 22, 28, 25, 27, 29, 28, 30, 23, 23, 25, 29, 26, 22, 27, 19, 28, 27, 24, 27, 30, 25, 27, 27, 27, 24, 28, 27, 26, 29, 26, 27, 28, 30, 22, 27, 10, 25, 64, 54, 55, 60, 54, 59, 54, 42, 58, 62, 56, 57, 61, 49, 56, 62, 45, 58, 62, 50, 56, 60, 51, 60, 62, 38, 60, 62, 44, 59, 64, 34, 60, 63, 39, 61, 62, 42, 61, 61, 40, 58, 62, 41, 60, 63, 42, 58, 60, 47, 60, 61, 24]\n",
      "=>  epoch 129 Prec1: 0.745\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resume_model(resume_file):\n",
    "    if not os.path.isfile(resume_file):\n",
    "        raise ValueError(\"Resume model file is not found at '{}'\".format(resume_file))\n",
    "    print(\"=> loading checkpoint '{}'\".format(resume_file))\n",
    "    checkpoint = torch.load(resume_file)\n",
    "    if 'epoch' in checkpoint:\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        start_epoch = None\n",
    "        \n",
    "    if 'best_prec1' in checkpoint:\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "    else:\n",
    "        best_prec1 = None\n",
    "        \n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = None\n",
    "    \n",
    "    if 'optimizer' in checkpoint:\n",
    "        opti_dict = checkpoint['optimizer']\n",
    "    else:\n",
    "        opti_dict = None\n",
    "        \n",
    "    if 'cfg' in checkpoint:\n",
    "        cfg = checkpoint['cfg']\n",
    "        print(\"-> model cfg is loading...\\n cfg: {}\".format(list(cfg)))\n",
    "    else:\n",
    "        cfg = None\n",
    "        print(\"-> not found model cfg...\")\n",
    "    print(\"=>  epoch {} Prec1: {}\".format(start_epoch, best_prec1))\n",
    "    return state_dict, opti_dict, start_epoch, best_prec1, cfg\n",
    "\n",
    "# root1\n",
    "root_path = r'D:\\Project\\Pycharm\\network-slimming\\logs'\n",
    "file_name = 'model_best.pth.tar'\n",
    "name = [\n",
    "    'ft_inherit_bn_resnet164_cifar100_percent_0.4_seed_2', \n",
    "    'ori_sparsity_resnet164_cifar100_s_1e_5'\n",
    "]\n",
    "file_path = os.path.join(root_path, name[1], file_name)\n",
    "\n",
    "# root2\n",
    "root_path2 = r'D:\\Project\\Gitee\\network-slimming\\logs'\n",
    "file_name2 = 'model_best.pt'\n",
    "pruned_name = 'pruned.pth.tar'\n",
    "name2 = [\n",
    "    'ft_at_resnet164_cifar100_percent_0.6_seed_2',\n",
    "    'bn_prune_resnet164_cifar100_percent_0.6',\n",
    "    'at_prune_resnet164_cifar100_percent_0.6'\n",
    "]\n",
    "file_path2 = os.path.join(root_path2, name2[0], file_name2)\n",
    "\n",
    "\n",
    "state_dict, opti_dict, start_epoch, best_prec1, cfg = resume_model(file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resnet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(5, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(8, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(8, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(11, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(13, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(15, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(9, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(10, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(9, 13, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(10, 14, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(14, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(9, 11, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(11, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(14, 14, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(14, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(8, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(13, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(10, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(9, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(10, 11, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(2, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(7, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(5, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(8, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(11, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(12, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(11, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(2, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(6, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(15, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(7, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(10, 29, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(29, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(21, 26, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(26, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(28, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(24, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(24, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(17, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(25, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(26, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(26, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(25, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(26, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(27, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(20, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(22, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(22, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(28, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(25, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(27, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(29, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(28, 30, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(30, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(23, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(23, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(25, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(29, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(26, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(22, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(27, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(19, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(28, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(27, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(24, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(27, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(30, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(25, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(27, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(27, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(24, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(28, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(27, 26, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(26, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(29, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(26, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(27, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(28, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(30, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(22, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(27, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(10, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(25, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(54, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(55, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(60, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(54, 59, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(59, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(54, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(42, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(62, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(56, 57, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(57, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(61, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(49, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(62, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(45, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(62, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(50, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(56, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(60, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(51, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(62, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(38, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(62, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(44, 59, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(34, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(60, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(63, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(39, 61, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(62, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(42, 61, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(61, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(40, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(62, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(41, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(60, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(63, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(42, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(58, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(60, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (select): channel_selection()\n",
       "      (conv1): Conv2d(47, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(60, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(61, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (select): channel_selection()\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=24, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet(depth=164, dataset='cifar100', cfg=cfg, conv_cfg=None)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "[MAdd]: channel_selection is not supported!\n",
      "[Flops]: channel_selection is not supported!\n",
      "[Memory]: channel_selection is not supported!\n",
      "                 module name  input shape output shape     params memory(MB)           MAdd          Flops  MemRead(B)  MemWrite(B) duration[%]   MemR+W(B)\n",
      "0                      conv1    3  32  32   16  32  32      432.0       0.06      868,352.0      442,368.0     14016.0      65536.0       0.00%     79552.0\n",
      "1               layer1.0.bn1   16  32  32   16  32  32       32.0       0.06       65,536.0       32,768.0     65664.0      65536.0       0.00%    131200.0\n",
      "2            layer1.0.select   16  32  32    5  32  32       16.0       0.02            0.0            0.0         0.0          0.0       2.25%         0.0\n",
      "3             layer1.0.conv1    5  32  32    8  32  32       40.0       0.03       73,728.0       40,960.0     20640.0      32768.0       0.00%     53408.0\n",
      "4               layer1.0.bn2    8  32  32    8  32  32       16.0       0.03       32,768.0       16,384.0     32832.0      32768.0       0.00%     65600.0\n",
      "5             layer1.0.conv2    8  32  32   13  32  32      936.0       0.05    1,903,616.0      958,464.0     36512.0      53248.0       0.00%     89760.0\n",
      "6               layer1.0.bn3   13  32  32   13  32  32       26.0       0.05       53,248.0       26,624.0     53352.0      53248.0       0.00%    106600.0\n",
      "7             layer1.0.conv3   13  32  32   64  32  32      832.0       0.25    1,638,400.0      851,968.0     56576.0     262144.0       0.00%    318720.0\n",
      "8              layer1.0.relu   13  32  32   13  32  32        0.0       0.05       13,312.0       13,312.0     53248.0      53248.0       0.00%    106496.0\n",
      "9      layer1.0.downsample.0   16  32  32   64  32  32     1024.0       0.25    2,031,616.0    1,048,576.0     69632.0     262144.0       0.00%    331776.0\n",
      "10              layer1.1.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       2.25%    524800.0\n",
      "11           layer1.1.select   64  32  32    8  32  32       64.0       0.03            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "12            layer1.1.conv1    8  32  32    9  32  32       72.0       0.04      138,240.0       73,728.0     33056.0      36864.0       0.00%     69920.0\n",
      "13              layer1.1.bn2    9  32  32    9  32  32       18.0       0.04       36,864.0       18,432.0     36936.0      36864.0       0.00%     73800.0\n",
      "14            layer1.1.conv2    9  32  32   11  32  32      891.0       0.04    1,813,504.0      912,384.0     40428.0      45056.0       0.00%     85484.0\n",
      "15              layer1.1.bn3   11  32  32   11  32  32       22.0       0.04       45,056.0       22,528.0     45144.0      45056.0       0.00%     90200.0\n",
      "16            layer1.1.conv3   11  32  32   64  32  32      704.0       0.25    1,376,256.0      720,896.0     47872.0     262144.0       2.25%    310016.0\n",
      "17             layer1.1.relu   11  32  32   11  32  32        0.0       0.04       11,264.0       11,264.0     45056.0      45056.0       0.00%     90112.0\n",
      "18              layer1.2.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "19           layer1.2.select   64  32  32   13  32  32       64.0       0.05            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "20            layer1.2.conv1   13  32  32   12  32  32      156.0       0.05      307,200.0      159,744.0     53872.0      49152.0       0.00%    103024.0\n",
      "21              layer1.2.bn2   12  32  32   12  32  32       24.0       0.05       49,152.0       24,576.0     49248.0      49152.0       0.00%     98400.0\n",
      "22            layer1.2.conv2   12  32  32   15  32  32     1620.0       0.06    3,302,400.0    1,658,880.0     55632.0      61440.0       0.00%    117072.0\n",
      "23              layer1.2.bn3   15  32  32   15  32  32       30.0       0.06       61,440.0       30,720.0     61560.0      61440.0       0.00%    123000.0\n",
      "24            layer1.2.conv3   15  32  32   64  32  32      960.0       0.25    1,900,544.0      983,040.0     65280.0     262144.0       0.00%    327424.0\n",
      "25             layer1.2.relu   15  32  32   15  32  32        0.0       0.06       15,360.0       15,360.0     61440.0      61440.0       0.00%    122880.0\n",
      "26              layer1.3.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       2.26%    524800.0\n",
      "27           layer1.3.select   64  32  32    9  32  32       64.0       0.04            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "28            layer1.3.conv1    9  32  32   10  32  32       90.0       0.04      174,080.0       92,160.0     37224.0      40960.0       0.00%     78184.0\n",
      "29              layer1.3.bn2   10  32  32   10  32  32       20.0       0.04       40,960.0       20,480.0     41040.0      40960.0       0.00%     82000.0\n",
      "30            layer1.3.conv2   10  32  32   10  32  32      900.0       0.04    1,832,960.0      921,600.0     44560.0      40960.0       0.00%     85520.0\n",
      "31              layer1.3.bn3   10  32  32   10  32  32       20.0       0.04       40,960.0       20,480.0     41040.0      40960.0       0.00%     82000.0\n",
      "32            layer1.3.conv3   10  32  32   64  32  32      640.0       0.25    1,245,184.0      655,360.0     43520.0     262144.0       0.00%    305664.0\n",
      "33             layer1.3.relu   10  32  32   10  32  32        0.0       0.04       10,240.0       10,240.0     40960.0      40960.0       0.00%     81920.0\n",
      "34              layer1.4.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "35           layer1.4.select   64  32  32    9  32  32       64.0       0.04            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "36            layer1.4.conv1    9  32  32   13  32  32      117.0       0.05      226,304.0      119,808.0     37332.0      53248.0       0.00%     90580.0\n",
      "37              layer1.4.bn2   13  32  32   13  32  32       26.0       0.05       53,248.0       26,624.0     53352.0      53248.0       0.00%    106600.0\n",
      "38            layer1.4.conv2   13  32  32   13  32  32     1521.0       0.05    3,101,696.0    1,557,504.0     59332.0      53248.0       0.00%    112580.0\n",
      "39              layer1.4.bn3   13  32  32   13  32  32       26.0       0.05       53,248.0       26,624.0     53352.0      53248.0       0.00%    106600.0\n",
      "40            layer1.4.conv3   13  32  32   64  32  32      832.0       0.25    1,638,400.0      851,968.0     56576.0     262144.0       0.00%    318720.0\n",
      "41             layer1.4.relu   13  32  32   13  32  32        0.0       0.05       13,312.0       13,312.0     53248.0      53248.0       0.00%    106496.0\n",
      "42              layer1.5.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "43           layer1.5.select   64  32  32   10  32  32       64.0       0.04            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "44            layer1.5.conv1   10  32  32   14  32  32      140.0       0.05      272,384.0      143,360.0     41520.0      57344.0       0.00%     98864.0\n",
      "45              layer1.5.bn2   14  32  32   14  32  32       28.0       0.05       57,344.0       28,672.0     57456.0      57344.0       0.00%    114800.0\n",
      "46            layer1.5.conv2   14  32  32   14  32  32     1764.0       0.05    3,598,336.0    1,806,336.0     64400.0      57344.0       0.00%    121744.0\n",
      "47              layer1.5.bn3   14  32  32   14  32  32       28.0       0.05       57,344.0       28,672.0     57456.0      57344.0       0.00%    114800.0\n",
      "48            layer1.5.conv3   14  32  32   64  32  32      896.0       0.25    1,769,472.0      917,504.0     60928.0     262144.0       0.00%    323072.0\n",
      "49             layer1.5.relu   14  32  32   14  32  32        0.0       0.05       14,336.0       14,336.0     57344.0      57344.0       0.00%    114688.0\n",
      "50              layer1.6.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "51           layer1.6.select   64  32  32    9  32  32       64.0       0.04            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "52            layer1.6.conv1    9  32  32   11  32  32       99.0       0.04      191,488.0      101,376.0     37260.0      45056.0       2.25%     82316.0\n",
      "53              layer1.6.bn2   11  32  32   11  32  32       22.0       0.04       45,056.0       22,528.0     45144.0      45056.0       0.00%     90200.0\n",
      "54            layer1.6.conv2   11  32  32   11  32  32     1089.0       0.04    2,219,008.0    1,115,136.0     49412.0      45056.0       0.00%     94468.0\n",
      "55              layer1.6.bn3   11  32  32   11  32  32       22.0       0.04       45,056.0       22,528.0     45144.0      45056.0       0.00%     90200.0\n",
      "56            layer1.6.conv3   11  32  32   64  32  32      704.0       0.25    1,376,256.0      720,896.0     47872.0     262144.0       0.00%    310016.0\n",
      "57             layer1.6.relu   11  32  32   11  32  32        0.0       0.04       11,264.0       11,264.0     45056.0      45056.0       0.00%     90112.0\n",
      "58              layer1.7.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "59           layer1.7.select   64  32  32   14  32  32       64.0       0.05            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "60            layer1.7.conv1   14  32  32   14  32  32      196.0       0.05      387,072.0      200,704.0     58128.0      57344.0       0.00%    115472.0\n",
      "61              layer1.7.bn2   14  32  32   14  32  32       28.0       0.05       57,344.0       28,672.0     57456.0      57344.0       0.00%    114800.0\n",
      "62            layer1.7.conv2   14  32  32   14  32  32     1764.0       0.05    3,598,336.0    1,806,336.0     64400.0      57344.0       0.00%    121744.0\n",
      "63              layer1.7.bn3   14  32  32   14  32  32       28.0       0.05       57,344.0       28,672.0     57456.0      57344.0       0.00%    114800.0\n",
      "64            layer1.7.conv3   14  32  32   64  32  32      896.0       0.25    1,769,472.0      917,504.0     60928.0     262144.0       0.00%    323072.0\n",
      "65             layer1.7.relu   14  32  32   14  32  32        0.0       0.05       14,336.0       14,336.0     57344.0      57344.0       0.00%    114688.0\n",
      "66              layer1.8.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "67           layer1.8.select   64  32  32    8  32  32       64.0       0.03            0.0            0.0         0.0          0.0       2.25%         0.0\n",
      "68            layer1.8.conv1    8  32  32   10  32  32       80.0       0.04      153,600.0       81,920.0     33088.0      40960.0       0.00%     74048.0\n",
      "69              layer1.8.bn2   10  32  32   10  32  32       20.0       0.04       40,960.0       20,480.0     41040.0      40960.0       0.00%     82000.0\n",
      "70            layer1.8.conv2   10  32  32   12  32  32     1080.0       0.05    2,199,552.0    1,105,920.0     45280.0      49152.0       0.00%     94432.0\n",
      "71              layer1.8.bn3   12  32  32   12  32  32       24.0       0.05       49,152.0       24,576.0     49248.0      49152.0       0.00%     98400.0\n",
      "72            layer1.8.conv3   12  32  32   64  32  32      768.0       0.25    1,507,328.0      786,432.0     52224.0     262144.0       0.00%    314368.0\n",
      "73             layer1.8.relu   12  32  32   12  32  32        0.0       0.05       12,288.0       12,288.0     49152.0      49152.0       0.00%     98304.0\n",
      "74              layer1.9.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "75           layer1.9.select   64  32  32   13  32  32       64.0       0.05            0.0            0.0         0.0          0.0       2.25%         0.0\n",
      "76            layer1.9.conv1   13  32  32   12  32  32      156.0       0.05      307,200.0      159,744.0     53872.0      49152.0       0.00%    103024.0\n",
      "77              layer1.9.bn2   12  32  32   12  32  32       24.0       0.05       49,152.0       24,576.0     49248.0      49152.0       0.00%     98400.0\n",
      "78            layer1.9.conv2   12  32  32   10  32  32     1080.0       0.04    2,201,600.0    1,105,920.0     53472.0      40960.0       0.00%     94432.0\n",
      "79              layer1.9.bn3   10  32  32   10  32  32       20.0       0.04       40,960.0       20,480.0     41040.0      40960.0       0.00%     82000.0\n",
      "80            layer1.9.conv3   10  32  32   64  32  32      640.0       0.25    1,245,184.0      655,360.0     43520.0     262144.0       0.00%    305664.0\n",
      "81             layer1.9.relu   10  32  32   10  32  32        0.0       0.04       10,240.0       10,240.0     40960.0      40960.0       0.00%     81920.0\n",
      "82             layer1.10.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "83          layer1.10.select   64  32  32    9  32  32       64.0       0.04            0.0            0.0         0.0          0.0       2.25%         0.0\n",
      "84           layer1.10.conv1    9  32  32   10  32  32       90.0       0.04      174,080.0       92,160.0     37224.0      40960.0       0.00%     78184.0\n",
      "85             layer1.10.bn2   10  32  32   10  32  32       20.0       0.04       40,960.0       20,480.0     41040.0      40960.0       0.00%     82000.0\n",
      "86           layer1.10.conv2   10  32  32   12  32  32     1080.0       0.05    2,199,552.0    1,105,920.0     45280.0      49152.0       0.00%     94432.0\n",
      "87             layer1.10.bn3   12  32  32   12  32  32       24.0       0.05       49,152.0       24,576.0     49248.0      49152.0       0.00%     98400.0\n",
      "88           layer1.10.conv3   12  32  32   64  32  32      768.0       0.25    1,507,328.0      786,432.0     52224.0     262144.0       0.00%    314368.0\n",
      "89            layer1.10.relu   12  32  32   12  32  32        0.0       0.05       12,288.0       12,288.0     49152.0      49152.0       0.00%     98304.0\n",
      "90             layer1.11.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "91          layer1.11.select   64  32  32   10  32  32       64.0       0.04            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "92           layer1.11.conv1   10  32  32   11  32  32      110.0       0.04      214,016.0      112,640.0     41400.0      45056.0       0.00%     86456.0\n",
      "93             layer1.11.bn2   11  32  32   11  32  32       22.0       0.04       45,056.0       22,528.0     45144.0      45056.0       0.00%     90200.0\n",
      "94           layer1.11.conv2   11  32  32   16  32  32     1584.0       0.06    3,227,648.0    1,622,016.0     51392.0      65536.0       0.28%    116928.0\n",
      "95             layer1.11.bn3   16  32  32   16  32  32       32.0       0.06       65,536.0       32,768.0     65664.0      65536.0       0.14%    131200.0\n",
      "96           layer1.11.conv3   16  32  32   64  32  32     1024.0       0.25    2,031,616.0    1,048,576.0     69632.0     262144.0       0.28%    331776.0\n",
      "97            layer1.11.relu   16  32  32   16  32  32        0.0       0.06       16,384.0       16,384.0     65536.0      65536.0       0.14%    131072.0\n",
      "98             layer1.12.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.14%    524800.0\n",
      "99          layer1.12.select   64  32  32    2  32  32       64.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "100          layer1.12.conv1    2  32  32    5  32  32       10.0       0.02       15,360.0       10,240.0      8232.0      20480.0       0.14%     28712.0\n",
      "101            layer1.12.bn2    5  32  32    5  32  32       10.0       0.02       20,480.0       10,240.0     20520.0      20480.0       0.14%     41000.0\n",
      "102          layer1.12.conv2    5  32  32    7  32  32      315.0       0.03      637,952.0      322,560.0     21740.0      28672.0       0.14%     50412.0\n",
      "103            layer1.12.bn3    7  32  32    7  32  32       14.0       0.03       28,672.0       14,336.0     28728.0      28672.0       0.14%     57400.0\n",
      "104          layer1.12.conv3    7  32  32   64  32  32      448.0       0.25      851,968.0      458,752.0     30464.0     262144.0       0.28%    292608.0\n",
      "105           layer1.12.relu    7  32  32    7  32  32        0.0       0.03        7,168.0        7,168.0     28672.0      28672.0       0.14%     57344.0\n",
      "106            layer1.13.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.14%    524800.0\n",
      "107         layer1.13.select   64  32  32    5  32  32       64.0       0.02            0.0            0.0         0.0          0.0       0.56%         0.0\n",
      "108          layer1.13.conv1    5  32  32    8  32  32       40.0       0.03       73,728.0       40,960.0     20640.0      32768.0       0.14%     53408.0\n",
      "109            layer1.13.bn2    8  32  32    8  32  32       16.0       0.03       32,768.0       16,384.0     32832.0      32768.0       0.14%     65600.0\n",
      "110          layer1.13.conv2    8  32  32   13  32  32      936.0       0.05    1,903,616.0      958,464.0     36512.0      53248.0       0.14%     89760.0\n",
      "111            layer1.13.bn3   13  32  32   13  32  32       26.0       0.05       53,248.0       26,624.0     53352.0      53248.0       0.14%    106600.0\n",
      "112          layer1.13.conv3   13  32  32   64  32  32      832.0       0.25    1,638,400.0      851,968.0     56576.0     262144.0       0.14%    318720.0\n",
      "113           layer1.13.relu   13  32  32   13  32  32        0.0       0.05       13,312.0       13,312.0     53248.0      53248.0       0.14%    106496.0\n",
      "114            layer1.14.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.14%    524800.0\n",
      "115         layer1.14.select   64  32  32   11  32  32       64.0       0.04            0.0            0.0         0.0          0.0       0.28%         0.0\n",
      "116          layer1.14.conv1   11  32  32   12  32  32      132.0       0.05      258,048.0      135,168.0     45584.0      49152.0       0.14%     94736.0\n",
      "117            layer1.14.bn2   12  32  32   12  32  32       24.0       0.05       49,152.0       24,576.0     49248.0      49152.0       0.14%     98400.0\n",
      "118          layer1.14.conv2   12  32  32   13  32  32     1404.0       0.05    2,862,080.0    1,437,696.0     54768.0      53248.0       0.14%    108016.0\n",
      "119            layer1.14.bn3   13  32  32   13  32  32       26.0       0.05       53,248.0       26,624.0     53352.0      53248.0       0.14%    106600.0\n",
      "120          layer1.14.conv3   13  32  32   64  32  32      832.0       0.25    1,638,400.0      851,968.0     56576.0     262144.0       0.28%    318720.0\n",
      "121           layer1.14.relu   13  32  32   13  32  32        0.0       0.05       13,312.0       13,312.0     53248.0      53248.0       0.14%    106496.0\n",
      "122            layer1.15.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.14%    524800.0\n",
      "123         layer1.15.select   64  32  32   11  32  32       64.0       0.04            0.0            0.0         0.0          0.0       0.14%         0.0\n",
      "124          layer1.15.conv1   11  32  32   12  32  32      132.0       0.05      258,048.0      135,168.0     45584.0      49152.0       0.14%     94736.0\n",
      "125            layer1.15.bn2   12  32  32   12  32  32       24.0       0.05       49,152.0       24,576.0     49248.0      49152.0       0.14%     98400.0\n",
      "126          layer1.15.conv2   12  32  32   16  32  32     1728.0       0.06    3,522,560.0    1,769,472.0     56064.0      65536.0       0.28%    121600.0\n",
      "127            layer1.15.bn3   16  32  32   16  32  32       32.0       0.06       65,536.0       32,768.0     65664.0      65536.0       0.14%    131200.0\n",
      "128          layer1.15.conv3   16  32  32   64  32  32     1024.0       0.25    2,031,616.0    1,048,576.0     69632.0     262144.0       0.14%    331776.0\n",
      "129           layer1.15.relu   16  32  32   16  32  32        0.0       0.06       16,384.0       16,384.0     65536.0      65536.0       0.14%    131072.0\n",
      "130            layer1.16.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.28%    524800.0\n",
      "131         layer1.16.select   64  32  32    2  32  32       64.0       0.01            0.0            0.0         0.0          0.0       0.29%         0.0\n",
      "132          layer1.16.conv1    2  32  32    6  32  32       12.0       0.02       18,432.0       12,288.0      8240.0      24576.0       0.00%     32816.0\n",
      "133            layer1.16.bn2    6  32  32    6  32  32       12.0       0.02       24,576.0       12,288.0     24624.0      24576.0       0.00%     49200.0\n",
      "134          layer1.16.conv2    6  32  32    8  32  32      432.0       0.03      876,544.0      442,368.0     26304.0      32768.0       0.00%     59072.0\n",
      "135            layer1.16.bn3    8  32  32    8  32  32       16.0       0.03       32,768.0       16,384.0     32832.0      32768.0       0.00%     65600.0\n",
      "136          layer1.16.conv3    8  32  32   64  32  32      512.0       0.25      983,040.0      524,288.0     34816.0     262144.0       0.00%    296960.0\n",
      "137           layer1.16.relu    8  32  32    8  32  32        0.0       0.03        8,192.0        8,192.0     32768.0      32768.0       0.00%     65536.0\n",
      "138            layer1.17.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       0.00%    524800.0\n",
      "139         layer1.17.select   64  32  32    6  32  32       64.0       0.02            0.0            0.0         0.0          0.0       2.32%         0.0\n",
      "140          layer1.17.conv1    6  32  32   10  32  32       60.0       0.04      112,640.0       61,440.0     24816.0      40960.0       0.00%     65776.0\n",
      "141            layer1.17.bn2   10  32  32   10  32  32       20.0       0.04       40,960.0       20,480.0     41040.0      40960.0       0.00%     82000.0\n",
      "142          layer1.17.conv2   10  32  32   15  32  32     1350.0       0.06    2,749,440.0    1,382,400.0     46360.0      61440.0       0.00%    107800.0\n",
      "143            layer1.17.bn3   15  32  32   15  32  32       30.0       0.06       61,440.0       30,720.0     61560.0      61440.0       0.00%    123000.0\n",
      "144          layer1.17.conv3   15  32  32   64  32  32      960.0       0.25    1,900,544.0      983,040.0     65280.0     262144.0       0.00%    327424.0\n",
      "145           layer1.17.relu   15  32  32   15  32  32        0.0       0.06       15,360.0       15,360.0     61440.0      61440.0       0.00%    122880.0\n",
      "146             layer2.0.bn1   64  32  32   64  32  32      128.0       0.25      262,144.0      131,072.0    262656.0     262144.0       2.25%    524800.0\n",
      "147          layer2.0.select   64  32  32    7  32  32       64.0       0.03            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "148           layer2.0.conv1    7  32  32   10  32  32       70.0       0.04      133,120.0       71,680.0     28952.0      40960.0       0.00%     69912.0\n",
      "149             layer2.0.bn2   10  32  32   10  32  32       20.0       0.04       40,960.0       20,480.0     41040.0      40960.0       0.00%     82000.0\n",
      "150           layer2.0.conv2   10  32  32   29  16  16     2610.0       0.03    1,328,896.0      668,160.0     51400.0      29696.0       0.00%     81096.0\n",
      "151             layer2.0.bn3   29  16  16   29  16  16       58.0       0.03       29,696.0       14,848.0     29928.0      29696.0       0.00%     59624.0\n",
      "152           layer2.0.conv3   29  16  16  128  16  16     3712.0       0.12    1,867,776.0      950,272.0     44544.0     131072.0       0.00%    175616.0\n",
      "153            layer2.0.relu   29  16  16   29  16  16        0.0       0.03        7,424.0        7,424.0     29696.0      29696.0       0.00%     59392.0\n",
      "154    layer2.0.downsample.0   64  32  32  128  16  16     8192.0       0.12    4,161,536.0    2,097,152.0    294912.0     131072.0       2.25%    425984.0\n",
      "155             layer2.1.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "156          layer2.1.select  128  16  16   21  16  16      128.0       0.02            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "157           layer2.1.conv1   21  16  16   26  16  16      546.0       0.03      272,896.0      139,776.0     23688.0      26624.0       0.00%     50312.0\n",
      "158             layer2.1.bn2   26  16  16   26  16  16       52.0       0.03       26,624.0       13,312.0     26832.0      26624.0       0.00%     53456.0\n",
      "159           layer2.1.conv2   26  16  16   28  16  16     6552.0       0.03    3,347,456.0    1,677,312.0     52832.0      28672.0       0.00%     81504.0\n",
      "160             layer2.1.bn3   28  16  16   28  16  16       56.0       0.03       28,672.0       14,336.0     28896.0      28672.0       0.00%     57568.0\n",
      "161           layer2.1.conv3   28  16  16  128  16  16     3584.0       0.12    1,802,240.0      917,504.0     43008.0     131072.0       0.00%    174080.0\n",
      "162            layer2.1.relu   28  16  16   28  16  16        0.0       0.03        7,168.0        7,168.0     28672.0      28672.0       0.00%     57344.0\n",
      "163             layer2.2.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "164          layer2.2.select  128  16  16   24  16  16      128.0       0.02            0.0            0.0         0.0          0.0       2.26%         0.0\n",
      "165           layer2.2.conv1   24  16  16   25  16  16      600.0       0.02      300,800.0      153,600.0     26976.0      25600.0       0.00%     52576.0\n",
      "166             layer2.2.bn2   25  16  16   25  16  16       50.0       0.02       25,600.0       12,800.0     25800.0      25600.0       0.00%     51400.0\n",
      "167           layer2.2.conv2   25  16  16   24  16  16     5400.0       0.02    2,758,656.0    1,382,400.0     47200.0      24576.0       0.00%     71776.0\n",
      "168             layer2.2.bn3   24  16  16   24  16  16       48.0       0.02       24,576.0       12,288.0     24768.0      24576.0       0.00%     49344.0\n",
      "169           layer2.2.conv3   24  16  16  128  16  16     3072.0       0.12    1,540,096.0      786,432.0     36864.0     131072.0       0.00%    167936.0\n",
      "170            layer2.2.relu   24  16  16   24  16  16        0.0       0.02        6,144.0        6,144.0     24576.0      24576.0       0.00%     49152.0\n",
      "171             layer2.3.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "172          layer2.3.select  128  16  16   17  16  16      128.0       0.02            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "173           layer2.3.conv1   17  16  16   25  16  16      425.0       0.02      211,200.0      108,800.0     19108.0      25600.0       2.25%     44708.0\n",
      "174             layer2.3.bn2   25  16  16   25  16  16       50.0       0.02       25,600.0       12,800.0     25800.0      25600.0       0.00%     51400.0\n",
      "175           layer2.3.conv2   25  16  16   26  16  16     5850.0       0.03    2,988,544.0    1,497,600.0     49000.0      26624.0       0.00%     75624.0\n",
      "176             layer2.3.bn3   26  16  16   26  16  16       52.0       0.03       26,624.0       13,312.0     26832.0      26624.0       0.00%     53456.0\n",
      "177           layer2.3.conv3   26  16  16  128  16  16     3328.0       0.12    1,671,168.0      851,968.0     39936.0     131072.0       0.00%    171008.0\n",
      "178            layer2.3.relu   26  16  16   26  16  16        0.0       0.03        6,656.0        6,656.0     26624.0      26624.0       0.00%     53248.0\n",
      "179             layer2.4.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "180          layer2.4.select  128  16  16   26  16  16      128.0       0.03            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "181           layer2.4.conv1   26  16  16   25  16  16      650.0       0.02      326,400.0      166,400.0     29224.0      25600.0       0.00%     54824.0\n",
      "182             layer2.4.bn2   25  16  16   25  16  16       50.0       0.02       25,600.0       12,800.0     25800.0      25600.0       2.26%     51400.0\n",
      "183           layer2.4.conv2   25  16  16   25  16  16     5625.0       0.02    2,873,600.0    1,440,000.0     48100.0      25600.0       0.00%     73700.0\n",
      "184             layer2.4.bn3   25  16  16   25  16  16       50.0       0.02       25,600.0       12,800.0     25800.0      25600.0       0.00%     51400.0\n",
      "185           layer2.4.conv3   25  16  16  128  16  16     3200.0       0.12    1,605,632.0      819,200.0     38400.0     131072.0       0.00%    169472.0\n",
      "186            layer2.4.relu   25  16  16   25  16  16        0.0       0.02        6,400.0        6,400.0     25600.0      25600.0       0.00%     51200.0\n",
      "187             layer2.5.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "188          layer2.5.select  128  16  16   26  16  16      128.0       0.03            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "189           layer2.5.conv1   26  16  16   27  16  16      702.0       0.03      352,512.0      179,712.0     29432.0      27648.0       0.00%     57080.0\n",
      "190             layer2.5.bn2   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.00%     55512.0\n",
      "191           layer2.5.conv2   27  16  16   20  16  16     4860.0       0.02    2,483,200.0    1,244,160.0     47088.0      20480.0       2.25%     67568.0\n",
      "192             layer2.5.bn3   20  16  16   20  16  16       40.0       0.02       20,480.0       10,240.0     20640.0      20480.0       0.00%     41120.0\n",
      "193           layer2.5.conv3   20  16  16  128  16  16     2560.0       0.12    1,277,952.0      655,360.0     30720.0     131072.0       0.89%    161792.0\n",
      "194            layer2.5.relu   20  16  16   20  16  16        0.0       0.02        5,120.0        5,120.0     20480.0      20480.0       0.00%     40960.0\n",
      "195             layer2.6.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.08%    263168.0\n",
      "196          layer2.6.select  128  16  16   22  16  16      128.0       0.02            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "197           layer2.6.conv1   22  16  16   22  16  16      484.0       0.02      242,176.0      123,904.0     24464.0      22528.0       0.00%     46992.0\n",
      "198             layer2.6.bn2   22  16  16   22  16  16       44.0       0.02       22,528.0       11,264.0     22704.0      22528.0       0.00%     45232.0\n",
      "199           layer2.6.conv2   22  16  16   28  16  16     5544.0       0.03    2,831,360.0    1,419,264.0     44704.0      28672.0       0.00%     73376.0\n",
      "200             layer2.6.bn3   28  16  16   28  16  16       56.0       0.03       28,672.0       14,336.0     28896.0      28672.0       0.00%     57568.0\n",
      "201           layer2.6.conv3   28  16  16  128  16  16     3584.0       0.12    1,802,240.0      917,504.0     43008.0     131072.0       2.31%    174080.0\n",
      "202            layer2.6.relu   28  16  16   28  16  16        0.0       0.03        7,168.0        7,168.0     28672.0      28672.0       0.00%     57344.0\n",
      "203             layer2.7.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "204          layer2.7.select  128  16  16   25  16  16      128.0       0.02            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "205           layer2.7.conv1   25  16  16   27  16  16      675.0       0.03      338,688.0      172,800.0     28300.0      27648.0       0.00%     55948.0\n",
      "206             layer2.7.bn2   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.00%     55512.0\n",
      "207           layer2.7.conv2   27  16  16   29  16  16     7047.0       0.03    3,600,640.0    1,804,032.0     55836.0      29696.0       2.25%     85532.0\n",
      "208             layer2.7.bn3   29  16  16   29  16  16       58.0       0.03       29,696.0       14,848.0     29928.0      29696.0       0.00%     59624.0\n",
      "209           layer2.7.conv3   29  16  16  128  16  16     3712.0       0.12    1,867,776.0      950,272.0     44544.0     131072.0       0.00%    175616.0\n",
      "210            layer2.7.relu   29  16  16   29  16  16        0.0       0.03        7,424.0        7,424.0     29696.0      29696.0       0.00%     59392.0\n",
      "211             layer2.8.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "212          layer2.8.select  128  16  16   28  16  16      128.0       0.03            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "213           layer2.8.conv1   28  16  16   30  16  16      840.0       0.03      422,400.0      215,040.0     32032.0      30720.0       2.25%     62752.0\n",
      "214             layer2.8.bn2   30  16  16   30  16  16       60.0       0.03       30,720.0       15,360.0     30960.0      30720.0       0.00%     61680.0\n",
      "215           layer2.8.conv2   30  16  16   23  16  16     6210.0       0.02    3,173,632.0    1,589,760.0     55560.0      23552.0       0.00%     79112.0\n",
      "216             layer2.8.bn3   23  16  16   23  16  16       46.0       0.02       23,552.0       11,776.0     23736.0      23552.0       0.00%     47288.0\n",
      "217           layer2.8.conv3   23  16  16  128  16  16     2944.0       0.12    1,474,560.0      753,664.0     35328.0     131072.0       0.00%    166400.0\n",
      "218            layer2.8.relu   23  16  16   23  16  16        0.0       0.02        5,888.0        5,888.0     23552.0      23552.0       0.00%     47104.0\n",
      "219             layer2.9.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "220          layer2.9.select  128  16  16   23  16  16      128.0       0.02            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "221           layer2.9.conv1   23  16  16   25  16  16      575.0       0.02      288,000.0      147,200.0     25852.0      25600.0       2.47%     51452.0\n",
      "222             layer2.9.bn2   25  16  16   25  16  16       50.0       0.02       25,600.0       12,800.0     25800.0      25600.0       0.14%     51400.0\n",
      "223           layer2.9.conv2   25  16  16   29  16  16     6525.0       0.03    3,333,376.0    1,670,400.0     51700.0      29696.0       0.14%     81396.0\n",
      "224             layer2.9.bn3   29  16  16   29  16  16       58.0       0.03       29,696.0       14,848.0     29928.0      29696.0       0.14%     59624.0\n",
      "225           layer2.9.conv3   29  16  16  128  16  16     3712.0       0.12    1,867,776.0      950,272.0     44544.0     131072.0       0.14%    175616.0\n",
      "226            layer2.9.relu   29  16  16   29  16  16        0.0       0.03        7,424.0        7,424.0     29696.0      29696.0       0.14%     59392.0\n",
      "227            layer2.10.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.14%    263168.0\n",
      "228         layer2.10.select  128  16  16   26  16  16      128.0       0.03            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "229          layer2.10.conv1   26  16  16   22  16  16      572.0       0.02      287,232.0      146,432.0     28912.0      22528.0       0.28%     51440.0\n",
      "230            layer2.10.bn2   22  16  16   22  16  16       44.0       0.02       22,528.0       11,264.0     22704.0      22528.0       0.28%     45232.0\n",
      "231          layer2.10.conv2   22  16  16   27  16  16     5346.0       0.03    2,730,240.0    1,368,576.0     43912.0      27648.0       0.28%     71560.0\n",
      "232            layer2.10.bn3   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.14%     55512.0\n",
      "233          layer2.10.conv3   27  16  16  128  16  16     3456.0       0.12    1,736,704.0      884,736.0     41472.0     131072.0       0.28%    172544.0\n",
      "234           layer2.10.relu   27  16  16   27  16  16        0.0       0.03        6,912.0        6,912.0     27648.0      27648.0       0.14%     55296.0\n",
      "235            layer2.11.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.28%    263168.0\n",
      "236         layer2.11.select  128  16  16   19  16  16      128.0       0.02            0.0            0.0         0.0          0.0       0.53%         0.0\n",
      "237          layer2.11.conv1   19  16  16   28  16  16      532.0       0.03      265,216.0      136,192.0     21584.0      28672.0       0.00%     50256.0\n",
      "238            layer2.11.bn2   28  16  16   28  16  16       56.0       0.03       28,672.0       14,336.0     28896.0      28672.0       0.00%     57568.0\n",
      "239          layer2.11.conv2   28  16  16   27  16  16     6804.0       0.03    3,476,736.0    1,741,824.0     55888.0      27648.0       0.00%     83536.0\n",
      "240            layer2.11.bn3   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.00%     55512.0\n",
      "241          layer2.11.conv3   27  16  16  128  16  16     3456.0       0.12    1,736,704.0      884,736.0     41472.0     131072.0       0.00%    172544.0\n",
      "242           layer2.11.relu   27  16  16   27  16  16        0.0       0.03        6,912.0        6,912.0     27648.0      27648.0       0.00%     55296.0\n",
      "243            layer2.12.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "244         layer2.12.select  128  16  16   24  16  16      128.0       0.02            0.0            0.0         0.0          0.0       2.29%         0.0\n",
      "245          layer2.12.conv1   24  16  16   27  16  16      648.0       0.03      324,864.0      165,888.0     27168.0      27648.0       0.00%     54816.0\n",
      "246            layer2.12.bn2   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.00%     55512.0\n",
      "247          layer2.12.conv2   27  16  16   30  16  16     7290.0       0.03    3,724,800.0    1,866,240.0     56808.0      30720.0       0.00%     87528.0\n",
      "248            layer2.12.bn3   30  16  16   30  16  16       60.0       0.03       30,720.0       15,360.0     30960.0      30720.0       0.00%     61680.0\n",
      "249          layer2.12.conv3   30  16  16  128  16  16     3840.0       0.12    1,933,312.0      983,040.0     46080.0     131072.0       0.00%    177152.0\n",
      "250           layer2.12.relu   30  16  16   30  16  16        0.0       0.03        7,680.0        7,680.0     30720.0      30720.0       0.00%     61440.0\n",
      "251            layer2.13.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "252         layer2.13.select  128  16  16   25  16  16      128.0       0.02            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "253          layer2.13.conv1   25  16  16   27  16  16      675.0       0.03      338,688.0      172,800.0     28300.0      27648.0       0.00%     55948.0\n",
      "254            layer2.13.bn2   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.00%     55512.0\n",
      "255          layer2.13.conv2   27  16  16   27  16  16     6561.0       0.03    3,352,320.0    1,679,616.0     53892.0      27648.0       0.00%     81540.0\n",
      "256            layer2.13.bn3   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.00%     55512.0\n",
      "257          layer2.13.conv3   27  16  16  128  16  16     3456.0       0.12    1,736,704.0      884,736.0     41472.0     131072.0       0.00%    172544.0\n",
      "258           layer2.13.relu   27  16  16   27  16  16        0.0       0.03        6,912.0        6,912.0     27648.0      27648.0       0.00%     55296.0\n",
      "259            layer2.14.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "260         layer2.14.select  128  16  16   27  16  16      128.0       0.03            0.0            0.0         0.0          0.0       2.25%         0.0\n",
      "261          layer2.14.conv1   27  16  16   24  16  16      648.0       0.02      325,632.0      165,888.0     30240.0      24576.0       0.00%     54816.0\n",
      "262            layer2.14.bn2   24  16  16   24  16  16       48.0       0.02       24,576.0       12,288.0     24768.0      24576.0       0.00%     49344.0\n",
      "263          layer2.14.conv2   24  16  16   28  16  16     6048.0       0.03    3,089,408.0    1,548,288.0     48768.0      28672.0       0.00%     77440.0\n",
      "264            layer2.14.bn3   28  16  16   28  16  16       56.0       0.03       28,672.0       14,336.0     28896.0      28672.0       0.00%     57568.0\n",
      "265          layer2.14.conv3   28  16  16  128  16  16     3584.0       0.12    1,802,240.0      917,504.0     43008.0     131072.0       0.00%    174080.0\n",
      "266           layer2.14.relu   28  16  16   28  16  16        0.0       0.03        7,168.0        7,168.0     28672.0      28672.0       0.00%     57344.0\n",
      "267            layer2.15.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "268         layer2.15.select  128  16  16   27  16  16      128.0       0.03            0.0            0.0         0.0          0.0       2.25%         0.0\n",
      "269          layer2.15.conv1   27  16  16   26  16  16      702.0       0.03      352,768.0      179,712.0     30456.0      26624.0       0.00%     57080.0\n",
      "270            layer2.15.bn2   26  16  16   26  16  16       52.0       0.03       26,624.0       13,312.0     26832.0      26624.0       0.00%     53456.0\n",
      "271          layer2.15.conv2   26  16  16   29  16  16     6786.0       0.03    3,467,008.0    1,737,216.0     53768.0      29696.0       0.00%     83464.0\n",
      "272            layer2.15.bn3   29  16  16   29  16  16       58.0       0.03       29,696.0       14,848.0     29928.0      29696.0       0.00%     59624.0\n",
      "273          layer2.15.conv3   29  16  16  128  16  16     3712.0       0.12    1,867,776.0      950,272.0     44544.0     131072.0       0.00%    175616.0\n",
      "274           layer2.15.relu   29  16  16   29  16  16        0.0       0.03        7,424.0        7,424.0     29696.0      29696.0       0.00%     59392.0\n",
      "275            layer2.16.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "276         layer2.16.select  128  16  16   26  16  16      128.0       0.03            0.0            0.0         0.0          0.0       2.25%         0.0\n",
      "277          layer2.16.conv1   26  16  16   27  16  16      702.0       0.03      352,512.0      179,712.0     29432.0      27648.0       0.00%     57080.0\n",
      "278            layer2.16.bn2   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.00%     55512.0\n",
      "279          layer2.16.conv2   27  16  16   28  16  16     6804.0       0.03    3,476,480.0    1,741,824.0     54864.0      28672.0       0.00%     83536.0\n",
      "280            layer2.16.bn3   28  16  16   28  16  16       56.0       0.03       28,672.0       14,336.0     28896.0      28672.0       0.00%     57568.0\n",
      "281          layer2.16.conv3   28  16  16  128  16  16     3584.0       0.12    1,802,240.0      917,504.0     43008.0     131072.0       0.00%    174080.0\n",
      "282           layer2.16.relu   28  16  16   28  16  16        0.0       0.03        7,168.0        7,168.0     28672.0      28672.0       0.00%     57344.0\n",
      "283            layer2.17.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.00%    263168.0\n",
      "284         layer2.17.select  128  16  16   30  16  16      128.0       0.03            0.0            0.0         0.0          0.0       0.00%         0.0\n",
      "285          layer2.17.conv1   30  16  16   22  16  16      660.0       0.02      332,288.0      168,960.0     33360.0      22528.0       0.00%     55888.0\n",
      "286            layer2.17.bn2   22  16  16   22  16  16       44.0       0.02       22,528.0       11,264.0     22704.0      22528.0       0.71%     45232.0\n",
      "287          layer2.17.conv2   22  16  16   27  16  16     5346.0       0.03    2,730,240.0    1,368,576.0     43912.0      27648.0       0.28%     71560.0\n",
      "288            layer2.17.bn3   27  16  16   27  16  16       54.0       0.03       27,648.0       13,824.0     27864.0      27648.0       0.14%     55512.0\n",
      "289          layer2.17.conv3   27  16  16  128  16  16     3456.0       0.12    1,736,704.0      884,736.0     41472.0     131072.0       0.14%    172544.0\n",
      "290           layer2.17.relu   27  16  16   27  16  16        0.0       0.03        6,912.0        6,912.0     27648.0      27648.0       0.14%     55296.0\n",
      "291             layer3.0.bn1  128  16  16  128  16  16      256.0       0.12      131,072.0       65,536.0    132096.0     131072.0       0.14%    263168.0\n",
      "292          layer3.0.select  128  16  16   10  16  16      128.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "293           layer3.0.conv1   10  16  16   25  16  16      250.0       0.02      121,600.0       64,000.0     11240.0      25600.0       0.14%     36840.0\n",
      "294             layer3.0.bn2   25  16  16   25  16  16       50.0       0.02       25,600.0       12,800.0     25800.0      25600.0       0.28%     51400.0\n",
      "295           layer3.0.conv2   25  16  16   64   8   8    14400.0       0.02    1,839,104.0      921,600.0     83200.0      16384.0       0.14%     99584.0\n",
      "296             layer3.0.bn3   64   8   8   64   8   8      128.0       0.02       16,384.0        8,192.0     16896.0      16384.0       0.14%     33280.0\n",
      "297           layer3.0.conv3   64   8   8  256   8   8    16384.0       0.06    2,080,768.0    1,048,576.0     81920.0      65536.0       0.14%    147456.0\n",
      "298            layer3.0.relu   64   8   8   64   8   8        0.0       0.02        4,096.0        4,096.0     16384.0      16384.0       0.14%     32768.0\n",
      "299    layer3.0.downsample.0  128  16  16  256   8   8    32768.0       0.06    4,177,920.0    2,097,152.0    262144.0      65536.0       0.28%    327680.0\n",
      "300             layer3.1.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "301          layer3.1.select  256   8   8   54   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.56%         0.0\n",
      "302           layer3.1.conv1   54   8   8   55   8   8     2970.0       0.01      376,640.0      190,080.0     25704.0      14080.0       0.14%     39784.0\n",
      "303             layer3.1.bn2   55   8   8   55   8   8      110.0       0.01       14,080.0        7,040.0     14520.0      14080.0       0.28%     28600.0\n",
      "304           layer3.1.conv2   55   8   8   60   8   8    29700.0       0.01    3,797,760.0    1,900,800.0    132880.0      15360.0       0.14%    148240.0\n",
      "305             layer3.1.bn3   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.28%     31200.0\n",
      "306           layer3.1.conv3   60   8   8  256   8   8    15360.0       0.06    1,949,696.0      983,040.0     76800.0      65536.0       0.28%    142336.0\n",
      "307            layer3.1.relu   60   8   8   60   8   8        0.0       0.01        3,840.0        3,840.0     15360.0      15360.0       0.14%     30720.0\n",
      "308             layer3.2.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "309          layer3.2.select  256   8   8   54   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "310           layer3.2.conv1   54   8   8   59   8   8     3186.0       0.01      404,032.0      203,904.0     26568.0      15104.0       0.00%     41672.0\n",
      "311             layer3.2.bn2   59   8   8   59   8   8      118.0       0.01       15,104.0        7,552.0     15576.0      15104.0       0.00%     30680.0\n",
      "312           layer3.2.conv2   59   8   8   54   8   8    28674.0       0.01    3,666,816.0    1,835,136.0    129800.0      13824.0       0.98%    143624.0\n",
      "313             layer3.2.bn3   54   8   8   54   8   8      108.0       0.01       13,824.0        6,912.0     14256.0      13824.0       0.14%     28080.0\n",
      "314           layer3.2.conv3   54   8   8  256   8   8    13824.0       0.06    1,753,088.0      884,736.0     69120.0      65536.0       0.28%    134656.0\n",
      "315            layer3.2.relu   54   8   8   54   8   8        0.0       0.01        3,456.0        3,456.0     13824.0      13824.0       0.14%     27648.0\n",
      "316             layer3.3.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "317          layer3.3.select  256   8   8   42   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "318           layer3.3.conv1   42   8   8   58   8   8     2436.0       0.01      308,096.0      155,904.0     20496.0      14848.0       0.14%     35344.0\n",
      "319             layer3.3.bn2   58   8   8   58   8   8      116.0       0.01       14,848.0        7,424.0     15312.0      14848.0       0.14%     30160.0\n",
      "320           layer3.3.conv2   58   8   8   62   8   8    32364.0       0.02    4,138,624.0    2,071,296.0    144304.0      15872.0       0.28%    160176.0\n",
      "321             layer3.3.bn3   62   8   8   62   8   8      124.0       0.02       15,872.0        7,936.0     16368.0      15872.0       0.14%     32240.0\n",
      "322           layer3.3.conv3   62   8   8  256   8   8    15872.0       0.06    2,015,232.0    1,015,808.0     79360.0      65536.0       0.14%    144896.0\n",
      "323            layer3.3.relu   62   8   8   62   8   8        0.0       0.02        3,968.0        3,968.0     15872.0      15872.0       0.14%     31744.0\n",
      "324             layer3.4.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "325          layer3.4.select  256   8   8   56   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.56%         0.0\n",
      "326           layer3.4.conv1   56   8   8   57   8   8     3192.0       0.01      404,928.0      204,288.0     27104.0      14592.0       0.14%     41696.0\n",
      "327             layer3.4.bn2   57   8   8   57   8   8      114.0       0.01       14,592.0        7,296.0     15048.0      14592.0       0.14%     29640.0\n",
      "328           layer3.4.conv2   57   8   8   61   8   8    31293.0       0.01    4,001,600.0    2,002,752.0    139764.0      15616.0       0.28%    155380.0\n",
      "329             layer3.4.bn3   61   8   8   61   8   8      122.0       0.01       15,616.0        7,808.0     16104.0      15616.0       0.14%     31720.0\n",
      "330           layer3.4.conv3   61   8   8  256   8   8    15616.0       0.06    1,982,464.0      999,424.0     78080.0      65536.0       0.14%    143616.0\n",
      "331            layer3.4.relu   61   8   8   61   8   8        0.0       0.01        3,904.0        3,904.0     15616.0      15616.0       0.14%     31232.0\n",
      "332             layer3.5.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.28%    133120.0\n",
      "333          layer3.5.select  256   8   8   49   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.28%         0.0\n",
      "334           layer3.5.conv1   49   8   8   56   8   8     2744.0       0.01      347,648.0      175,616.0     23520.0      14336.0       0.14%     37856.0\n",
      "335             layer3.5.bn2   56   8   8   56   8   8      112.0       0.01       14,336.0        7,168.0     14784.0      14336.0       0.14%     29120.0\n",
      "336           layer3.5.conv2   56   8   8   62   8   8    31248.0       0.02    3,995,776.0    1,999,872.0    139328.0      15872.0       0.28%    155200.0\n",
      "337             layer3.5.bn3   62   8   8   62   8   8      124.0       0.02       15,872.0        7,936.0     16368.0      15872.0       0.14%     32240.0\n",
      "338           layer3.5.conv3   62   8   8  256   8   8    15872.0       0.06    2,015,232.0    1,015,808.0     79360.0      65536.0       0.28%    144896.0\n",
      "339            layer3.5.relu   62   8   8   62   8   8        0.0       0.02        3,968.0        3,968.0     15872.0      15872.0       0.14%     31744.0\n",
      "340             layer3.6.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.28%    133120.0\n",
      "341          layer3.6.select  256   8   8   45   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.28%         0.0\n",
      "342           layer3.6.conv1   45   8   8   58   8   8     2610.0       0.01      330,368.0      167,040.0     21960.0      14848.0       0.28%     36808.0\n",
      "343             layer3.6.bn2   58   8   8   58   8   8      116.0       0.01       14,848.0        7,424.0     15312.0      14848.0       0.14%     30160.0\n",
      "344           layer3.6.conv2   58   8   8   62   8   8    32364.0       0.02    4,138,624.0    2,071,296.0    144304.0      15872.0       0.28%    160176.0\n",
      "345             layer3.6.bn3   62   8   8   62   8   8      124.0       0.02       15,872.0        7,936.0     16368.0      15872.0       0.14%     32240.0\n",
      "346           layer3.6.conv3   62   8   8  256   8   8    15872.0       0.06    2,015,232.0    1,015,808.0     79360.0      65536.0       0.14%    144896.0\n",
      "347            layer3.6.relu   62   8   8   62   8   8        0.0       0.02        3,968.0        3,968.0     15872.0      15872.0       0.14%     31744.0\n",
      "348             layer3.7.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "349          layer3.7.select  256   8   8   50   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.56%         0.0\n",
      "350           layer3.7.conv1   50   8   8   56   8   8     2800.0       0.01      354,816.0      179,200.0     24000.0      14336.0       0.14%     38336.0\n",
      "351             layer3.7.bn2   56   8   8   56   8   8      112.0       0.01       14,336.0        7,168.0     14784.0      14336.0       0.14%     29120.0\n",
      "352           layer3.7.conv2   56   8   8   60   8   8    30240.0       0.01    3,866,880.0    1,935,360.0    135296.0      15360.0       0.14%    150656.0\n",
      "353             layer3.7.bn3   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.14%     31200.0\n",
      "354           layer3.7.conv3   60   8   8  256   8   8    15360.0       0.06    1,949,696.0      983,040.0     76800.0      65536.0       0.14%    142336.0\n",
      "355            layer3.7.relu   60   8   8   60   8   8        0.0       0.01        3,840.0        3,840.0     15360.0      15360.0       0.14%     30720.0\n",
      "356             layer3.8.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "357          layer3.8.select  256   8   8   51   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.42%         0.0\n",
      "358           layer3.8.conv1   51   8   8   60   8   8     3060.0       0.01      387,840.0      195,840.0     25296.0      15360.0       0.14%     40656.0\n",
      "359             layer3.8.bn2   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.28%     31200.0\n",
      "360           layer3.8.conv2   60   8   8   62   8   8    33480.0       0.02    4,281,472.0    2,142,720.0    149280.0      15872.0       0.14%    165152.0\n",
      "361             layer3.8.bn3   62   8   8   62   8   8      124.0       0.02       15,872.0        7,936.0     16368.0      15872.0       0.14%     32240.0\n",
      "362           layer3.8.conv3   62   8   8  256   8   8    15872.0       0.06    2,015,232.0    1,015,808.0     79360.0      65536.0       0.14%    144896.0\n",
      "363            layer3.8.relu   62   8   8   62   8   8        0.0       0.02        3,968.0        3,968.0     15872.0      15872.0       0.14%     31744.0\n",
      "364             layer3.9.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "365          layer3.9.select  256   8   8   38   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "366           layer3.9.conv1   38   8   8   60   8   8     2280.0       0.01      288,000.0      145,920.0     18848.0      15360.0       0.14%     34208.0\n",
      "367             layer3.9.bn2   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.14%     31200.0\n",
      "368           layer3.9.conv2   60   8   8   62   8   8    33480.0       0.02    4,281,472.0    2,142,720.0    149280.0      15872.0       0.28%    165152.0\n",
      "369             layer3.9.bn3   62   8   8   62   8   8      124.0       0.02       15,872.0        7,936.0     16368.0      15872.0       0.14%     32240.0\n",
      "370           layer3.9.conv3   62   8   8  256   8   8    15872.0       0.06    2,015,232.0    1,015,808.0     79360.0      65536.0       0.14%    144896.0\n",
      "371            layer3.9.relu   62   8   8   62   8   8        0.0       0.02        3,968.0        3,968.0     15872.0      15872.0       0.14%     31744.0\n",
      "372            layer3.10.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "373         layer3.10.select  256   8   8   44   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "374          layer3.10.conv1   44   8   8   59   8   8     2596.0       0.01      328,512.0      166,144.0     21648.0      15104.0       0.14%     36752.0\n",
      "375            layer3.10.bn2   59   8   8   59   8   8      118.0       0.01       15,104.0        7,552.0     15576.0      15104.0       0.14%     30680.0\n",
      "376          layer3.10.conv2   59   8   8   64   8   8    33984.0       0.02    4,345,856.0    2,174,976.0    151040.0      16384.0       0.28%    167424.0\n",
      "377            layer3.10.bn3   64   8   8   64   8   8      128.0       0.02       16,384.0        8,192.0     16896.0      16384.0       0.14%     33280.0\n",
      "378          layer3.10.conv3   64   8   8  256   8   8    16384.0       0.06    2,080,768.0    1,048,576.0     81920.0      65536.0       0.14%    147456.0\n",
      "379           layer3.10.relu   64   8   8   64   8   8        0.0       0.02        4,096.0        4,096.0     16384.0      16384.0       0.14%     32768.0\n",
      "380            layer3.11.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "381         layer3.11.select  256   8   8   34   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.56%         0.0\n",
      "382          layer3.11.conv1   34   8   8   60   8   8     2040.0       0.01      257,280.0      130,560.0     16864.0      15360.0       0.14%     32224.0\n",
      "383            layer3.11.bn2   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.14%     31200.0\n",
      "384          layer3.11.conv2   60   8   8   63   8   8    34020.0       0.02    4,350,528.0    2,177,280.0    151440.0      16128.0       0.14%    167568.0\n",
      "385            layer3.11.bn3   63   8   8   63   8   8      126.0       0.02       16,128.0        8,064.0     16632.0      16128.0       0.28%     32760.0\n",
      "386          layer3.11.conv3   63   8   8  256   8   8    16128.0       0.06    2,048,000.0    1,032,192.0     80640.0      65536.0       0.14%    146176.0\n",
      "387           layer3.11.relu   63   8   8   63   8   8        0.0       0.02        4,032.0        4,032.0     16128.0      16128.0       0.14%     32256.0\n",
      "388            layer3.12.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "389         layer3.12.select  256   8   8   39   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "390          layer3.12.conv1   39   8   8   61   8   8     2379.0       0.01      300,608.0      152,256.0     19500.0      15616.0       0.14%     35116.0\n",
      "391            layer3.12.bn2   61   8   8   61   8   8      122.0       0.01       15,616.0        7,808.0     16104.0      15616.0       0.14%     31720.0\n",
      "392          layer3.12.conv2   61   8   8   62   8   8    34038.0       0.02    4,352,896.0    2,178,432.0    151768.0      15872.0       0.28%    167640.0\n",
      "393            layer3.12.bn3   62   8   8   62   8   8      124.0       0.02       15,872.0        7,936.0     16368.0      15872.0       0.14%     32240.0\n",
      "394          layer3.12.conv3   62   8   8  256   8   8    15872.0       0.06    2,015,232.0    1,015,808.0     79360.0      65536.0       0.28%    144896.0\n",
      "395           layer3.12.relu   62   8   8   62   8   8        0.0       0.02        3,968.0        3,968.0     15872.0      15872.0       0.14%     31744.0\n",
      "396            layer3.13.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "397         layer3.13.select  256   8   8   42   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.28%         0.0\n",
      "398          layer3.13.conv1   42   8   8   61   8   8     2562.0       0.01      324,032.0      163,968.0     21000.0      15616.0       0.28%     36616.0\n",
      "399            layer3.13.bn2   61   8   8   61   8   8      122.0       0.01       15,616.0        7,808.0     16104.0      15616.0       0.14%     31720.0\n",
      "400          layer3.13.conv2   61   8   8   61   8   8    33489.0       0.01    4,282,688.0    2,143,296.0    149572.0      15616.0       0.14%    165188.0\n",
      "401            layer3.13.bn3   61   8   8   61   8   8      122.0       0.01       15,616.0        7,808.0     16104.0      15616.0       0.14%     31720.0\n",
      "402          layer3.13.conv3   61   8   8  256   8   8    15616.0       0.06    1,982,464.0      999,424.0     78080.0      65536.0       0.14%    143616.0\n",
      "403           layer3.13.relu   61   8   8   61   8   8        0.0       0.01        3,904.0        3,904.0     15616.0      15616.0       0.00%     31232.0\n",
      "404            layer3.14.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "405         layer3.14.select  256   8   8   40   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.42%         0.0\n",
      "406          layer3.14.conv1   40   8   8   58   8   8     2320.0       0.01      293,248.0      148,480.0     19520.0      14848.0       0.14%     34368.0\n",
      "407            layer3.14.bn2   58   8   8   58   8   8      116.0       0.01       14,848.0        7,424.0     15312.0      14848.0       0.14%     30160.0\n",
      "408          layer3.14.conv2   58   8   8   62   8   8    32364.0       0.02    4,138,624.0    2,071,296.0    144304.0      15872.0       0.28%    160176.0\n",
      "409            layer3.14.bn3   62   8   8   62   8   8      124.0       0.02       15,872.0        7,936.0     16368.0      15872.0       0.28%     32240.0\n",
      "410          layer3.14.conv3   62   8   8  256   8   8    15872.0       0.06    2,015,232.0    1,015,808.0     79360.0      65536.0       0.14%    144896.0\n",
      "411           layer3.14.relu   62   8   8   62   8   8        0.0       0.02        3,968.0        3,968.0     15872.0      15872.0       0.14%     31744.0\n",
      "412            layer3.15.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.28%    133120.0\n",
      "413         layer3.15.select  256   8   8   41   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "414          layer3.15.conv1   41   8   8   60   8   8     2460.0       0.01      311,040.0      157,440.0     20336.0      15360.0       0.14%     35696.0\n",
      "415            layer3.15.bn2   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.14%     31200.0\n",
      "416          layer3.15.conv2   60   8   8   63   8   8    34020.0       0.02    4,350,528.0    2,177,280.0    151440.0      16128.0       0.28%    167568.0\n",
      "417            layer3.15.bn3   63   8   8   63   8   8      126.0       0.02       16,128.0        8,064.0     16632.0      16128.0       0.14%     32760.0\n",
      "418          layer3.15.conv3   63   8   8  256   8   8    16128.0       0.06    2,048,000.0    1,032,192.0     80640.0      65536.0       0.14%    146176.0\n",
      "419           layer3.15.relu   63   8   8   63   8   8        0.0       0.02        4,032.0        4,032.0     16128.0      16128.0       0.14%     32256.0\n",
      "420            layer3.16.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "421         layer3.16.select  256   8   8   42   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "422          layer3.16.conv1   42   8   8   58   8   8     2436.0       0.01      308,096.0      155,904.0     20496.0      14848.0       0.14%     35344.0\n",
      "423            layer3.16.bn2   58   8   8   58   8   8      116.0       0.01       14,848.0        7,424.0     15312.0      14848.0       0.14%     30160.0\n",
      "424          layer3.16.conv2   58   8   8   60   8   8    31320.0       0.01    4,005,120.0    2,004,480.0    140128.0      15360.0       0.28%    155488.0\n",
      "425            layer3.16.bn3   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.14%     31200.0\n",
      "426          layer3.16.conv3   60   8   8  256   8   8    15360.0       0.06    1,949,696.0      983,040.0     76800.0      65536.0       0.28%    142336.0\n",
      "427           layer3.16.relu   60   8   8   60   8   8        0.0       0.01        3,840.0        3,840.0     15360.0      15360.0       0.28%     30720.0\n",
      "428            layer3.17.bn1  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.28%    133120.0\n",
      "429         layer3.17.select  256   8   8   47   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.70%         0.0\n",
      "430          layer3.17.conv1   47   8   8   60   8   8     2820.0       0.01      357,120.0      180,480.0     23312.0      15360.0       0.14%     38672.0\n",
      "431            layer3.17.bn2   60   8   8   60   8   8      120.0       0.01       15,360.0        7,680.0     15840.0      15360.0       0.28%     31200.0\n",
      "432          layer3.17.conv2   60   8   8   61   8   8    32940.0       0.01    4,212,416.0    2,108,160.0    147120.0      15616.0       0.28%    162736.0\n",
      "433            layer3.17.bn3   61   8   8   61   8   8      122.0       0.01       15,616.0        7,808.0     16104.0      15616.0       0.14%     31720.0\n",
      "434          layer3.17.conv3   61   8   8  256   8   8    15616.0       0.06    1,982,464.0      999,424.0     78080.0      65536.0       0.14%    143616.0\n",
      "435           layer3.17.relu   61   8   8   61   8   8        0.0       0.01        3,904.0        3,904.0     15616.0      15616.0       0.14%     31232.0\n",
      "436                       bn  256   8   8  256   8   8      512.0       0.06       65,536.0       32,768.0     67584.0      65536.0       0.14%    133120.0\n",
      "437                   select  256   8   8   24   8   8      256.0       0.01            0.0            0.0         0.0          0.0       0.56%         0.0\n",
      "438                     relu   24   8   8   24   8   8        0.0       0.01        1,536.0        1,536.0      6144.0       6144.0       0.14%     12288.0\n",
      "439                  avgpool   24   8   8   24   1   1        0.0       0.00        1,536.0        1,536.0      6144.0         96.0       0.14%      6240.0\n",
      "440                       fc           24          100     2500.0       0.00        4,700.0        2,400.0     10096.0        400.0       0.28%     10496.0\n",
      "total                                                   1184869.0      25.32  303,501,532.0  153,464,672.0     10096.0        400.0     100.00%  48200612.0\n",
      "===========================================================================================================================================================\n",
      "Total params: 1,184,869\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 25.32MB\n",
      "Total MAdd: 303.5MMAdd\n",
      "Total Flops: 153.46MFlops\n",
      "Total MemR+W: 45.97MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchstat import stat\n",
    "\n",
    "stat(model, (3, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python37] *",
   "language": "python",
   "name": "conda-env-python37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
