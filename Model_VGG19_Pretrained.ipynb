{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99013a1d",
   "metadata": {},
   "source": [
    "#### 0. 1 cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40fb7785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f043c",
   "metadata": {},
   "source": [
    "#### 0.2 pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b716d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759314a0",
   "metadata": {},
   "source": [
    "### 1. Pretrained VGG19 Model with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685eca0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg19_bn(pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.9.0', 'vgg19_bn', pretrained=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5df78c",
   "metadata": {},
   "source": [
    "#### 1.1 VGG19 Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32582874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431e8569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[1].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60792d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5763e-07,  9.8841e-08,  2.4442e-01,  2.2678e-01,  2.4756e-01,\n",
       "         5.9402e-07,  1.5344e-01,  5.5348e-01,  1.8775e-01,  4.0541e-08,\n",
       "         7.3613e-08,  6.7899e-07,  1.9412e-08,  3.7765e-07, -1.5930e-07,\n",
       "         4.6782e-01, -5.3255e-07,  1.8139e-02,  1.9039e-08,  2.0468e-07,\n",
       "        -3.8896e-05,  8.2928e-07,  3.6763e-01, -5.7934e-08,  1.0634e-07,\n",
       "         7.7692e-09, -1.2172e-07,  1.0209e-07,  1.0130e-07,  3.9670e-08,\n",
       "         4.9515e-08,  6.8137e-07,  1.0767e-07,  6.3167e-08,  5.1813e-01,\n",
       "         3.9662e-01,  4.0801e-06,  3.2676e-01,  3.0591e-08,  1.4233e-05,\n",
       "        -4.0759e-07,  2.1676e-04,  1.8606e-06,  2.0022e-08,  6.8042e-07,\n",
       "         1.0519e-06,  3.0301e-07,  1.7505e-08,  1.4339e-07,  2.6455e-08,\n",
       "         3.7188e-01, -1.0622e-06,  1.9643e-01,  6.8300e-01,  1.8162e-01,\n",
       "        -1.1994e-07,  3.4379e-01,  1.2878e-04,  6.3663e-01,  6.3070e-01,\n",
       "         2.0438e-07,  1.0555e-08,  1.4975e-08,  3.4700e-06])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[1].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf406ad",
   "metadata": {},
   "source": [
    "#### 1.2 VGG19 Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5805e963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b640d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf05f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.2130e-08, -3.1717e-08, -6.7120e-08],\n",
       "          [-2.9321e-08, -2.1596e-08, -5.3207e-08],\n",
       "          [-5.3304e-08, -4.0832e-08, -6.6434e-08]],\n",
       "\n",
       "         [[ 1.3710e-07,  1.2848e-07,  7.1270e-08],\n",
       "          [ 1.5487e-07,  1.5186e-07,  8.3582e-08],\n",
       "          [ 9.3392e-08,  9.9451e-08,  4.1284e-08]],\n",
       "\n",
       "         [[ 1.7435e-07,  1.7840e-07,  1.0561e-07],\n",
       "          [ 1.9644e-07,  1.9769e-07,  1.2560e-07],\n",
       "          [ 1.1935e-07,  1.3447e-07,  5.9638e-08]]],\n",
       "\n",
       "\n",
       "        [[[-6.3467e-08, -5.2714e-08, -3.8511e-08],\n",
       "          [-1.4349e-08, -9.0426e-09, -1.7164e-08],\n",
       "          [ 2.2282e-08,  1.9084e-08,  1.8150e-09]],\n",
       "\n",
       "         [[-1.2191e-08, -6.1933e-09,  8.5022e-09],\n",
       "          [ 3.8618e-08,  5.8869e-08,  2.2082e-08],\n",
       "          [ 6.2501e-08,  7.0528e-08,  2.7185e-08]],\n",
       "\n",
       "         [[ 3.0045e-08,  3.8003e-08,  3.5389e-08],\n",
       "          [ 8.6743e-08,  1.1626e-07,  8.0502e-08],\n",
       "          [ 7.9653e-08,  1.0057e-07,  6.7894e-08]]],\n",
       "\n",
       "\n",
       "        [[[-1.3565e-01, -2.9960e-01, -1.5151e-01],\n",
       "          [-2.6804e-01, -3.0505e-01, -9.4600e-02],\n",
       "          [-1.4124e-01, -8.1448e-02,  1.7112e-01]],\n",
       "\n",
       "         [[ 2.0333e-01,  4.0123e-01,  1.3747e-01],\n",
       "          [ 4.2157e-01,  6.7595e-01,  3.2725e-01],\n",
       "          [ 1.3103e-01,  3.2654e-01,  6.4284e-02]],\n",
       "\n",
       "         [[-4.1092e-02,  4.6723e-02, -9.7236e-03],\n",
       "          [ 2.9558e-02,  3.5226e-02, -1.2338e-01],\n",
       "          [-9.1504e-02, -1.7836e-01, -3.1635e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-5.0743e-08, -6.6759e-08, -7.8941e-08],\n",
       "          [-4.6060e-08, -7.9228e-08, -7.3020e-08],\n",
       "          [-7.2354e-08, -7.5888e-08, -7.5602e-08]],\n",
       "\n",
       "         [[-1.8643e-08, -5.4891e-08, -5.5029e-08],\n",
       "          [-2.9716e-08, -6.0283e-08, -5.9039e-08],\n",
       "          [-5.4538e-08, -5.7847e-08, -5.4300e-08]],\n",
       "\n",
       "         [[-9.0202e-09, -4.0853e-08, -4.0702e-08],\n",
       "          [-2.0145e-08, -4.0434e-08, -4.7582e-08],\n",
       "          [-4.9317e-08, -4.9057e-08, -4.8955e-08]]],\n",
       "\n",
       "\n",
       "        [[[-7.8860e-09,  4.4172e-09,  4.1371e-08],\n",
       "          [-9.1843e-09,  4.9512e-09,  6.3214e-08],\n",
       "          [ 1.2759e-09,  2.6705e-08,  6.1890e-08]],\n",
       "\n",
       "         [[-1.6242e-09, -5.1753e-10,  3.5820e-08],\n",
       "          [-1.5325e-08,  9.5484e-09,  6.6321e-08],\n",
       "          [ 5.8824e-09,  3.6904e-08,  7.9135e-08]],\n",
       "\n",
       "         [[ 7.1371e-09,  3.5890e-09,  2.7016e-08],\n",
       "          [-2.0624e-09,  1.1004e-08,  5.9306e-08],\n",
       "          [ 2.0382e-08,  3.8491e-08,  6.5097e-08]]],\n",
       "\n",
       "\n",
       "        [[[-5.1288e-07,  4.4387e-06,  2.8007e-06],\n",
       "          [ 1.4936e-06,  6.4549e-06,  4.6987e-06],\n",
       "          [ 1.3737e-06,  3.8875e-06,  3.2473e-06]],\n",
       "\n",
       "         [[-2.3703e-06,  3.5919e-06,  1.5149e-06],\n",
       "          [ 6.3111e-07,  6.6647e-06,  4.2479e-06],\n",
       "          [-2.4147e-07,  2.5924e-06,  1.3272e-06]],\n",
       "\n",
       "         [[ 4.0485e-06,  1.1047e-05,  7.5509e-06],\n",
       "          [ 7.4615e-06,  1.4459e-05,  1.0637e-05],\n",
       "          [ 4.2098e-06,  7.7080e-06,  5.1014e-06]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ac1bc",
   "metadata": {},
   "source": [
    "### 2. Pretrained VGG19 Model without Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95428969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Administrator/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = models.vgg19(pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.9.0', 'vgg19', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3b274",
   "metadata": {},
   "source": [
    "### 2.1 features[0]: Conv2d\n",
    "\n",
    "- 输入通道为RGB彩色图像三通道，输出通道为64，卷积核大小为3×3\n",
    "- Conv2d weight：3个卷积核为一组，一共有64组卷积核，即 `model.features[0].weight.shape = [64, 3, 3, 3]`\n",
    "- Conv2d bias：3个卷积核为一组，卷积核的结果相加，产生一个通道的结果。一共有64组，使用偏置对其调整，即 `model.features[0].bias.shape=[64]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d7c12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ef92ce2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514ada80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0].bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a5a2e",
   "metadata": {},
   "source": [
    "### 2.2 features[2]: Conv2d \n",
    "\n",
    "- 输入通道为64，输出通道为64，卷积核大小为3×3\n",
    "- Conv2d weight：64个3×3的卷积核为一组，一共有64组，即 `model.features[2].weight.shape=[64, 64, 3, 3]`\n",
    "- Conv2d bias: 64个3×3的卷积核为一组，一共有64组，每一组内对应位置求和，即`model.features[2].bias.shape=[64]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "257a02df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2c0693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[2].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5d53ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[2].bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d71f62",
   "metadata": {},
   "source": [
    "### 2.3 Prune Conv2d Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3120c35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-5.8394e-02, -1.4754e-01,  1.8017e-01, -2.8633e-01,  1.4053e-02,\n",
       "         -3.9571e-02,  1.0259e-01, -8.8048e-04, -7.7598e-02, -1.8744e-01,\n",
       "         -3.4763e-02,  2.2564e-02, -6.1512e-02, -3.9896e-01, -2.3920e-01,\n",
       "          1.1534e-01, -1.1614e-01, -2.2889e-01, -1.6896e-01, -2.4117e-01,\n",
       "          5.9063e-01,  4.4781e-04, -3.8955e-02, -8.2188e-01, -1.5366e-01,\n",
       "          2.2464e-02,  4.2782e-01, -5.8604e-02, -4.8700e-02,  3.0054e-01,\n",
       "         -5.6536e-02,  5.2338e-02,  2.4208e-01, -3.2136e-03,  4.8011e-01,\n",
       "         -8.3752e-03,  1.7856e-01,  3.9528e-01, -2.2767e-02,  5.7821e-02,\n",
       "          6.5798e-04, -1.3895e-01, -1.6600e-01,  7.9103e-03,  1.1497e-01,\n",
       "          1.0045e-01,  2.3931e-01,  4.0163e-01, -1.3901e-01, -4.5015e-01,\n",
       "          1.9804e-01, -1.4634e-01, -1.1509e-01,  7.2199e-02, -1.0609e-02,\n",
       "          3.5647e-01,  9.0835e-02, -1.5250e-01,  2.1137e-01, -2.0729e-01,\n",
       "          1.1017e-02,  2.3209e-01,  9.1408e-02,  2.5651e-02],\n",
       "        grad_fn=<CloneBackward>),\n",
       " tensor([0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "         1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "         0., 1., 1., 0., 1., 0., 1., 1., 1., 1.]),\n",
       " tensor([-0.0000e+00, -0.0000e+00, 1.8017e-01, -0.0000e+00, 1.4053e-02, -0.0000e+00,\n",
       "         1.0259e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 2.2564e-02,\n",
       "         -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1534e-01, -0.0000e+00, -0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00, 5.9063e-01, 4.4781e-04, -0.0000e+00, -0.0000e+00,\n",
       "         -0.0000e+00, 2.2464e-02, 4.2782e-01, -0.0000e+00, -0.0000e+00, 3.0054e-01,\n",
       "         -0.0000e+00, 5.2338e-02, 2.4208e-01, -0.0000e+00, 4.8011e-01, -0.0000e+00,\n",
       "         1.7856e-01, 3.9528e-01, -0.0000e+00, 5.7821e-02, 6.5798e-04, -0.0000e+00,\n",
       "         -0.0000e+00, 7.9103e-03, 1.1497e-01, 1.0045e-01, 2.3931e-01, 4.0163e-01,\n",
       "         -0.0000e+00, -0.0000e+00, 1.9804e-01, -0.0000e+00, -0.0000e+00, 7.2199e-02,\n",
       "         -0.0000e+00, 3.5647e-01, 9.0835e-02, -0.0000e+00, 2.1137e-01, -0.0000e+00,\n",
       "         1.1017e-02, 2.3209e-01, 9.1408e-02, 2.5651e-02],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = model.features[2].bias.clone()\n",
    "mask = bias.gt(0.).float()\n",
    "bias_mask = bias.mul(mask)\n",
    "bias, mask, bias_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da38c92",
   "metadata": {},
   "source": [
    "### 2.4 Prune Conv2d Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3b52f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = model.features[2].weight.clone()\n",
    "num_channel = weight.shape[0]\n",
    "mask = torch.randn(num_channel).gt(0.).float()\n",
    "for i in range(num_channel):\n",
    "    if mask[i] <= 0:\n",
    "        weight[i] = torch.zeros()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
