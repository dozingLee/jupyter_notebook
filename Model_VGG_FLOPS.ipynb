{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  VGG FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "default_cfg = {\n",
    "    11: [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "    13: [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "    16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n",
    "    19: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n",
    "}\n",
    "\n",
    "\n",
    "def make_layers(cfg):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n",
    "            layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class vgg(nn.Module):\n",
    "    def __init__(self, dataset='cifar10', depth=19, cfg=None, conv_cfg=None):\n",
    "        \"\"\"\n",
    "        note: default initialize weight & have batch normalization\n",
    "\n",
    "        :param dataset: `cifar10` or `cifar100`\n",
    "        :param depth: `11`, `13`, '16', or `19` (default)\n",
    "        :param cfg: vgg model convolutional layer's channel config\n",
    "        :param conv_cfg:\n",
    "            like [2, 4, 8, 12]\n",
    "            return convolutional layer's channel config (index starts at 1)\n",
    "\n",
    "        \"\"\"\n",
    "        super(vgg, self).__init__()\n",
    "\n",
    "        # model value\n",
    "        self.conv_cfg = conv_cfg\n",
    "\n",
    "        # model config\n",
    "        if cfg is None:\n",
    "            cfg = default_cfg[depth]\n",
    "\n",
    "        # model feature\n",
    "        self.feature = make_layers(cfg)\n",
    "\n",
    "        # model classifier\n",
    "        if dataset == 'cifar10':\n",
    "            num_classes = 10\n",
    "        elif dataset == 'cifar100':\n",
    "            num_classes = 100\n",
    "        else:\n",
    "            raise ValueError('Model `dataset` parameter is Error!')\n",
    "        self.classifier = nn.Linear(cfg[-1], num_classes)\n",
    "\n",
    "        # model initialize weight\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(0.5)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # model feature\n",
    "        conv_value = []\n",
    "        if self.conv_cfg:\n",
    "            conv_idx = 0\n",
    "            for k, m in enumerate(self.feature):\n",
    "                x = m(x)\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    conv_idx += 1\n",
    "                    if conv_idx in self.conv_cfg:\n",
    "                        conv_value.append(x.clone())\n",
    "        else:\n",
    "            x = self.feature(x)\n",
    "\n",
    "        # model classifier\n",
    "        x = nn.AvgPool2d(2)(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y = self.classifier(x)\n",
    "\n",
    "        # return value\n",
    "        if len(conv_value):\n",
    "            return y, conv_value\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'D:\\Project\\Pycharm\\network-slimming\\logs\\ft_inherit_at_vgg19_cifar100_percent_0.5_seed_2\\model_best.pth.tar'\n",
      "-> model cfg is loading...\n",
      "=>  epoch 151 Prec1: 0.726200 cfg: [21, 60, 'M', 107, 125, 'M', 240, 229, 201, 204, 'M', 315, 241, 141, 171, 'M', 149, 144, 165, 238]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resume_model(resume_file):\n",
    "    if not os.path.isfile(resume_file):\n",
    "        raise ValueError(\"Resume model file is not found at '{}'\".format(resume_file))\n",
    "    print(\"=> loading checkpoint '{}'\".format(resume_file))\n",
    "    checkpoint = torch.load(resume_file)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    opti_dict = checkpoint['optimizer']\n",
    "    if 'cfg' in checkpoint:\n",
    "        cfg = checkpoint['cfg']\n",
    "        print(\"-> model cfg is loading...\")\n",
    "    else:\n",
    "        cfg = None\n",
    "        print(\"-> not found model cfg...\")\n",
    "    print(\"=>  epoch {} Prec1: {:f} cfg: {}\"\n",
    "          .format(start_epoch, best_prec1, list(cfg)))\n",
    "    return state_dict, opti_dict, start_epoch, best_prec1, cfg\n",
    "\n",
    "\n",
    "root_path = r'D:\\Project\\Pycharm\\network-slimming\\logs'\n",
    "file_name = 'model_best.pth.tar'\n",
    "name = [\n",
    "    'ft_inherit_bn_vgg19_cifar10_percent_0.7_seed_2', \n",
    "    'ft_inherit_bn_vgg19_cifar100_percent_0.5_seed_2',\n",
    "    'ft_inherit_at_vgg19_cifar10_percent_0.7_seed_2',\n",
    "    'ft_inherit_at_vgg19_cifar100_percent_0.5_seed_2'\n",
    "]\n",
    "\n",
    "\n",
    "file_path = os.path.join(root_path, name[3], file_name)\n",
    "state_dict, opti_dict, start_epoch, best_prec1, cfg = resume_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg(\n",
       "  (feature): Sequential(\n",
       "    (0): Conv2d(3, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(21, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(60, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(107, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(125, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(240, 229, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (18): BatchNorm2d(229, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(229, 201, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (21): BatchNorm2d(201, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(201, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (24): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(204, 315, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (28): BatchNorm2d(315, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(315, 241, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (31): BatchNorm2d(241, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(241, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (34): BatchNorm2d(141, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(141, 171, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (37): BatchNorm2d(171, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(171, 149, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (41): BatchNorm2d(149, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(149, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (44): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(144, 165, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (47): BatchNorm2d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(165, 238, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (50): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=238, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vgg(depth=19, dataset='cifar100', cfg=cfg, conv_cfg=None)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      module name  input shape output shape     params memory(MB)           MAdd          Flops  MemRead(B)  MemWrite(B) duration[%]   MemR+W(B)\n",
      "0       feature.0    3  32  32   21  32  32      567.0       0.08    1,139,712.0      580,608.0     14556.0      86016.0       3.25%    100572.0\n",
      "1       feature.1   21  32  32   21  32  32       42.0       0.08       86,016.0       43,008.0     86184.0      86016.0       1.62%    172200.0\n",
      "2       feature.2   21  32  32   21  32  32        0.0       0.08       21,504.0       21,504.0     86016.0      86016.0       1.63%    172032.0\n",
      "3       feature.3   21  32  32   60  32  32    11340.0       0.23   23,162,880.0   11,612,160.0    131376.0     245760.0       4.87%    377136.0\n",
      "4       feature.4   60  32  32   60  32  32      120.0       0.23      245,760.0      122,880.0    246240.0     245760.0       0.00%    492000.0\n",
      "5       feature.5   60  32  32   60  32  32        0.0       0.23       61,440.0       61,440.0    245760.0     245760.0       1.62%    491520.0\n",
      "6       feature.6   60  32  32   60  16  16        0.0       0.06       46,080.0       61,440.0    245760.0      61440.0       1.62%    307200.0\n",
      "7       feature.7   60  16  16  107  16  16    57780.0       0.10   29,555,968.0   14,791,680.0    292560.0     109568.0       4.88%    402128.0\n",
      "8       feature.8  107  16  16  107  16  16      214.0       0.10      109,568.0       54,784.0    110424.0     109568.0       1.62%    219992.0\n",
      "9       feature.9  107  16  16  107  16  16        0.0       0.10       27,392.0       27,392.0    109568.0     109568.0       0.00%    219136.0\n",
      "10     feature.10  107  16  16  125  16  16   120375.0       0.12   61,600,000.0   30,816,000.0    591068.0     128000.0       6.49%    719068.0\n",
      "11     feature.11  125  16  16  125  16  16      250.0       0.12      128,000.0       64,000.0    129000.0     128000.0       1.62%    257000.0\n",
      "12     feature.12  125  16  16  125  16  16        0.0       0.12       32,000.0       32,000.0    128000.0     128000.0       1.67%    256000.0\n",
      "13     feature.13  125  16  16  125   8   8        0.0       0.03       24,000.0       32,000.0    128000.0      32000.0       3.20%    160000.0\n",
      "14     feature.14  125   8   8  240   8   8   270000.0       0.06   34,544,640.0   17,280,000.0   1112000.0      61440.0       6.50%   1173440.0\n",
      "15     feature.15  240   8   8  240   8   8      480.0       0.06       61,440.0       30,720.0     63360.0      61440.0       1.62%    124800.0\n",
      "16     feature.16  240   8   8  240   8   8        0.0       0.06       15,360.0       15,360.0     61440.0      61440.0       1.62%    122880.0\n",
      "17     feature.17  240   8   8  229   8   8   494640.0       0.06   63,299,264.0   31,656,960.0   2040000.0      58624.0       9.74%   2098624.0\n",
      "18     feature.18  229   8   8  229   8   8      458.0       0.06       58,624.0       29,312.0     60456.0      58624.0       1.63%    119080.0\n",
      "19     feature.19  229   8   8  229   8   8        0.0       0.06       14,656.0       14,656.0     58624.0      58624.0       0.00%    117248.0\n",
      "20     feature.20  229   8   8  201   8   8   414261.0       0.05   53,012,544.0   26,512,704.0   1715668.0      51456.0       6.49%   1767124.0\n",
      "21     feature.21  201   8   8  201   8   8      402.0       0.05       51,456.0       25,728.0     53064.0      51456.0       1.62%    104520.0\n",
      "22     feature.22  201   8   8  201   8   8        0.0       0.05       12,864.0       12,864.0     51456.0      51456.0       1.63%    102912.0\n",
      "23     feature.23  201   8   8  204   8   8   369036.0       0.05   47,223,552.0   23,618,304.0   1527600.0      52224.0       4.87%   1579824.0\n",
      "24     feature.24  204   8   8  204   8   8      408.0       0.05       52,224.0       26,112.0     53856.0      52224.0       1.62%    106080.0\n",
      "25     feature.25  204   8   8  204   8   8        0.0       0.05       13,056.0       13,056.0     52224.0      52224.0       0.00%    104448.0\n",
      "26     feature.26  204   8   8  204   4   4        0.0       0.01        9,792.0       13,056.0     52224.0      13056.0       1.75%     65280.0\n",
      "27     feature.27  204   4   4  315   4   4   578340.0       0.02   18,501,840.0    9,253,440.0   2326416.0      20160.0       0.00%   2346576.0\n",
      "28     feature.28  315   4   4  315   4   4      630.0       0.02       20,160.0       10,080.0     22680.0      20160.0       0.00%     42840.0\n",
      "29     feature.29  315   4   4  315   4   4        0.0       0.02        5,040.0        5,040.0     20160.0      20160.0       0.00%     40320.0\n",
      "30     feature.30  315   4   4  241   4   4   683235.0       0.01   21,859,664.0   10,931,760.0   2753100.0      15424.0       0.00%   2768524.0\n",
      "31     feature.31  241   4   4  241   4   4      482.0       0.01       15,424.0        7,712.0     17352.0      15424.0       0.00%     32776.0\n",
      "32     feature.32  241   4   4  241   4   4        0.0       0.01        3,856.0        3,856.0     15424.0      15424.0       0.00%     30848.0\n",
      "33     feature.33  241   4   4  141   4   4   305829.0       0.01    9,784,272.0    4,893,264.0   1238740.0       9024.0       0.00%   1247764.0\n",
      "34     feature.34  141   4   4  141   4   4      282.0       0.01        9,024.0        4,512.0     10152.0       9024.0       0.81%     19176.0\n",
      "35     feature.35  141   4   4  141   4   4        0.0       0.01        2,256.0        2,256.0      9024.0       9024.0       0.00%     18048.0\n",
      "36     feature.36  141   4   4  171   4   4   216999.0       0.01    6,941,232.0    3,471,984.0    877020.0      10944.0       0.00%    887964.0\n",
      "37     feature.37  171   4   4  171   4   4      342.0       0.01       10,944.0        5,472.0     12312.0      10944.0       0.00%     23256.0\n",
      "38     feature.38  171   4   4  171   4   4        0.0       0.01        2,736.0        2,736.0     10944.0      10944.0       0.00%     21888.0\n",
      "39     feature.39  171   4   4  171   2   2        0.0       0.00        2,052.0        2,736.0     10944.0       2736.0       0.00%     13680.0\n",
      "40     feature.40  171   2   2  149   2   2   229311.0       0.00    1,833,892.0      917,244.0    919980.0       2384.0       0.00%    922364.0\n",
      "41     feature.41  149   2   2  149   2   2      298.0       0.00        2,384.0        1,192.0      3576.0       2384.0       0.00%      5960.0\n",
      "42     feature.42  149   2   2  149   2   2        0.0       0.00          596.0          596.0      2384.0       2384.0       0.00%      4768.0\n",
      "43     feature.43  149   2   2  144   2   2   193104.0       0.00    1,544,256.0      772,416.0    774800.0       2304.0       0.00%    777104.0\n",
      "44     feature.44  144   2   2  144   2   2      288.0       0.00        2,304.0        1,152.0      3456.0       2304.0       0.00%      5760.0\n",
      "45     feature.45  144   2   2  144   2   2        0.0       0.00          576.0          576.0      2304.0       2304.0      25.98%      4608.0\n",
      "46     feature.46  144   2   2  165   2   2   213840.0       0.00    1,710,060.0      855,360.0    857664.0       2640.0       0.00%    860304.0\n",
      "47     feature.47  165   2   2  165   2   2      330.0       0.00        2,640.0        1,320.0      3960.0       2640.0       0.00%      6600.0\n",
      "48     feature.48  165   2   2  165   2   2        0.0       0.00          660.0          660.0      2640.0       2640.0       0.00%      5280.0\n",
      "49     feature.49  165   2   2  238   2   2   353430.0       0.00    2,826,488.0    1,413,720.0   1416360.0       3808.0       0.00%   1420168.0\n",
      "50     feature.50  238   2   2  238   2   2      476.0       0.00        3,808.0        1,904.0      5712.0       3808.0       0.00%      9520.0\n",
      "51     feature.51  238   2   2  238   2   2        0.0       0.00          952.0          952.0      3808.0       3808.0       0.00%      7616.0\n",
      "52     classifier          238          100    23900.0       0.00       47,500.0       23,800.0     96552.0        400.0       0.00%     96952.0\n",
      "total                                        4541489.0       2.56  379,744,408.0  190,155,468.0     96552.0        400.0     100.00%  23552908.0\n",
      "================================================================================================================================================\n",
      "Total params: 4,541,489\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 2.56MB\n",
      "Total MAdd: 379.74MMAdd\n",
      "Total Flops: 190.16MFlops\n",
      "Total MemR+W: 22.46MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchstat import stat\n",
    "\n",
    "stat(model, (3, 32, 32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python37] *",
   "language": "python",
   "name": "conda-env-python37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
