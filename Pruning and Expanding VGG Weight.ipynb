{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning and Expanding VGG Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "            datasets.CIFAR10('./data.cifar10', train=False, download=True, transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
    "            batch_size=256, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 one batch dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 3, 32, 32]), torch.Size([256]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, data = next(enumerate(dataloader))\n",
    "data_batch = data[0].clone()\n",
    "value_shape, data_shape = data[0].shape, data[1].shape\n",
    "value_shape, data_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 one item of one batch dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx, data_idx = 0, 6\n",
    "\n",
    "data1 = data[batch_idx][data_idx].clone().unsqueeze(0)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. VGG19 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 vgg config strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['vgg']\n",
    "\n",
    "defaultcfg = {\n",
    "    11: [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "    13: [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "    16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n",
    "    19: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n",
    "}\n",
    "\n",
    "\n",
    "class vgg(nn.Module):\n",
    "    def __init__(self, dataset='cifar10', depth=19, init_weights=True, cfg=None, batch_norm=True):\n",
    "        super(vgg, self).__init__()\n",
    "        if cfg is None:\n",
    "            cfg = defaultcfg[depth]\n",
    "\n",
    "        self.feature = self.make_layers(cfg, batch_norm)\n",
    "\n",
    "        if dataset == 'cifar10':\n",
    "            num_classes = 10\n",
    "        elif dataset == 'cifar100':\n",
    "            num_classes = 100\n",
    "        self.classifier = nn.Linear(cfg[-1], num_classes)\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def make_layers(self, cfg, batch_norm=False):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = nn.AvgPool2d(2)(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y = self.classifier(x)\n",
    "        return y\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(0.5)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()## Attention-based Feature Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 instance vgg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg(\n",
       "  (feature): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vgg(dataset='cifar10', depth=19)\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 initialize model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.0054, -0.0220, -0.0080],\n",
       "           [-0.0397,  0.0566,  0.0228],\n",
       "           [ 0.0515,  0.0953, -0.0053]],\n",
       " \n",
       "          [[ 0.0369,  0.0198, -0.0199],\n",
       "           [-0.0630,  0.0354,  0.0105],\n",
       "           [ 0.1169,  0.0763,  0.0212]],\n",
       " \n",
       "          [[-0.0721,  0.0405, -0.0013],\n",
       "           [ 0.0713,  0.0088, -0.0064],\n",
       "           [ 0.0437,  0.0748,  0.0370]]]]),\n",
       " tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._initialize_weights()\n",
    "model.feature[0].weight.data[:1], model.feature[1].weight.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 load model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Project/Pycharm/network-slimming/logs/sparsity_vgg19_cifar10_s_1e-4\\\\model_best.pth.tar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "url_path = 'D:/Project/Pycharm/network-slimming/logs/'\n",
    "\n",
    "baseline_vgg19_cifar10 = 'baseline_vgg19_cifar10'\n",
    "baseline_vgg19_cifar100 = 'baseline_vgg19_cifar100'\n",
    "sparsity_vgg19_cifar10_s_1e_4 = 'sparsity_vgg19_cifar10_s_1e-4'\n",
    "sparsity_resnet_cifar10_s_1e_5 = 'sparsity_resnet_cifar10_s_1e-5'\n",
    "\n",
    "fine_tune_expand_more_vgg19_cifar100_percent_0_5 = 'fine_tune_expand_more_vgg19_cifar100_percent_0.5'\n",
    "fine_tune_expand_vgg19_cifar100_percent_0_5 = 'fine_tune_expand_vgg19_cifar100_percent_0.5'\n",
    "fine_tune_expand_vgg19_percent_0_7 = 'fine_tune_expand_vgg19_percent_0.7'\n",
    "\n",
    "model_name = 'model_best.pth.tar'\n",
    "\n",
    "model_path = os.path.join(url_path, sparsity_vgg19_cifar10_s_1e_4, model_name)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, tensor(0.9347))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "epoch1 = checkpoint['epoch']\n",
    "best_prec1 = checkpoint['best_prec1']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "epoch1, best_prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.3343e-04, -9.5322e-04, -1.4197e-03],\n",
       "           [-1.6418e-03, -2.4356e-03, -1.2043e-03],\n",
       "           [-5.6899e-04,  2.8932e-04,  6.0596e-04]],\n",
       " \n",
       "          [[ 4.9176e-04,  6.0720e-05,  1.3321e-04],\n",
       "           [-1.1490e-03, -1.2976e-03,  5.5458e-05],\n",
       "           [ 7.4775e-05,  1.2027e-03,  1.5041e-03]],\n",
       " \n",
       "          [[ 5.2284e-04, -1.3844e-04, -1.8975e-05],\n",
       "           [-1.2763e-03, -1.6820e-03, -6.6350e-04],\n",
       "           [-8.2281e-04, -1.8182e-04,  8.5114e-05]]]]),\n",
       " tensor([-1.9593e-07,  2.1987e-01,  3.1219e-01, -6.0219e-08,  2.6633e-01]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature[0].weight.data[:1], model.feature[1].weight.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prune Weight Alogrithm\n",
    "    basic attention-based gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 裁剪 Feature Map，包含 batch size，沿着该方向求平均值\n",
    "def activation_based_gamma_batch(weight_data):\n",
    "    d0, d1, d2 = weight_data.shape[0], weight_data.shape[1], weight_data.shape[2]\n",
    "    A = weight_data.view(d0, d1, d2, -1)\n",
    "#     return activation_based_gamma(torch.sum(A, dim=0))  # 返回所有batch的和\n",
    "    return activation_based_gamma(torch.mean(A, dim=0))  # 返回所有batch的平均值\n",
    "\n",
    "\n",
    "# 裁剪 Conv2d Weight ，不需要 batch size\n",
    "def activation_based_gamma(weight_data):\n",
    "    d1, d2 = weight_data.shape[0], weight_data.shape[1]\n",
    "    \n",
    "    # 1. A: feature map data\n",
    "    A = weight_data.view(d1, d2, -1).abs()\n",
    "    c, h, w = A.shape\n",
    "    \n",
    "    # 2. Fsum(A): sum of values along the channel direction\n",
    "    FsumA = torch.sum(A, dim=0)\n",
    "        \n",
    "    # 3. ||Fsum(A)||2: two norm\n",
    "    FsumA_norm = torch.linalg.norm(FsumA)\n",
    "    \n",
    "    # 4. F(A) / ||F(A)||2: normalize weight data\n",
    "    F_all = FsumA / FsumA_norm\n",
    "    \n",
    "    # 5. F(Aj) / ||F(Aj)||^2 & gamma = ∑ | F(A) / ||F(A)||2 - F(Aj) / ||F(Aj)||2 |\n",
    "    gamma = torch.zeros(c)\n",
    "    for j in range(c):\n",
    "        FAj = FsumA - A[j]\n",
    "        FAj_norm = torch.linalg.norm(FAj)\n",
    "        Fj = FAj / FAj_norm\n",
    "#         gamma[j] = (F_all - Fj).abs().sum()   # L1-norm\n",
    "        gamma[j] = torch.linalg.norm(F_all - Fj, ord=2)  # L2-norm\n",
    "\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 observe better algorithm according to random data\n",
    "    feature map(batch_size, num_channel, feature_width, feature_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "    '0': torch.randn(64, 512, 4, 4), \n",
    "    '1': torch.randn(64, 512, 8, 8),\n",
    "    '2': torch.randn(64, 512, 16, 16),\n",
    "    \n",
    "    '3': torch.randn(64, 256, 4, 4),\n",
    "    '4': torch.randn(64, 256, 8, 8),\n",
    "    '5': torch.randn(64, 256, 16, 16),\n",
    "    \n",
    "    '6': torch.randn(64, 128, 4, 4),\n",
    "    '7': torch.randn(64, 128, 8, 8),\n",
    "    '8': torch.randn(64, 128, 16, 16)\n",
    "}\n",
    "\n",
    "y = {\n",
    "    '0': torch.randn(64, 512, 4, 4), \n",
    "    '3': torch.randn(64, 256, 4, 4),\n",
    "    '6': torch.randn(64, 128, 4, 4),\n",
    "    \n",
    "    '1': torch.randn(64, 512, 8, 8),\n",
    "    '4': torch.randn(64, 256, 8, 8),\n",
    "    '7': torch.randn(64, 128, 8, 8),\n",
    "    \n",
    "    '2': torch.randn(64, 512, 16, 16),\n",
    "    '5': torch.randn(64, 256, 16, 16),\n",
    "    '8': torch.randn(64, 128, 16, 16)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 L1-norm \n",
    "\n",
    "$$\n",
    "\\gamma = \\sum \\left| \\frac{F(A)}{\\|F(A)\\|_2} - \\frac{F(A_j)}{\\|F(A_j)\\|_2} \\right|\n",
    "$$\n",
    "\n",
    "    只要 Feature map 的`(feature_width, feature_height)`一致，那么`num_channel`的`sum`就是一样的，即在固定`(feature_width, feature_height)`的情况下，`num_channel`越大，每个channel占比的影响就越大。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma[j] = (F_all - Fj).abs().sum()\n",
      "torch.Size([64, 512, 4, 4]) tensor(2.3386) tensor(0.0046)\n",
      "torch.Size([64, 512, 8, 8]) tensor(4.8243) tensor(0.0094)\n",
      "torch.Size([64, 512, 16, 16]) tensor(9.6430) tensor(0.0188)\n",
      "torch.Size([64, 256, 4, 4]) tensor(2.3424) tensor(0.0092)\n",
      "torch.Size([64, 256, 8, 8]) tensor(4.8285) tensor(0.0189)\n",
      "torch.Size([64, 256, 16, 16]) tensor(9.6434) tensor(0.0377)\n",
      "torch.Size([64, 128, 4, 4]) tensor(2.3575) tensor(0.0184)\n",
      "torch.Size([64, 128, 8, 8]) tensor(4.7923) tensor(0.0374)\n",
      "torch.Size([64, 128, 16, 16]) tensor(9.6803) tensor(0.0756)\n"
     ]
    }
   ],
   "source": [
    "print('gamma[j] = (F_all - Fj).abs().sum()')\n",
    "for key in x:\n",
    "    based_gamma = activation_based_gamma_batch(x[key])\n",
    "    print(x[key].shape, torch.sum(based_gamma), torch.mean(based_gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 L2-norm\n",
    "\n",
    "$$\n",
    "\\gamma = \\left\\| \\frac{F(A)}{\\|F(A)\\|_2} - \\frac{F(A_j)}{\\|F(A_j)\\|_2} \\right\\|_2\n",
    "$$\n",
    "\n",
    "    与L1-norm一样，只要`(feature_width, feature_height)`一致，那么`num_channel`的`sum`就是一样的。但是L2-norm会因为`num_channel`越大，每个channel占比的影响越小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma[j] = torch.linalg.norm(F_all - Fj, ord=2)\n",
      "[64, 512, 4, 4] tensor(0.5762) tensor(0.0011)\n",
      "[64, 512, 8, 8] tensor(0.4688) tensor(0.0009)\n",
      "[64, 512, 16, 16] tensor(0.3550) tensor(0.0007)\n",
      "[64, 256, 4, 4] tensor(0.5748) tensor(0.0022)\n",
      "[64, 256, 8, 8] tensor(0.4703) tensor(0.0018)\n",
      "[64, 256, 16, 16] tensor(0.3521) tensor(0.0014)\n",
      "[64, 128, 4, 4] tensor(0.5808) tensor(0.0045)\n",
      "[64, 128, 8, 8] tensor(0.4646) tensor(0.0036)\n",
      "[64, 128, 16, 16] tensor(0.3548) tensor(0.0028)\n"
     ]
    }
   ],
   "source": [
    "print('gamma[j] = torch.linalg.norm(F_all - Fj, ord=2)')\n",
    "for key in x:\n",
    "    based_gamma = activation_based_gamma_batch(x[key])\n",
    "    print(list(x[key].shape), torch.sum(based_gamma), torch.mean(based_gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Deprecated: max(sum(abs(data)), dim=0)\n",
    "    等价于 gamma[j] = torch.linalg.norm(F_all - Fj, ord=1)\n",
    "    已弃用，因为既不是L1-norm，也不是L2-norm。但是有个有趣的现象是，无论`num_channel`怎么变换，总的gamma值是一致的。随着num_channel的增加，每个channel的占比影响减少。\n",
    "    \n",
    "$$\n",
    "\\gamma = \\sum \\left\\| \\frac{Q^j_S}{\\|Q^j_S\\|_2} - \\frac{Q^j_T}{\\|Q^j_T\\|_2} \\right\\|_p\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma[j] = torch.linalg.norm(F_all - Fj, ord=1)\n",
      "torch.Size([64, 512, 4, 4]) tensor(0.5806) tensor(0.0011)\n",
      "torch.Size([64, 256, 4, 4]) tensor(0.5726) tensor(0.0022)\n",
      "torch.Size([64, 128, 4, 4]) tensor(0.5767) tensor(0.0045)\n",
      "torch.Size([64, 512, 8, 8]) tensor(0.4651) tensor(0.0009)\n",
      "torch.Size([64, 256, 8, 8]) tensor(0.4694) tensor(0.0018)\n",
      "torch.Size([64, 128, 8, 8]) tensor(0.4710) tensor(0.0037)\n",
      "torch.Size([64, 512, 16, 16]) tensor(0.3537) tensor(0.0007)\n",
      "torch.Size([64, 256, 16, 16]) tensor(0.3520) tensor(0.0014)\n",
      "torch.Size([64, 128, 16, 16]) tensor(0.3531) tensor(0.0028)\n"
     ]
    }
   ],
   "source": [
    "print('gamma[j] = torch.linalg.norm(F_all - Fj, ord=1)')\n",
    "for key in x:\n",
    "    based_gamma = activation_based_gamma_batch(x[key])\n",
    "    print(x[key].shape, torch.sum(based_gamma), torch.mean(based_gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 observe one batch or all batches gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 one batch gamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]),\n",
       " tensor([0.1797, 0.1796, 0.1793, 0.1787, 0.1769, 0.1758, 0.1793, 0.1770, 0.1769,\n",
       "         0.1754, 0.1740, 0.1781, 0.1760, 0.1754, 0.1771, 0.1798, 0.1789, 0.1761,\n",
       "         0.1767, 0.1754, 0.1746, 0.1763, 0.1779, 0.1781, 0.1758, 0.1760, 0.1770,\n",
       "         0.1767, 0.1759, 0.1763, 0.1778, 0.1741, 0.1752, 0.1749, 0.1767, 0.1782,\n",
       "         0.1773, 0.1798, 0.1775, 0.1763, 0.1800, 0.1773, 0.1756, 0.1740, 0.1770,\n",
       "         0.1760, 0.1787, 0.1769, 0.1795, 0.1793, 0.1801, 0.1786, 0.1742, 0.1767,\n",
       "         0.1770, 0.1750, 0.1760, 0.1795, 0.1772, 0.1769, 0.1791, 0.1743, 0.1756,\n",
       "         0.1762, 0.1774, 0.1743, 0.1791, 0.1765, 0.1787, 0.1805, 0.1740, 0.1772,\n",
       "         0.1776, 0.1803, 0.1773, 0.1769, 0.1749, 0.1756, 0.1786, 0.1778, 0.1784,\n",
       "         0.1789, 0.1770, 0.1782, 0.1748, 0.1759, 0.1761, 0.1763, 0.1796, 0.1774,\n",
       "         0.1755, 0.1754, 0.1741, 0.1804, 0.1771, 0.1766, 0.1756, 0.1769, 0.1805,\n",
       "         0.1785, 0.1798, 0.1780, 0.1772, 0.1745, 0.1796, 0.1763, 0.1780, 0.1767,\n",
       "         0.1766, 0.1771, 0.1768, 0.1782, 0.1760, 0.1777, 0.1784, 0.1764, 0.1762,\n",
       "         0.1767, 0.1778, 0.1762, 0.1768, 0.1773, 0.1760, 0.1783, 0.1769, 0.1776,\n",
       "         0.1799, 0.1759]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.normal(10, 5, size=(32, 512, 2, 2))\n",
    "x = torch.randn(64, 128, 16, 16)\n",
    "\n",
    "batch_size, num_channel, input_width, input_height = x.shape\n",
    "\n",
    "gamma = torch.zeros(num_channel)\n",
    "for i in range(batch_size):\n",
    "    data = x[i].clone().squeeze(0)\n",
    "    gamma += activation_based_gamma(data)\n",
    "\n",
    "gamma.shape, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 all batches' gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]),\n",
       " tensor([0.0028, 0.0030, 0.0027, 0.0028, 0.0033, 0.0026, 0.0032, 0.0026, 0.0027,\n",
       "         0.0028, 0.0025, 0.0026, 0.0027, 0.0023, 0.0032, 0.0027, 0.0029, 0.0027,\n",
       "         0.0024, 0.0032, 0.0030, 0.0025, 0.0029, 0.0028, 0.0026, 0.0026, 0.0028,\n",
       "         0.0027, 0.0026, 0.0026, 0.0026, 0.0029, 0.0028, 0.0024, 0.0024, 0.0028,\n",
       "         0.0027, 0.0028, 0.0027, 0.0030, 0.0028, 0.0028, 0.0026, 0.0026, 0.0028,\n",
       "         0.0026, 0.0027, 0.0025, 0.0029, 0.0029, 0.0032, 0.0024, 0.0028, 0.0024,\n",
       "         0.0029, 0.0026, 0.0031, 0.0031, 0.0027, 0.0033, 0.0026, 0.0028, 0.0032,\n",
       "         0.0028, 0.0029, 0.0030, 0.0026, 0.0030, 0.0028, 0.0025, 0.0029, 0.0029,\n",
       "         0.0030, 0.0029, 0.0029, 0.0030, 0.0028, 0.0025, 0.0031, 0.0027, 0.0030,\n",
       "         0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029, 0.0027, 0.0025, 0.0030,\n",
       "         0.0027, 0.0028, 0.0028, 0.0028, 0.0026, 0.0031, 0.0028, 0.0029, 0.0027,\n",
       "         0.0029, 0.0026, 0.0026, 0.0023, 0.0029, 0.0026, 0.0024, 0.0023, 0.0027,\n",
       "         0.0025, 0.0027, 0.0029, 0.0028, 0.0031, 0.0027, 0.0029, 0.0030, 0.0030,\n",
       "         0.0026, 0.0027, 0.0025, 0.0024, 0.0031, 0.0027, 0.0030, 0.0031, 0.0028,\n",
       "         0.0024, 0.0031]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = activation_based_gamma_batch(x)\n",
    "gamma.shape, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pruning Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 number of channel & weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5504, 5504)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_channel, num_weight = 0, 0\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        num_channel += m.weight.data.shape[0]\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        num_weight += m.weight.data.shape[0]\n",
    "        \n",
    "num_channel, num_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Pruning the Conv2d Weight\n",
    "\n",
    "##### 4.2.1 conv2d gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.5326e-05, 1.7719e-02, 8.4088e-03, 1.7871e-05, 7.4207e-03, 1.1284e-02,\n",
       "         5.4985e-06, 2.1443e-03, 2.4170e-02, 1.6162e-05, 6.6668e-06, 1.2856e-02,\n",
       "         6.0110e-06, 7.9992e-04, 2.8856e-02, 1.0777e-02, 7.6803e-03, 5.2634e-05,\n",
       "         5.9979e-06, 3.4912e-04, 1.2163e-02, 2.0054e-02, 8.0653e-06, 2.4019e-02,\n",
       "         5.8209e-03, 1.1782e-05, 2.2681e-02, 8.0056e-03, 2.7430e-02, 7.5525e-03,\n",
       "         2.2810e-02, 3.2574e-03, 1.5393e-02, 5.8843e-06, 1.8252e-02, 1.9143e-02,\n",
       "         4.4849e-03, 1.3133e-02, 3.9484e-04, 2.5633e-03, 5.5649e-06, 2.7661e-05,\n",
       "         3.3130e-02, 1.8082e-05, 2.5268e-05, 1.3858e-02, 2.1784e-02, 2.6519e-03,\n",
       "         2.0591e-04, 2.3056e-03, 4.8556e-03, 3.8576e-02, 8.4763e-06, 7.4939e-06,\n",
       "         3.4897e-02, 1.5302e-02, 8.3965e-06, 2.0118e-02, 4.3423e-03, 3.8959e-02,\n",
       "         9.1252e-06, 3.5502e-02, 7.8835e-03, 6.1306e-05, 9.4059e-03, 1.0069e-02,\n",
       "         1.0224e-02, 1.1451e-02, 5.1413e-03, 1.2001e-02, 2.0187e-02, 1.0213e-02,\n",
       "         1.2197e-02, 1.3964e-02, 9.9661e-03, 1.1507e-02, 1.0922e-02, 7.8831e-03,\n",
       "         9.1364e-03, 8.3836e-03, 6.7556e-03, 9.0236e-03, 7.9920e-03, 8.1152e-03,\n",
       "         7.8239e-03, 8.6642e-03, 1.1487e-02, 1.5862e-02, 8.0692e-03, 1.0320e-02,\n",
       "         7.9699e-03, 1.8955e-02, 1.4374e-02, 6.5576e-03, 1.3299e-02, 8.1861e-03,\n",
       "         9.0669e-03, 8.9523e-03, 8.9634e-03, 1.4963e-02]),\n",
       " [tensor([4.5326e-05, 1.7719e-02, 8.4088e-03, 1.7871e-05, 7.4207e-03, 1.1284e-02,\n",
       "          5.4985e-06, 2.1443e-03, 2.4170e-02, 1.6162e-05, 6.6668e-06, 1.2856e-02,\n",
       "          6.0110e-06, 7.9992e-04, 2.8856e-02, 1.0777e-02, 7.6803e-03, 5.2634e-05,\n",
       "          5.9979e-06, 3.4912e-04, 1.2163e-02, 2.0054e-02, 8.0653e-06, 2.4019e-02,\n",
       "          5.8209e-03, 1.1782e-05, 2.2681e-02, 8.0056e-03, 2.7430e-02, 7.5525e-03,\n",
       "          2.2810e-02, 3.2574e-03, 1.5393e-02, 5.8843e-06, 1.8252e-02, 1.9143e-02,\n",
       "          4.4849e-03, 1.3133e-02, 3.9484e-04, 2.5633e-03, 5.5649e-06, 2.7661e-05,\n",
       "          3.3130e-02, 1.8082e-05, 2.5268e-05, 1.3858e-02, 2.1784e-02, 2.6519e-03,\n",
       "          2.0591e-04, 2.3056e-03, 4.8556e-03, 3.8576e-02, 8.4763e-06, 7.4939e-06,\n",
       "          3.4897e-02, 1.5302e-02, 8.3965e-06, 2.0118e-02, 4.3423e-03, 3.8959e-02,\n",
       "          9.1252e-06, 3.5502e-02, 7.8835e-03, 6.1306e-05]),\n",
       "  tensor([0.0094, 0.0101, 0.0102, 0.0115, 0.0051, 0.0120, 0.0202, 0.0102, 0.0122,\n",
       "          0.0140, 0.0100, 0.0115, 0.0109, 0.0079, 0.0091, 0.0084, 0.0068, 0.0090,\n",
       "          0.0080, 0.0081, 0.0078, 0.0087, 0.0115, 0.0159, 0.0081, 0.0103, 0.0080,\n",
       "          0.0190, 0.0144, 0.0066, 0.0133, 0.0082, 0.0091, 0.0090, 0.0090, 0.0150,\n",
       "          0.0106, 0.0080, 0.0114, 0.0063, 0.0072, 0.0087, 0.0059, 0.0058, 0.0087,\n",
       "          0.0135, 0.0068, 0.0077, 0.0101, 0.0095, 0.0109, 0.0104, 0.0091, 0.0111,\n",
       "          0.0076, 0.0112, 0.0118, 0.0084, 0.0117, 0.0099, 0.0105, 0.0076, 0.0103,\n",
       "          0.0146]),\n",
       "  tensor([0.0023, 0.0035, 0.0026, 0.0034, 0.0031, 0.0026, 0.0032, 0.0030, 0.0030,\n",
       "          0.0028, 0.0029, 0.0030, 0.0026, 0.0049, 0.0025, 0.0018, 0.0020, 0.0041,\n",
       "          0.0044, 0.0032, 0.0045, 0.0022, 0.0026, 0.0032, 0.0038, 0.0041, 0.0050,\n",
       "          0.0032, 0.0027, 0.0044, 0.0038, 0.0023, 0.0040, 0.0034, 0.0030, 0.0041,\n",
       "          0.0066, 0.0020, 0.0036, 0.0026, 0.0043, 0.0027, 0.0036, 0.0045, 0.0038,\n",
       "          0.0035, 0.0038, 0.0032, 0.0048, 0.0055, 0.0028, 0.0032, 0.0039, 0.0032,\n",
       "          0.0043, 0.0043, 0.0045, 0.0041, 0.0022, 0.0041, 0.0028, 0.0032, 0.0010,\n",
       "          0.0048, 0.0032, 0.0035, 0.0027, 0.0039, 0.0030, 0.0023, 0.0028, 0.0043,\n",
       "          0.0024, 0.0044, 0.0034, 0.0024, 0.0068, 0.0026, 0.0043, 0.0059, 0.0033,\n",
       "          0.0046, 0.0057, 0.0032, 0.0039, 0.0034, 0.0043, 0.0037, 0.0055, 0.0029,\n",
       "          0.0040, 0.0022, 0.0033, 0.0046, 0.0052, 0.0028, 0.0033, 0.0047, 0.0026,\n",
       "          0.0028, 0.0047, 0.0021, 0.0025, 0.0049, 0.0032, 0.0047, 0.0037, 0.0029,\n",
       "          0.0024, 0.0024, 0.0037, 0.0029, 0.0018, 0.0050, 0.0035, 0.0030, 0.0026,\n",
       "          0.0029, 0.0031, 0.0024, 0.0044, 0.0041, 0.0055, 0.0032, 0.0053, 0.0028,\n",
       "          0.0048, 0.0059]),\n",
       "  tensor([0.0026, 0.0024, 0.0027, 0.0028, 0.0033, 0.0030, 0.0029, 0.0025, 0.0025,\n",
       "          0.0032, 0.0030, 0.0028, 0.0030, 0.0031, 0.0030, 0.0027, 0.0028, 0.0033,\n",
       "          0.0035, 0.0035, 0.0032, 0.0031, 0.0025, 0.0034, 0.0032, 0.0030, 0.0032,\n",
       "          0.0028, 0.0032, 0.0028, 0.0029, 0.0031, 0.0036, 0.0027, 0.0027, 0.0025,\n",
       "          0.0031, 0.0028, 0.0033, 0.0037, 0.0028, 0.0024, 0.0031, 0.0032, 0.0039,\n",
       "          0.0025, 0.0039, 0.0027, 0.0030, 0.0029, 0.0027, 0.0023, 0.0024, 0.0027,\n",
       "          0.0025, 0.0029, 0.0026, 0.0030, 0.0023, 0.0025, 0.0027, 0.0030, 0.0028,\n",
       "          0.0021, 0.0026, 0.0033, 0.0029, 0.0031, 0.0024, 0.0034, 0.0033, 0.0027,\n",
       "          0.0032, 0.0038, 0.0028, 0.0030, 0.0028, 0.0025, 0.0030, 0.0030, 0.0031,\n",
       "          0.0028, 0.0027, 0.0034, 0.0024, 0.0025, 0.0031, 0.0031, 0.0033, 0.0027,\n",
       "          0.0033, 0.0028, 0.0026, 0.0034, 0.0034, 0.0030, 0.0027, 0.0031, 0.0024,\n",
       "          0.0023, 0.0031, 0.0030, 0.0032, 0.0029, 0.0029, 0.0030, 0.0038, 0.0032,\n",
       "          0.0032, 0.0028, 0.0028, 0.0026, 0.0031, 0.0027, 0.0028, 0.0026, 0.0026,\n",
       "          0.0025, 0.0030, 0.0032, 0.0027, 0.0027, 0.0029, 0.0032, 0.0029, 0.0030,\n",
       "          0.0028, 0.0028]),\n",
       "  tensor([1.6544e-03, 1.3760e-03, 1.6388e-03, 1.3025e-03, 1.2901e-03, 1.4072e-03,\n",
       "          1.2682e-03, 1.3450e-03, 1.3153e-03, 1.3617e-03, 1.3870e-03, 1.2487e-03,\n",
       "          1.3722e-03, 2.4461e-03, 1.0295e-03, 1.2574e-03, 1.5217e-03, 1.4348e-03,\n",
       "          1.4942e-03, 1.4983e-03, 1.5450e-03, 1.3865e-03, 1.7973e-03, 1.3423e-03,\n",
       "          9.4718e-04, 2.0254e-03, 1.3492e-03, 1.3463e-03, 1.3880e-03, 1.4225e-03,\n",
       "          1.2739e-03, 1.3279e-03, 1.4028e-03, 1.2584e-03, 1.3272e-03, 1.8707e-03,\n",
       "          1.2957e-03, 1.4650e-03, 1.3491e-03, 1.2949e-03, 1.1701e-03, 1.4109e-03,\n",
       "          1.7050e-03, 1.6140e-03, 1.5228e-03, 1.6481e-03, 1.1900e-03, 1.5695e-03,\n",
       "          1.4731e-03, 1.6828e-03, 1.5388e-03, 2.0505e-06, 2.0022e-03, 1.3529e-03,\n",
       "          1.4573e-03, 1.4435e-03, 1.2855e-03, 1.1623e-03, 1.3911e-03, 1.4080e-03,\n",
       "          1.4372e-03, 1.4143e-03, 1.6511e-03, 1.5249e-03, 1.1226e-03, 1.3612e-03,\n",
       "          1.5388e-03, 1.4925e-03, 1.5904e-03, 1.4116e-03, 1.2796e-03, 1.2846e-03,\n",
       "          1.5023e-03, 1.1799e-03, 1.4886e-03, 1.8819e-03, 1.4523e-03, 1.3813e-03,\n",
       "          1.7416e-03, 1.3967e-03, 1.1130e-03, 1.3188e-03, 1.1824e-03, 1.2744e-03,\n",
       "          1.5582e-03, 1.4413e-03, 1.2556e-03, 1.5978e-03, 1.5804e-03, 1.2252e-03,\n",
       "          1.3967e-03, 1.6706e-03, 1.3354e-03, 1.1670e-03, 1.5739e-03, 1.3614e-03,\n",
       "          1.5201e-03, 1.4349e-03, 1.2958e-03, 1.4237e-03, 1.2926e-03, 1.1628e-03,\n",
       "          1.2668e-03, 1.2639e-03, 1.6259e-03, 1.6989e-03, 1.4280e-03, 1.8913e-03,\n",
       "          1.2907e-03, 1.4292e-03, 1.3578e-03, 1.6815e-03, 1.1632e-03, 1.4448e-03,\n",
       "          1.2764e-03, 1.3519e-03, 1.3834e-03, 1.2111e-03, 1.6849e-03, 1.2599e-03,\n",
       "          1.6310e-03, 1.1994e-03, 1.1514e-03, 1.3605e-03, 1.6137e-03, 1.2394e-03,\n",
       "          1.2940e-03, 1.3906e-03, 1.3083e-03, 1.1435e-03, 1.2689e-03, 1.6167e-03,\n",
       "          1.5039e-03, 1.1949e-03, 1.3380e-03, 1.7318e-03, 1.2910e-03, 1.1433e-03,\n",
       "          1.3266e-03, 1.3530e-03, 1.7401e-03, 1.4101e-03, 1.4492e-03, 1.4212e-03,\n",
       "          1.2615e-03, 1.4261e-03, 1.3743e-03, 1.6265e-03, 1.2930e-03, 1.3931e-03,\n",
       "          1.3310e-03, 1.4157e-03, 1.4956e-03, 1.2819e-03, 1.3721e-03, 1.5376e-03,\n",
       "          1.2306e-03, 1.4324e-03, 1.6673e-03, 1.5150e-03, 1.5159e-03, 1.3098e-03,\n",
       "          1.1536e-03, 1.3529e-03, 1.3421e-03, 1.3728e-03, 1.4248e-03, 1.4351e-03,\n",
       "          1.3768e-03, 1.3401e-03, 1.2498e-03, 1.1919e-03, 1.3209e-03, 1.3995e-03,\n",
       "          1.3241e-03, 1.2378e-03, 1.1235e-03, 1.5611e-03, 1.7821e-03, 1.3170e-03,\n",
       "          1.3392e-03, 1.5744e-03, 1.4942e-03, 1.5971e-03, 1.2039e-03, 1.2344e-03,\n",
       "          1.1478e-03, 1.4123e-03, 1.9468e-03, 1.4383e-03, 1.5657e-03, 1.2961e-03,\n",
       "          1.4532e-03, 1.4906e-03, 1.5221e-03, 1.3737e-03, 1.9044e-03, 1.5027e-03,\n",
       "          1.3942e-03, 1.4026e-03, 1.3429e-03, 1.7109e-03, 1.2745e-03, 1.6106e-03,\n",
       "          1.3395e-03, 1.2673e-03, 1.3579e-03, 1.5940e-03, 1.5253e-03, 1.6705e-03,\n",
       "          1.8329e-03, 1.5391e-03, 1.1410e-03, 1.4450e-03, 1.0187e-03, 1.5337e-03,\n",
       "          1.2650e-03, 1.2540e-03, 1.5911e-03, 1.4357e-03, 1.1407e-03, 1.3464e-03,\n",
       "          1.4358e-03, 1.5672e-03, 1.2034e-03, 1.5124e-03, 1.2366e-03, 1.2661e-03,\n",
       "          1.5652e-03, 1.1975e-03, 1.3615e-03, 1.2527e-03, 1.4439e-03, 1.5263e-03,\n",
       "          1.5152e-03, 1.4084e-03, 1.7468e-03, 1.3809e-03, 1.7835e-03, 1.3636e-03,\n",
       "          1.3241e-03, 1.2929e-03, 1.3872e-03, 1.3700e-03, 1.1362e-03, 1.3205e-03,\n",
       "          1.4823e-03, 1.5755e-03, 2.3142e-03, 1.7123e-03, 1.4251e-03, 1.6316e-03,\n",
       "          1.2617e-03, 2.7036e-06, 1.4283e-03, 1.8423e-03]),\n",
       "  tensor([1.4467e-03, 1.5263e-03, 1.4424e-03, 1.6560e-03, 1.3811e-03, 8.2280e-04,\n",
       "          1.5798e-03, 1.4742e-03, 1.3274e-03, 1.0595e-03, 1.5218e-03, 1.4644e-03,\n",
       "          1.1711e-03, 1.3359e-03, 1.1596e-03, 8.8725e-04, 1.4645e-03, 9.7905e-04,\n",
       "          1.4014e-03, 1.6994e-03, 1.4464e-03, 1.4617e-03, 1.6362e-03, 1.5150e-03,\n",
       "          1.3790e-03, 1.4607e-03, 1.5088e-03, 1.4474e-03, 1.8819e-03, 1.2634e-03,\n",
       "          1.0910e-03, 1.2696e-03, 1.5021e-03, 1.3521e-03, 1.1210e-03, 1.5738e-03,\n",
       "          1.1638e-03, 1.4116e-03, 1.7561e-03, 1.3672e-03, 1.2332e-03, 1.4810e-03,\n",
       "          1.3162e-03, 1.3805e-03, 1.5847e-03, 1.3218e-03, 2.0281e-03, 1.5573e-03,\n",
       "          1.3078e-03, 1.5609e-05, 1.4643e-03, 1.3897e-03, 1.5670e-03, 1.6723e-03,\n",
       "          1.3733e-03, 1.3158e-03, 1.2786e-03, 1.0349e-03, 1.5812e-03, 1.4070e-03,\n",
       "          1.5666e-03, 1.2651e-03, 1.5244e-03, 1.2010e-03, 1.4605e-03, 1.3587e-03,\n",
       "          1.0796e-03, 1.3870e-03, 1.2530e-03, 1.4868e-03, 1.3301e-03, 1.3929e-03,\n",
       "          1.2935e-03, 1.7726e-03, 1.4718e-03, 1.1816e-03, 1.5689e-03, 1.5427e-03,\n",
       "          1.2397e-03, 1.2553e-03, 1.6706e-03, 1.2544e-03, 1.4133e-03, 1.3227e-03,\n",
       "          1.0956e-03, 1.5837e-03, 1.3523e-03, 1.5349e-03, 1.1523e-03, 1.3565e-03,\n",
       "          1.1130e-03, 1.1938e-03, 1.1828e-03, 1.2139e-03, 1.2304e-03, 1.5527e-03,\n",
       "          1.5594e-03, 1.5642e-03, 1.4313e-03, 1.2295e-03, 9.4878e-04, 1.5864e-03,\n",
       "          1.4200e-03, 1.8105e-03, 1.3983e-03, 1.6031e-03, 1.3743e-03, 1.2715e-03,\n",
       "          1.1915e-03, 1.3909e-03, 1.2671e-03, 1.1555e-03, 1.1823e-03, 1.1021e-03,\n",
       "          1.3636e-03, 1.6717e-03, 1.5299e-03, 1.4949e-03, 1.4119e-03, 1.5957e-03,\n",
       "          1.2157e-03, 2.7785e-06, 1.4708e-03, 1.4006e-03, 1.4603e-03, 1.3115e-03,\n",
       "          1.4875e-03, 1.8381e-03, 1.5670e-03, 1.6733e-03, 1.4007e-03, 1.7920e-03,\n",
       "          1.3141e-03, 1.7934e-03, 8.3047e-04, 1.7684e-03, 8.4865e-04, 1.3036e-03,\n",
       "          1.3706e-03, 1.6891e-03, 1.3735e-03, 1.1944e-03, 1.4012e-03, 1.6290e-03,\n",
       "          1.2295e-03, 1.5556e-03, 1.3823e-03, 1.0047e-03, 1.2131e-03, 1.5185e-03,\n",
       "          1.1738e-03, 1.4000e-03, 9.1630e-04, 1.5559e-03, 1.3579e-03, 1.0475e-03,\n",
       "          1.4840e-03, 1.6034e-03, 1.3717e-03, 1.6055e-03, 1.1905e-03, 1.2234e-03,\n",
       "          1.4844e-03, 1.6117e-03, 1.7026e-03, 1.3816e-03, 1.1937e-03, 1.2183e-03,\n",
       "          1.3357e-03, 1.3014e-03, 1.0908e-03, 1.2249e-03, 1.5909e-03, 1.3106e-03,\n",
       "          1.3103e-05, 1.2484e-03, 1.7648e-03, 1.6906e-03, 1.3334e-03, 1.2290e-03,\n",
       "          1.0947e-03, 1.1898e-03, 1.7087e-03, 1.6563e-03, 1.8165e-03, 1.4784e-03,\n",
       "          1.7639e-03, 8.5881e-04, 1.3698e-03, 1.1628e-03, 1.4834e-03, 1.2282e-03,\n",
       "          1.3597e-03, 3.2156e-04, 1.5118e-03, 1.4726e-03, 1.4631e-03, 1.1073e-03,\n",
       "          1.2709e-03, 1.1839e-03, 1.7555e-03, 1.6217e-03, 1.8663e-03, 1.0261e-03,\n",
       "          1.3583e-03, 1.0951e-03, 1.3047e-03, 1.6697e-03, 1.3701e-03, 1.5554e-03,\n",
       "          1.4191e-03, 1.3367e-03, 1.4233e-03, 1.4538e-03, 1.2834e-03, 1.4934e-03,\n",
       "          2.1786e-03, 1.2981e-03, 1.5222e-03, 1.8220e-03, 1.1948e-03, 1.3548e-03,\n",
       "          1.2271e-03, 1.4285e-03, 1.2271e-03, 1.2116e-03, 1.1283e-03, 1.9547e-03,\n",
       "          1.3435e-03, 5.9161e-04, 1.5563e-03, 1.6071e-03, 1.3265e-03, 1.4560e-03,\n",
       "          1.1753e-03, 1.6177e-03, 2.3589e-03, 1.1705e-03, 1.2603e-03, 1.3945e-03,\n",
       "          1.4763e-03, 4.1247e-05, 1.8431e-04, 1.4661e-03, 1.0634e-03, 9.9109e-04,\n",
       "          1.0646e-03, 1.2970e-03, 1.3242e-03, 1.1118e-03, 1.3622e-03, 1.5737e-03,\n",
       "          1.3582e-03, 1.3623e-03, 1.4033e-03, 2.0238e-03]),\n",
       "  tensor([1.4516e-03, 1.3551e-03, 2.8908e-03, 1.1705e-03, 7.3911e-05, 4.2323e-04,\n",
       "          1.8872e-03, 4.2898e-05, 1.2265e-03, 1.6180e-03, 1.6189e-03, 1.4045e-03,\n",
       "          1.8601e-03, 1.8823e-03, 1.7364e-03, 1.4306e-03, 1.0869e-03, 1.7121e-03,\n",
       "          2.7425e-04, 1.2671e-03, 2.0783e-03, 4.5092e-04, 2.0370e-03, 3.2720e-04,\n",
       "          1.6054e-03, 1.7217e-03, 9.6276e-04, 1.3909e-03, 6.9577e-06, 1.4177e-03,\n",
       "          8.1327e-05, 1.5307e-03, 1.0506e-03, 1.5589e-03, 1.4631e-03, 2.1285e-03,\n",
       "          4.3467e-06, 8.8347e-04, 1.4020e-03, 2.0652e-03, 6.6004e-04, 1.3574e-03,\n",
       "          2.4046e-03, 1.4415e-03, 1.7360e-03, 1.8096e-03, 1.4864e-03, 2.0269e-03,\n",
       "          1.2849e-03, 1.6190e-03, 1.5416e-03, 1.2942e-03, 1.8921e-03, 1.4183e-03,\n",
       "          1.7317e-03, 1.0072e-03, 1.7876e-03, 2.1436e-03, 1.8537e-03, 1.8964e-03,\n",
       "          8.7897e-04, 3.7182e-06, 1.6523e-03, 1.5993e-03, 1.5998e-03, 8.7826e-04,\n",
       "          1.1500e-03, 1.6266e-03, 1.3270e-03, 1.7644e-03, 1.4508e-03, 9.4137e-04,\n",
       "          1.8458e-03, 1.3017e-03, 2.0232e-03, 1.5637e-03, 1.3983e-03, 2.4382e-03,\n",
       "          2.3207e-03, 1.3938e-03, 1.0926e-03, 1.3860e-03, 1.5228e-03, 1.8432e-03,\n",
       "          2.2989e-03, 1.6049e-03, 1.3411e-03, 2.0460e-03, 1.7862e-03, 1.3456e-03,\n",
       "          1.9907e-03, 2.4415e-05, 1.6270e-03, 2.4097e-03, 2.4063e-03, 2.0416e-05,\n",
       "          1.2306e-03, 5.0440e-05, 7.5775e-06, 1.1610e-03, 2.2500e-03, 1.5006e-03,\n",
       "          2.3912e-03, 2.2380e-03, 7.5068e-04, 7.5973e-04, 1.1953e-03, 2.7287e-03,\n",
       "          1.5403e-03, 2.2368e-03, 2.0803e-03, 1.3537e-03, 1.6470e-03, 1.8887e-03,\n",
       "          2.1935e-03, 1.2279e-03, 1.9450e-03, 1.5319e-03, 1.4682e-03, 1.7600e-03,\n",
       "          2.3553e-03, 1.2293e-03, 1.4272e-03, 2.8450e-05, 1.7913e-03, 1.8433e-03,\n",
       "          1.7494e-03, 6.4547e-04, 1.6788e-03, 2.7676e-03, 1.5446e-03, 1.8049e-03,\n",
       "          2.4537e-03, 8.4268e-04, 1.4393e-03, 1.6482e-03, 1.3016e-03, 1.7670e-03,\n",
       "          1.5864e-03, 2.1319e-03, 1.2862e-03, 1.3607e-03, 1.7969e-03, 1.9425e-03,\n",
       "          2.5044e-03, 1.1719e-03, 1.2907e-03, 1.9179e-03, 1.9021e-03, 1.6065e-03,\n",
       "          1.9443e-03, 7.0880e-04, 1.7064e-03, 1.6268e-03, 1.8914e-03, 1.1866e-03,\n",
       "          1.2113e-03, 1.4913e-03, 2.2169e-03, 1.5697e-03, 2.1809e-03, 1.7495e-03,\n",
       "          7.0894e-05, 9.9120e-04, 1.0654e-03, 1.3460e-03, 1.8858e-05, 1.9706e-03,\n",
       "          1.1792e-03, 8.4848e-04, 2.1921e-03, 1.4689e-03, 2.0233e-03, 1.8273e-03,\n",
       "          1.8171e-03, 2.1542e-03, 4.8109e-06, 7.5036e-04, 1.8073e-03, 2.3557e-03,\n",
       "          1.4392e-03, 1.8989e-03, 2.3511e-03, 1.6803e-03, 1.2916e-03, 2.0986e-03,\n",
       "          1.0768e-03, 1.5009e-03, 5.3217e-04, 1.6073e-03, 1.6440e-03, 1.1360e-03,\n",
       "          4.8621e-06, 1.5802e-03, 1.4755e-03, 1.4693e-03, 1.2886e-03, 4.4646e-04,\n",
       "          1.9874e-03, 4.3592e-04, 1.6761e-03, 1.4365e-03, 9.7383e-04, 1.8070e-03,\n",
       "          1.3415e-03, 5.6345e-04, 1.9717e-03, 1.4417e-03, 1.4015e-03, 6.7084e-04,\n",
       "          1.8903e-03, 1.3541e-03, 1.8571e-03, 1.8431e-03, 1.1104e-03, 1.5294e-03,\n",
       "          1.1697e-03, 1.5220e-03, 1.3202e-03, 1.4438e-03, 1.7047e-03, 7.0264e-04,\n",
       "          2.2624e-03, 1.0555e-03, 1.6435e-03, 1.4431e-03, 1.6337e-03, 1.8703e-03,\n",
       "          1.3231e-03, 1.3712e-03, 1.4747e-03, 9.3622e-04, 1.3719e-03, 1.9615e-03,\n",
       "          5.8777e-05, 3.2267e-05, 1.7128e-03, 8.3761e-04, 2.2181e-03, 7.1227e-04,\n",
       "          1.1023e-03, 1.7695e-03, 1.8272e-03, 1.3689e-03, 1.9799e-03, 1.3713e-03,\n",
       "          1.2510e-03, 1.4641e-03, 1.8614e-03, 1.3581e-03, 1.7155e-03, 1.6986e-03,\n",
       "          2.7914e-03, 1.4515e-03, 6.4349e-04, 1.6237e-03]),\n",
       "  tensor([1.4667e-04, 1.1335e-03, 2.0483e-05, 1.9937e-03, 2.2301e-03, 2.9449e-03,\n",
       "          1.9374e-03, 2.3121e-03, 2.3766e-03, 2.0738e-03, 2.7924e-03, 2.0538e-05,\n",
       "          1.8004e-03, 2.7571e-03, 2.5222e-03, 1.8741e-03, 1.5051e-03, 2.6511e-03,\n",
       "          2.1256e-03, 1.6241e-05, 7.2225e-06, 6.1377e-06, 2.3553e-03, 2.2538e-03,\n",
       "          1.9752e-03, 2.5434e-03, 1.9201e-03, 2.1680e-03, 1.9835e-03, 1.1427e-03,\n",
       "          2.0494e-03, 1.4881e-03, 1.9520e-03, 4.6892e-05, 1.4813e-05, 2.9733e-05,\n",
       "          2.7243e-03, 1.1611e-05, 1.5961e-03, 1.2958e-05, 1.7832e-03, 1.9141e-03,\n",
       "          1.5158e-03, 3.7512e-04, 1.7851e-03, 1.9907e-03, 5.3729e-06, 8.0891e-04,\n",
       "          3.0183e-04, 2.1344e-03, 1.5320e-03, 1.4704e-03, 2.2509e-03, 2.9624e-04,\n",
       "          7.8791e-06, 2.3556e-03, 1.4703e-03, 1.5898e-04, 2.0181e-03, 2.3024e-03,\n",
       "          4.8353e-06, 1.9960e-03, 3.6889e-05, 1.9849e-03, 1.7217e-03, 2.3405e-03,\n",
       "          2.4105e-03, 2.9278e-05, 2.2514e-03, 2.7576e-03, 1.5826e-03, 1.5230e-03,\n",
       "          2.3372e-03, 2.0478e-03, 2.1315e-03, 1.2348e-03, 3.0448e-05, 2.0010e-03,\n",
       "          2.3277e-03, 2.2755e-03, 2.1225e-03, 2.3937e-05, 5.0856e-05, 1.8283e-03,\n",
       "          2.5400e-03, 2.1291e-03, 9.3675e-04, 1.7553e-03, 9.2254e-04, 7.5770e-04,\n",
       "          2.1883e-03, 1.1268e-03, 2.4476e-03, 1.5615e-03, 1.7043e-03, 2.0672e-03,\n",
       "          2.5245e-03, 1.1968e-05, 7.2413e-06, 1.4877e-03, 1.8815e-03, 1.6764e-03,\n",
       "          2.1806e-03, 1.4518e-03, 2.0874e-03, 1.7708e-03, 1.0929e-03, 1.4578e-03,\n",
       "          1.8766e-03, 1.9712e-03, 9.5513e-04, 5.7857e-04, 7.1840e-05, 1.6940e-03,\n",
       "          1.9462e-03, 1.4825e-03, 2.0476e-03, 2.5041e-03, 5.0016e-05, 2.0184e-03,\n",
       "          1.1379e-04, 6.7498e-06, 2.3497e-03, 9.5279e-04, 1.3237e-03, 2.5421e-03,\n",
       "          1.8922e-03, 1.9367e-03, 1.9218e-03, 1.6334e-03, 1.4285e-04, 1.4344e-03,\n",
       "          2.6899e-03, 1.4546e-03, 2.0544e-03, 1.9868e-03, 2.0299e-03, 2.1997e-03,\n",
       "          1.2213e-03, 1.5417e-03, 1.7996e-03, 2.0507e-03, 6.0150e-05, 1.6500e-03,\n",
       "          1.9778e-03, 1.2095e-03, 2.0868e-03, 1.8796e-03, 8.6533e-04, 1.7290e-03,\n",
       "          1.0734e-03, 1.6456e-03, 1.3287e-03, 2.6035e-03, 6.8062e-06, 1.9716e-03,\n",
       "          2.2609e-04, 1.9499e-04, 1.0939e-04, 1.7148e-03, 2.2048e-05, 1.9873e-03,\n",
       "          1.3231e-03, 1.4384e-03, 9.4828e-05, 4.9964e-06, 2.6791e-03, 1.7219e-03,\n",
       "          2.2561e-03, 1.4657e-03, 1.0987e-03, 1.0622e-03, 1.6932e-03, 3.0704e-05,\n",
       "          2.2426e-03, 1.7382e-03, 1.2751e-05, 1.4739e-03, 1.8575e-03, 1.3608e-03,\n",
       "          1.1152e-03, 7.5666e-04, 1.9168e-03, 1.6932e-03, 1.7019e-03, 6.1488e-05,\n",
       "          2.5539e-03, 1.5063e-03, 1.2756e-03, 5.8571e-05, 1.4841e-03, 1.9026e-03,\n",
       "          1.4494e-04, 2.4160e-03, 1.0178e-03, 2.7508e-03, 2.1807e-03, 2.5207e-03,\n",
       "          1.8902e-05, 1.2569e-04, 2.1800e-03, 1.8146e-03, 1.5671e-03, 2.0166e-03,\n",
       "          2.0651e-03, 9.5600e-04, 4.4546e-05, 1.5683e-03, 1.4703e-03, 5.5294e-06,\n",
       "          2.5793e-03, 1.3237e-05, 2.1618e-03, 2.2785e-03, 2.3356e-03, 1.6239e-03,\n",
       "          1.0462e-03, 2.7052e-03, 4.8337e-04, 2.4060e-03, 1.3246e-03, 2.5005e-03,\n",
       "          1.5173e-03, 1.2928e-05, 1.7741e-03, 1.9039e-03, 2.3751e-03, 2.3616e-03,\n",
       "          7.8236e-04, 1.3968e-03, 2.6458e-03, 2.1748e-03, 1.5628e-03, 1.7297e-03,\n",
       "          4.4678e-05, 2.1016e-03, 2.6932e-03, 1.1263e-03, 1.0717e-03, 1.0793e-03,\n",
       "          2.0383e-03, 4.8371e-05, 6.4573e-05, 1.5467e-03, 1.0030e-03, 2.6615e-03,\n",
       "          2.6006e-03, 5.9400e-05, 9.1870e-04, 9.7553e-04, 2.1447e-03, 2.2120e-03,\n",
       "          2.2198e-03, 1.4648e-03, 1.4702e-03, 1.3562e-03]),\n",
       "  tensor([2.3043e-05, 7.1326e-05, 1.2246e-03, 2.6389e-03, 7.5579e-06, 7.3611e-06,\n",
       "          9.8293e-06, 3.2461e-03, 6.3498e-06, 3.1202e-03, 1.6450e-05, 1.6836e-03,\n",
       "          7.4300e-06, 1.7092e-04, 3.0084e-03, 9.9034e-05, 2.0130e-03, 3.5334e-03,\n",
       "          2.8238e-03, 2.1068e-04, 2.3804e-04, 9.5765e-05, 1.6024e-03, 2.8965e-04,\n",
       "          4.0981e-04, 1.9052e-03, 1.3429e-05, 2.4191e-03, 2.9426e-04, 1.7014e-03,\n",
       "          2.5649e-03, 3.4756e-04, 2.4631e-03, 9.3018e-04, 6.8939e-04, 1.2688e-03,\n",
       "          6.4845e-05, 9.1871e-05, 2.2828e-03, 2.7552e-03, 1.7154e-03, 2.1611e-04,\n",
       "          8.4231e-05, 4.9391e-05, 1.9257e-03, 2.3233e-03, 1.8315e-05, 1.5383e-05,\n",
       "          1.9318e-05, 1.3800e-03, 1.6054e-05, 1.3087e-05, 1.0055e-05, 2.3387e-03,\n",
       "          3.1802e-04, 2.0390e-03, 2.0451e-03, 8.2861e-06, 1.3889e-05, 3.9868e-04,\n",
       "          2.8335e-04, 3.0137e-05, 2.0622e-05, 1.1877e-05, 5.6698e-05, 7.5268e-06,\n",
       "          5.9767e-04, 6.0147e-04, 7.3402e-06, 5.2867e-06, 7.2527e-05, 3.1248e-04,\n",
       "          1.3047e-03, 2.6281e-04, 5.4135e-04, 1.9891e-05, 1.3523e-03, 1.0955e-05,\n",
       "          1.2103e-05, 2.3283e-04, 1.7579e-03, 1.3270e-03, 1.2688e-04, 1.3347e-05,\n",
       "          1.2649e-03, 2.3801e-04, 1.9820e-03, 3.7969e-04, 2.0521e-03, 4.5622e-04,\n",
       "          1.7884e-03, 1.0084e-05, 7.2791e-05, 5.8250e-04, 2.4935e-03, 1.1213e-04,\n",
       "          2.0969e-03, 3.9122e-05, 7.8536e-06, 1.6124e-05, 1.2443e-03, 8.4162e-04,\n",
       "          2.1512e-03, 2.7892e-03, 2.1171e-05, 7.4161e-06, 3.2288e-04, 5.6237e-06,\n",
       "          1.3750e-05, 3.4387e-03, 5.7522e-06, 3.5124e-05, 2.0930e-04, 5.1308e-04,\n",
       "          8.0267e-05, 5.9461e-06, 2.1673e-03, 1.2179e-03, 1.3010e-03, 2.1786e-05,\n",
       "          1.1199e-05, 1.5633e-04, 1.0336e-03, 2.8181e-03, 8.5087e-06, 1.7798e-05,\n",
       "          2.9033e-05, 9.5548e-05, 7.9850e-04, 1.3274e-05, 1.7339e-04, 1.0929e-05,\n",
       "          1.7327e-03, 9.5340e-06, 1.9759e-03, 8.8876e-06, 3.7764e-05, 1.4757e-03,\n",
       "          1.3562e-03, 6.5051e-06, 9.6417e-06, 2.2170e-03, 8.5538e-04, 2.1669e-05,\n",
       "          6.5967e-06, 9.3606e-04, 1.6434e-05, 1.2224e-04, 5.4673e-06, 1.5732e-03,\n",
       "          4.4985e-05, 1.3704e-04, 5.0833e-05, 7.9579e-04, 3.7366e-05, 6.1258e-06,\n",
       "          2.1342e-04, 1.1239e-03, 1.1374e-03, 1.2140e-05, 1.5694e-03, 2.2011e-05,\n",
       "          6.1630e-05, 4.8313e-05, 1.3997e-04, 8.9860e-04, 2.0507e-03, 2.0691e-03,\n",
       "          1.4072e-03, 1.0135e-05, 6.4315e-06, 1.6147e-05, 1.5730e-03, 3.1251e-05,\n",
       "          1.7752e-03, 1.2164e-05, 8.3945e-06, 2.0047e-05, 3.2031e-05, 1.4872e-03,\n",
       "          6.0527e-06, 1.2615e-03, 1.7528e-03, 4.3262e-04, 6.6799e-04, 1.7355e-03,\n",
       "          1.3475e-03, 7.3824e-04, 1.2628e-03, 1.1872e-03, 1.8674e-03, 2.1129e-03,\n",
       "          1.3997e-03, 1.6845e-04, 1.9320e-05, 8.2731e-04, 5.7370e-04, 1.8292e-04,\n",
       "          7.2227e-04, 1.1644e-05, 2.6007e-03, 2.2257e-03, 1.6500e-03, 6.4640e-06,\n",
       "          6.3618e-06, 2.7488e-03, 1.9592e-05, 1.1565e-03, 1.7916e-03, 2.1434e-05,\n",
       "          1.4712e-05, 2.2688e-03, 2.3884e-04, 2.2156e-05, 1.7136e-03, 3.8671e-04,\n",
       "          2.7945e-03, 4.2618e-05, 2.9377e-03, 6.2972e-06, 4.9894e-05, 1.3276e-05,\n",
       "          1.0555e-04, 3.6725e-03, 5.6781e-04, 5.6783e-05, 9.6587e-06, 1.7852e-03,\n",
       "          3.5933e-04, 9.7615e-04, 3.6571e-05, 5.7387e-06, 5.5112e-06, 1.3610e-03,\n",
       "          6.0083e-06, 7.9704e-06, 1.7458e-03, 5.4842e-05, 1.1173e-04, 5.1687e-05,\n",
       "          1.7987e-04, 4.6785e-05, 1.4146e-05, 2.0274e-05, 6.7309e-04, 1.1732e-05,\n",
       "          2.4908e-03, 2.6076e-05, 2.6402e-03, 4.3451e-04, 9.7920e-05, 9.8800e-04,\n",
       "          2.1627e-03, 1.8118e-03, 4.0951e-05, 1.8476e-03, 1.6188e-03, 8.1832e-05,\n",
       "          7.2343e-04, 7.8141e-04, 2.0713e-03, 1.9487e-03, 8.4466e-04, 7.8611e-06,\n",
       "          2.6900e-03, 1.9552e-03, 1.6362e-03, 1.0644e-05, 1.0219e-05, 2.6649e-05,\n",
       "          1.8383e-03, 9.3541e-06, 9.5215e-05, 8.0872e-06, 2.0407e-04, 1.9140e-04,\n",
       "          8.4721e-06, 1.7293e-05, 4.2207e-04, 1.2062e-05, 7.5026e-06, 1.0200e-03,\n",
       "          2.9948e-04, 2.7770e-04, 5.2450e-06, 2.1869e-05, 7.0559e-06, 1.0374e-03,\n",
       "          1.0863e-04, 8.3655e-06, 1.9256e-03, 7.1655e-06, 8.4843e-05, 2.3978e-03,\n",
       "          6.7460e-05, 2.4210e-03, 4.6723e-04, 6.0323e-06, 8.5287e-05, 2.2172e-03,\n",
       "          1.4710e-03, 2.9522e-03, 2.0654e-03, 1.6234e-05, 1.4073e-05, 2.2653e-05,\n",
       "          1.0597e-03, 1.8819e-05, 5.7482e-06, 4.8081e-05, 1.1984e-04, 1.9631e-03,\n",
       "          9.3789e-06, 2.0954e-05, 2.2138e-03, 6.0785e-06, 9.6991e-05, 2.1352e-03,\n",
       "          2.1928e-03, 2.3046e-03, 1.9287e-03, 7.0838e-04, 1.7951e-03, 2.5373e-04,\n",
       "          3.6481e-04, 1.3059e-03, 1.0219e-03, 1.2711e-03, 9.6548e-04, 1.4100e-03,\n",
       "          6.5482e-04, 2.0669e-04, 2.3459e-03, 2.0491e-05, 6.8047e-04, 9.5669e-04,\n",
       "          1.1218e-03, 2.4372e-05, 6.1862e-06, 1.5585e-05, 1.0322e-03, 2.3306e-05,\n",
       "          3.7345e-04, 1.2350e-05, 5.9095e-05, 4.1849e-04, 8.1068e-06, 4.4076e-03,\n",
       "          6.8761e-06, 2.5850e-03, 1.1585e-03, 4.6275e-03, 1.9472e-04, 2.3811e-03,\n",
       "          8.7263e-04, 9.4561e-05, 4.3335e-04, 1.2939e-04, 2.5666e-03, 1.4954e-05,\n",
       "          4.1117e-05, 7.4366e-04, 3.9035e-05, 1.7822e-05, 2.1694e-04, 2.1804e-03,\n",
       "          2.9220e-05, 3.0068e-03, 2.1437e-03, 1.1589e-04, 1.1002e-04, 5.5642e-06,\n",
       "          1.7258e-05, 7.1917e-04, 1.9438e-04, 1.9996e-05, 1.7935e-05, 4.2538e-05,\n",
       "          7.0299e-06, 5.2673e-04, 7.6634e-06, 1.2792e-03, 5.7868e-06, 1.8789e-04,\n",
       "          2.0290e-05, 4.5184e-04, 1.1049e-05, 6.3786e-05, 1.7552e-04, 9.0560e-04,\n",
       "          3.5903e-04, 1.9378e-03, 4.6587e-05, 2.8590e-03, 9.1779e-05, 1.4929e-03,\n",
       "          4.9867e-06, 2.2171e-03, 1.1434e-05, 1.3795e-05, 7.5017e-06, 7.3385e-04,\n",
       "          4.4567e-05, 3.1000e-03, 1.0596e-05, 3.8416e-05, 6.8489e-05, 1.5789e-03,\n",
       "          2.2058e-03, 2.2464e-05, 2.0103e-03, 1.3387e-05, 1.6357e-03, 1.2062e-05,\n",
       "          2.1775e-05, 5.3269e-06, 8.2992e-05, 8.1248e-05, 5.3877e-06, 2.5280e-03,\n",
       "          6.9646e-04, 5.6464e-06, 2.6355e-03, 5.6378e-06, 4.1436e-05, 4.7633e-05,\n",
       "          1.7494e-05, 2.8482e-03, 1.3812e-05, 1.3683e-03, 5.0977e-05, 1.7155e-03,\n",
       "          4.6941e-04, 2.2542e-03, 1.4518e-03, 4.5667e-05, 1.5940e-05, 6.7364e-06,\n",
       "          1.6885e-03, 9.3435e-06, 5.4449e-04, 1.6957e-05, 2.1423e-05, 1.3410e-03,\n",
       "          1.4796e-03, 1.1754e-05, 3.8035e-04, 1.6443e-05, 5.0180e-04, 1.1356e-03,\n",
       "          2.4054e-05, 1.1903e-03, 3.1446e-05, 2.3274e-05, 1.9319e-03, 1.6536e-05,\n",
       "          1.5345e-04, 8.6093e-04, 3.0429e-05, 1.4176e-05, 5.2904e-05, 1.2357e-03,\n",
       "          1.8966e-05, 1.4534e-05, 2.4706e-04, 1.9063e-03, 4.6845e-04, 6.3874e-04,\n",
       "          2.7530e-03, 1.1623e-03, 7.6726e-06, 1.8308e-04, 1.0120e-03, 9.5756e-05,\n",
       "          1.7049e-05, 5.2535e-05, 1.6430e-03, 1.8162e-03, 1.4810e-03, 1.0125e-05,\n",
       "          1.1759e-03, 3.4343e-03, 1.6205e-05, 8.3591e-06, 2.0633e-05, 2.4453e-04,\n",
       "          1.0497e-03, 2.5260e-03, 2.4128e-05, 2.3897e-03, 7.3834e-05, 2.7032e-03,\n",
       "          1.1240e-03, 1.0824e-03, 1.6412e-03, 1.8966e-05, 2.0797e-03, 1.1965e-03,\n",
       "          1.7577e-05, 8.8609e-06, 9.4461e-06, 6.7459e-05, 8.4835e-04, 8.1277e-05,\n",
       "          1.4801e-04, 9.9847e-06, 1.1426e-03, 4.5730e-05, 8.2877e-04, 1.7222e-04,\n",
       "          3.6649e-05, 2.7897e-05]),\n",
       "  tensor([8.6839e-05, 1.4749e-03, 2.0626e-04, 1.5866e-03, 1.3097e-03, 7.0004e-05,\n",
       "          4.9801e-05, 2.1117e-05, 2.7783e-05, 2.5288e-05, 1.9463e-04, 2.4774e-04,\n",
       "          2.2661e-03, 2.4555e-05, 2.4991e-05, 1.2181e-03, 3.7092e-04, 2.5482e-05,\n",
       "          4.6315e-05, 4.8007e-04, 1.1279e-04, 3.1446e-05, 5.5914e-05, 2.1669e-05,\n",
       "          2.0584e-05, 4.4841e-03, 2.2591e-03, 2.9039e-05, 8.7370e-05, 2.1511e-05,\n",
       "          2.5799e-05, 1.6507e-03, 2.6487e-03, 8.4607e-04, 1.8514e-03, 1.8474e-03,\n",
       "          5.5847e-03, 2.6421e-05, 9.9900e-04, 2.2954e-05, 3.8467e-05, 3.9277e-05,\n",
       "          2.7538e-05, 7.2690e-05, 1.6947e-04, 3.1977e-05, 7.3633e-05, 1.9393e-05,\n",
       "          8.8296e-05, 1.8643e-04, 1.4147e-03, 6.4801e-04, 1.5375e-04, 9.0968e-04,\n",
       "          2.6666e-05, 1.0318e-04, 1.4318e-03, 2.6811e-03, 4.4104e-04, 3.8653e-03,\n",
       "          4.1720e-05, 1.8569e-04, 2.4621e-05, 1.8292e-03, 4.9622e-04, 4.4182e-05,\n",
       "          7.7934e-04, 1.7514e-04, 2.0857e-03, 9.6735e-04, 7.2884e-04, 4.0654e-04,\n",
       "          4.0748e-03, 2.8462e-03, 7.5976e-05, 2.4614e-05, 2.2821e-05, 5.7106e-05,\n",
       "          3.0037e-05, 1.1467e-03, 2.2526e-05, 2.3075e-05, 1.4900e-03, 2.0871e-05,\n",
       "          5.1849e-04, 2.4252e-05, 3.4387e-05, 4.4384e-05, 2.5973e-05, 2.1049e-05,\n",
       "          1.5289e-04, 2.3950e-05, 2.6083e-05, 3.3860e-05, 3.1627e-05, 1.4450e-03,\n",
       "          1.0071e-03, 3.6872e-04, 2.0819e-05, 4.1240e-05, 8.2096e-04, 4.5389e-04,\n",
       "          2.7430e-05, 2.3209e-03, 7.1377e-05, 4.1081e-05, 2.6376e-05, 8.5963e-05,\n",
       "          2.2758e-03, 2.1282e-05, 2.2108e-03, 2.1452e-04, 8.7530e-05, 2.7027e-05,\n",
       "          1.4930e-03, 1.8636e-05, 5.6831e-04, 3.1397e-05, 3.1198e-05, 4.2181e-04,\n",
       "          1.4473e-04, 3.4751e-04, 1.6022e-04, 2.1220e-05, 6.4860e-04, 2.0482e-05,\n",
       "          3.0657e-05, 4.7286e-04, 8.7007e-04, 1.9117e-05, 8.4071e-05, 2.5141e-04,\n",
       "          8.1026e-05, 1.4449e-04, 3.3983e-04, 1.0722e-04, 1.0146e-04, 3.0716e-03,\n",
       "          7.6612e-04, 2.3837e-03, 4.1419e-04, 2.8287e-04, 2.3838e-03, 5.3783e-04,\n",
       "          2.2459e-05, 2.8318e-05, 1.8613e-03, 1.6910e-03, 7.3801e-05, 2.3370e-04,\n",
       "          3.4838e-04, 2.5376e-05, 3.2160e-04, 1.0513e-03, 2.8751e-05, 2.0149e-05,\n",
       "          1.7480e-03, 1.9148e-05, 2.5174e-05, 3.9089e-03, 2.3024e-05, 2.3397e-05,\n",
       "          2.5725e-05, 1.9160e-03, 1.1194e-04, 4.1564e-05, 4.0587e-05, 6.1516e-05,\n",
       "          4.5457e-04, 3.5411e-05, 1.6839e-03, 3.2252e-05, 2.0008e-05, 1.3658e-03,\n",
       "          1.7463e-04, 8.1595e-05, 3.1897e-05, 2.7079e-03, 8.5305e-04, 2.8894e-05,\n",
       "          3.4140e-04, 4.9502e-03, 2.3905e-05, 1.1690e-03, 1.8962e-03, 4.9230e-04,\n",
       "          7.4467e-05, 4.5580e-04, 1.3017e-03, 4.3135e-03, 1.8267e-03, 2.3635e-04,\n",
       "          1.6452e-03, 6.0346e-04, 2.1579e-05, 1.0963e-04, 1.5726e-04, 2.9971e-04,\n",
       "          2.2175e-03, 2.3918e-03, 4.8949e-05, 2.3106e-05, 3.1798e-03, 6.1520e-04,\n",
       "          1.9809e-04, 3.3843e-05, 4.4902e-05, 1.4218e-04, 2.1375e-03, 3.2042e-04,\n",
       "          3.4726e-04, 4.7836e-05, 2.7820e-05, 1.1118e-03, 7.0226e-05, 9.2626e-05,\n",
       "          3.6395e-05, 8.2822e-05, 3.4269e-05, 1.4888e-03, 5.8611e-05, 1.5056e-03,\n",
       "          2.1342e-03, 3.1284e-05, 2.2448e-05, 1.1215e-03, 8.0960e-05, 2.6752e-05,\n",
       "          5.8160e-05, 4.6441e-05, 4.2113e-04, 1.5391e-04, 9.9533e-05, 5.1338e-05,\n",
       "          8.9933e-04, 2.5521e-05, 2.8143e-05, 3.1409e-04, 5.5669e-04, 4.3303e-05,\n",
       "          3.7582e-05, 4.5786e-04, 1.8009e-03, 1.7894e-03, 2.1067e-05, 2.6851e-05,\n",
       "          2.2101e-05, 3.5719e-05, 1.5922e-03, 7.0909e-05, 4.4395e-03, 7.6764e-04,\n",
       "          7.6087e-05, 3.2098e-05, 6.2739e-03, 8.7816e-05, 4.7000e-05, 5.0878e-04,\n",
       "          1.1112e-04, 1.9272e-04, 9.5799e-04, 2.2142e-05, 5.2540e-03, 2.4353e-05,\n",
       "          9.4221e-04, 1.9047e-03, 2.1815e-05, 2.0312e-05, 1.2053e-04, 6.9232e-05,\n",
       "          6.3196e-05, 4.3569e-05, 2.3197e-04, 1.2553e-03, 2.4946e-05, 5.0628e-04,\n",
       "          2.6913e-04, 1.5690e-03, 6.8037e-05, 2.4532e-05, 3.4447e-05, 4.7352e-04,\n",
       "          1.2044e-03, 2.2001e-05, 1.1688e-04, 2.4454e-05, 9.7082e-05, 1.1568e-03,\n",
       "          1.8700e-03, 2.7679e-05, 2.7706e-05, 2.0341e-05, 4.5509e-05, 9.5690e-05,\n",
       "          3.1772e-04, 7.3721e-05, 2.2917e-04, 4.4727e-05, 6.1192e-04, 3.6248e-05,\n",
       "          1.3158e-03, 2.3748e-04, 2.3543e-03, 2.8870e-05, 6.7406e-05, 2.3202e-05,\n",
       "          2.0779e-04, 2.0311e-04, 2.4055e-05, 1.9856e-03, 3.9389e-04, 3.3009e-05,\n",
       "          7.9654e-05, 5.1026e-05, 1.9563e-04, 3.0897e-03, 4.0092e-05, 2.7450e-05,\n",
       "          5.8595e-04, 5.0307e-04, 2.2271e-03, 1.1603e-04, 3.8985e-05, 1.7394e-03,\n",
       "          1.9883e-05, 2.2044e-05, 2.4152e-05, 4.4996e-04, 2.6331e-03, 2.3210e-04,\n",
       "          2.2185e-04, 7.3486e-05, 2.5024e-04, 3.3804e-04, 3.0609e-05, 3.0140e-05,\n",
       "          1.3968e-04, 1.9485e-05, 1.4685e-04, 3.1904e-05, 2.3459e-05, 3.1423e-05,\n",
       "          8.6378e-05, 2.2967e-05, 4.5105e-05, 2.1934e-05, 3.0893e-05, 7.6835e-04,\n",
       "          3.7596e-05, 8.5146e-05, 2.4589e-05, 7.4259e-04, 9.7600e-05, 1.8091e-03,\n",
       "          2.1345e-05, 3.3290e-05, 2.3968e-05, 2.6452e-05, 1.7748e-04, 2.7064e-04,\n",
       "          5.1403e-03, 2.4370e-05, 1.2668e-04, 1.1854e-04, 1.8039e-03, 1.0736e-04,\n",
       "          5.5617e-05, 4.5121e-03, 8.7649e-05, 2.3871e-05, 3.0178e-03, 4.3843e-05,\n",
       "          2.9088e-05, 4.1067e-05, 2.8572e-05, 2.0380e-05, 1.3024e-03, 2.9515e-03,\n",
       "          4.6136e-03, 6.2593e-05, 1.6453e-04, 2.4481e-05, 1.3451e-03, 2.3037e-05,\n",
       "          2.3500e-03, 3.1260e-05, 8.4169e-05, 3.7388e-05, 1.7387e-03, 2.6238e-05,\n",
       "          2.7916e-05, 8.8605e-05, 2.5617e-03, 2.5392e-05, 2.6202e-05, 5.2528e-03,\n",
       "          3.7966e-05, 1.6034e-04, 2.3046e-05, 1.2923e-04, 5.3265e-03, 2.3082e-05,\n",
       "          2.2618e-05, 2.0454e-05, 4.8658e-05, 3.3630e-05, 9.4507e-05, 3.2126e-05,\n",
       "          2.2015e-05, 3.1211e-03, 9.9748e-05, 2.1084e-04, 1.3368e-03, 3.2735e-05,\n",
       "          2.3593e-05, 3.3623e-04, 2.0051e-05, 2.3176e-05, 1.0751e-04, 5.4374e-03,\n",
       "          3.9604e-05, 3.0929e-05, 1.3336e-03, 2.8041e-05, 4.9644e-05, 7.1137e-04,\n",
       "          2.0304e-05, 5.5372e-05, 5.8569e-03, 1.1096e-04, 2.6448e-05, 3.8710e-04,\n",
       "          1.5355e-04, 2.4179e-05, 5.7266e-04, 2.2387e-05, 9.9881e-05, 1.8442e-03,\n",
       "          1.2811e-04, 1.7581e-04, 2.6005e-03, 3.6061e-03, 9.0052e-04, 2.5844e-04,\n",
       "          2.9743e-05, 4.1088e-05, 2.4966e-05, 1.3953e-04, 4.4582e-04, 2.7640e-05,\n",
       "          5.0959e-03, 2.2152e-05, 2.5544e-04, 1.3612e-03, 1.9094e-05, 2.4275e-03,\n",
       "          2.7755e-05, 6.4969e-05, 7.0336e-05, 1.7330e-04, 3.4652e-05, 3.8028e-04,\n",
       "          2.4790e-03, 6.4581e-05, 6.5256e-04, 3.0191e-05, 5.5056e-05, 1.8928e-03,\n",
       "          7.9391e-05, 5.6776e-04, 4.0496e-03, 1.1931e-04, 1.2836e-04, 2.3321e-03,\n",
       "          4.2814e-05, 3.8796e-04, 6.1034e-05, 5.6722e-05, 2.2455e-05, 2.7822e-05,\n",
       "          1.9990e-05, 2.6174e-05, 7.3258e-05, 5.5627e-05, 8.7505e-05, 5.5718e-05,\n",
       "          5.7839e-05, 4.3672e-05, 3.5648e-05, 1.8901e-03, 2.3230e-05, 4.2228e-04,\n",
       "          2.8814e-05, 1.9266e-04, 1.9050e-04, 3.3442e-03, 3.3289e-04, 5.4692e-05,\n",
       "          1.5953e-03, 8.0671e-05, 8.6624e-04, 6.7144e-05, 2.6674e-03, 2.2411e-05,\n",
       "          3.0765e-05, 2.2537e-05, 2.9045e-04, 1.9518e-05, 3.8494e-03, 1.9481e-05,\n",
       "          1.5137e-04, 4.5875e-03])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_weight = torch.zeros(num_weight)\n",
    "gamma_weight_record = []\n",
    "\n",
    "index = 0\n",
    "for k, m in enumerate(model.feature):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        gamma = activation_based_gamma(m.weight.data)\n",
    "        gamma_weight_record.append(gamma)\n",
    "        \n",
    "        size = m.weight.data.shape[0]\n",
    "        gamma_weight[index:(index+size)] = gamma.clone()\n",
    "        index += size\n",
    "\n",
    "gamma_weight[:100], gamma_weight_record[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3852, tensor(0.0015))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_rate = 0.7\n",
    "y, i = torch.sort(gamma_weight)\n",
    "thre_index = int(num_weight * pruning_rate)\n",
    "thre = y[thre_index]\n",
    "\n",
    "thre_index, thre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.3 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 0\t total channel: 64\tremaining channel: 40\n",
      "layer index: 3\t total channel: 64\tremaining channel: 64\n",
      "layer index: 7\t total channel: 128\tremaining channel: 127\n",
      "layer index: 10\t total channel: 128\tremaining channel: 128\n",
      "layer index: 14\t total channel: 256\tremaining channel: 84\n",
      "layer index: 17\t total channel: 256\tremaining channel: 88\n",
      "layer index: 20\t total channel: 256\tremaining channel: 137\n",
      "layer index: 23\t total channel: 256\tremaining channel: 158\n",
      "layer index: 27\t total channel: 512\tremaining channel: 123\n",
      "layer index: 30\t total channel: 512\tremaining channel: 88\n",
      "layer index: 33\t total channel: 512\tremaining channel: 104\n",
      "layer index: 36\t total channel: 512\tremaining channel: 98\n",
      "layer index: 40\t total channel: 512\tremaining channel: 97\n",
      "layer index: 43\t total channel: 512\tremaining channel: 105\n",
      "layer index: 46\t total channel: 512\tremaining channel: 119\n",
      "layer index: 49\t total channel: 512\tremaining channel: 91\n",
      "Pruned ratio: 0.700036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_pruned = 0  # 裁剪个数\n",
    "cfg_num = []\n",
    "\n",
    "for k, m in enumerate(model.feature):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        gamma = activation_based_gamma(m.weight.data)\n",
    "        mask = gamma.gt(thre).float()\n",
    "        \n",
    "        num_remain = int(torch.sum(mask))\n",
    "        num_pruned += mask.shape[0] - num_remain\n",
    "        \n",
    "        cfg_num.append(num_remain)\n",
    "        \n",
    "        print('layer index: {:d}\\t total channel: {:d}\\tremaining channel: {:d}'.\n",
    "                      format(k, mask.shape[0], num_remain))\n",
    "\n",
    "pruned_ratio = num_pruned / num_weight\n",
    "print('Pruned ratio: {:4f}\\r\\n'.format(pruned_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Pruning the Feature Map\n",
    "\n",
    "##### 4.3.1 one batch gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0000, 0.0178, 0.0036, 0.0000, 0.0145, 0.0092, 0.0000, 0.0000, 0.0169,\n",
       "         0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.0200, 0.0063, 0.0056, 0.0000,\n",
       "         0.0000, 0.0000, 0.0067, 0.0158, 0.0000, 0.0307, 0.0000, 0.0000, 0.0290,\n",
       "         0.0069, 0.0277, 0.0029, 0.0178, 0.0021, 0.0156, 0.0000, 0.0270, 0.0092,\n",
       "         0.0072, 0.0059, 0.0000, 0.0011, 0.0000, 0.0000, 0.0288, 0.0000, 0.0000,\n",
       "         0.0048, 0.0179, 0.0059, 0.0000, 0.0000, 0.0040, 0.0197, 0.0000, 0.0000,\n",
       "         0.0233, 0.0188, 0.0000, 0.0183, 0.0013, 0.0306, 0.0000, 0.0262, 0.0039,\n",
       "         0.0000]),\n",
       " tensor([0.0089, 0.0257, 0.0184, 0.0111, 0.0057, 0.0117, 0.0248, 0.0110, 0.0136,\n",
       "         0.0206, 0.0160, 0.0184, 0.0122, 0.0098, 0.0102, 0.0091, 0.0141, 0.0108,\n",
       "         0.0117, 0.0173, 0.0129, 0.0313, 0.0105, 0.0233, 0.0127, 0.0110, 0.0084,\n",
       "         0.0105, 0.0215, 0.0218, 0.0220, 0.0232, 0.0030, 0.0163, 0.0091, 0.0256,\n",
       "         0.0249, 0.0130, 0.0128, 0.0108, 0.0115, 0.0095, 0.0003, 0.0077, 0.0175,\n",
       "         0.0256, 0.0129, 0.0429, 0.0281, 0.0180, 0.0134, 0.0165, 0.0075, 0.0227,\n",
       "         0.0117, 0.0220, 0.0171, 0.0209, 0.0219, 0.0219, 0.0071, 0.0066, 0.0122,\n",
       "         0.0162]),\n",
       " tensor([0.0049, 0.0074, 0.0048, 0.0075, 0.0082, 0.0077, 0.0125, 0.0078, 0.0051,\n",
       "         0.0046, 0.0074, 0.0063, 0.0051, 0.0052, 0.0081, 0.0032, 0.0013, 0.0080,\n",
       "         0.0073, 0.0064, 0.0065, 0.0053, 0.0028, 0.0057, 0.0086, 0.0082, 0.0065,\n",
       "         0.0058, 0.0042, 0.0030, 0.0101, 0.0032, 0.0055, 0.0176, 0.0013, 0.0143,\n",
       "         0.0081, 0.0045, 0.0077, 0.0064, 0.0048, 0.0054, 0.0072, 0.0087, 0.0056,\n",
       "         0.0073, 0.0062, 0.0046, 0.0088, 0.0102, 0.0088, 0.0178, 0.0073, 0.0103,\n",
       "         0.0064, 0.0097, 0.0122, 0.0156, 0.0036, 0.0084, 0.0030, 0.0085, 0.0007,\n",
       "         0.0110, 0.0112, 0.0060, 0.0035, 0.0063, 0.0062, 0.0074, 0.0069, 0.0154,\n",
       "         0.0044, 0.0075, 0.0064, 0.0046, 0.0087, 0.0055, 0.0081, 0.0104, 0.0065,\n",
       "         0.0069, 0.0091, 0.0100, 0.0078, 0.0074, 0.0070, 0.0121, 0.0123, 0.0082,\n",
       "         0.0056, 0.0049, 0.0055, 0.0086, 0.0066, 0.0089, 0.0099, 0.0086, 0.0077,\n",
       "         0.0058, 0.0052, 0.0040, 0.0098, 0.0055, 0.0111, 0.0108, 0.0084, 0.0000,\n",
       "         0.0066, 0.0081, 0.0076, 0.0110, 0.0038, 0.0049, 0.0066, 0.0092, 0.0035,\n",
       "         0.0061, 0.0108, 0.0086, 0.0051, 0.0066, 0.0072, 0.0040, 0.0100, 0.0052,\n",
       "         0.0087, 0.0106]),\n",
       " tensor([0.0062, 0.0267, 0.0098, 0.0225, 0.0081, 0.0186, 0.0214, 0.0046, 0.0156,\n",
       "         0.0149, 0.0164, 0.0118, 0.0138, 0.0189, 0.0066, 0.0147, 0.0129, 0.0070,\n",
       "         0.0172, 0.0081, 0.0112, 0.0111, 0.0130, 0.0089, 0.0108, 0.0304, 0.0147,\n",
       "         0.0094, 0.0182, 0.0172, 0.0088, 0.0107, 0.0182, 0.0243, 0.0173, 0.0088,\n",
       "         0.0085, 0.0163, 0.0148, 0.0073, 0.0089, 0.0067, 0.0115, 0.0131, 0.0112,\n",
       "         0.0087, 0.0130, 0.0056, 0.0127, 0.0082, 0.0073, 0.0108, 0.0128, 0.0157,\n",
       "         0.0111, 0.0223, 0.0044, 0.0152, 0.0085, 0.0177, 0.0127, 0.0235, 0.0139,\n",
       "         0.0151, 0.0173, 0.0094, 0.0050, 0.0152, 0.0150, 0.0288, 0.0079, 0.0116,\n",
       "         0.0131, 0.0102, 0.0125, 0.0056, 0.0080, 0.0075, 0.0148, 0.0166, 0.0215,\n",
       "         0.0198, 0.0111, 0.0143, 0.0116, 0.0104, 0.0123, 0.0224, 0.0051, 0.0191,\n",
       "         0.0096, 0.0143, 0.0192, 0.0084, 0.0204, 0.0122, 0.0121, 0.0157, 0.0067,\n",
       "         0.0043, 0.0106, 0.0135, 0.0222, 0.0220, 0.0075, 0.0105, 0.0214, 0.0049,\n",
       "         0.0111, 0.0168, 0.0078, 0.0179, 0.0153, 0.0220, 0.0225, 0.0067, 0.0160,\n",
       "         0.0148, 0.0109, 0.0099, 0.0293, 0.0104, 0.0098, 0.0067, 0.0065, 0.0082,\n",
       "         0.0096, 0.0249]),\n",
       " tensor([0.0030, 0.0064, 0.0009, 0.0075, 0.0067, 0.0092, 0.0137, 0.0108, 0.0049,\n",
       "         0.0059, 0.0060, 0.0170, 0.0092, 0.0000, 0.0028, 0.0019, 0.0083, 0.0092,\n",
       "         0.0035, 0.0063, 0.0096, 0.0070, 0.0138, 0.0028, 0.0095, 0.0056, 0.0046,\n",
       "         0.0194, 0.0021, 0.0007, 0.0063, 0.0082, 0.0038, 0.0127, 0.0089, 0.0089,\n",
       "         0.0053, 0.0054, 0.0127, 0.0021, 0.0082, 0.0039, 0.0027, 0.0072, 0.0003,\n",
       "         0.0019, 0.0022, 0.0094, 0.0119, 0.0164, 0.0010, 0.0000, 0.0047, 0.0098,\n",
       "         0.0108, 0.0098, 0.0083, 0.0073, 0.0004, 0.0119, 0.0066, 0.0074, 0.0093,\n",
       "         0.0054, 0.0067, 0.0089, 0.0023, 0.0065, 0.0102, 0.0079, 0.0163, 0.0104,\n",
       "         0.0068, 0.0036, 0.0048, 0.0010, 0.0041, 0.0031, 0.0028, 0.0048, 0.0051,\n",
       "         0.0071, 0.0071, 0.0113, 0.0058, 0.0093, 0.0066, 0.0016, 0.0050, 0.0093,\n",
       "         0.0090, 0.0071, 0.0080, 0.0142, 0.0028, 0.0027, 0.0074, 0.0064, 0.0059,\n",
       "         0.0089, 0.0059, 0.0085, 0.0118, 0.0016, 0.0036, 0.0149, 0.0037, 0.0073,\n",
       "         0.0041, 0.0074, 0.0130, 0.0170, 0.0084, 0.0052, 0.0039, 0.0026, 0.0047,\n",
       "         0.0046, 0.0054, 0.0048, 0.0087, 0.0140, 0.0050, 0.0055, 0.0062, 0.0103,\n",
       "         0.0071, 0.0038, 0.0067, 0.0113, 0.0111, 0.0086, 0.0003, 0.0116, 0.0055,\n",
       "         0.0097, 0.0056, 0.0056, 0.0106, 0.0065, 0.0131, 0.0047, 0.0100, 0.0091,\n",
       "         0.0084, 0.0074, 0.0068, 0.0046, 0.0033, 0.0084, 0.0048, 0.0121, 0.0071,\n",
       "         0.0073, 0.0088, 0.0059, 0.0044, 0.0083, 0.0100, 0.0027, 0.0031, 0.0083,\n",
       "         0.0073, 0.0049, 0.0029, 0.0123, 0.0027, 0.0101, 0.0114, 0.0067, 0.0042,\n",
       "         0.0051, 0.0086, 0.0033, 0.0043, 0.0036, 0.0085, 0.0060, 0.0037, 0.0102,\n",
       "         0.0100, 0.0046, 0.0077, 0.0104, 0.0005, 0.0093, 0.0065, 0.0024, 0.0071,\n",
       "         0.0043, 0.0023, 0.0060, 0.0073, 0.0015, 0.0069, 0.0065, 0.0082, 0.0013,\n",
       "         0.0101, 0.0005, 0.0091, 0.0038, 0.0131, 0.0080, 0.0071, 0.0112, 0.0022,\n",
       "         0.0079, 0.0079, 0.0082, 0.0074, 0.0102, 0.0076, 0.0148, 0.0044, 0.0124,\n",
       "         0.0039, 0.0108, 0.0001, 0.0112, 0.0057, 0.0037, 0.0083, 0.0053, 0.0059,\n",
       "         0.0085, 0.0068, 0.0016, 0.0047, 0.0029, 0.0154, 0.0145, 0.0095, 0.0090,\n",
       "         0.0060, 0.0035, 0.0103, 0.0109, 0.0069, 0.0050, 0.0070, 0.0052, 0.0047,\n",
       "         0.0098, 0.0035, 0.0080, 0.0080, 0.0032, 0.0088, 0.0137, 0.0078, 0.0090,\n",
       "         0.0086, 0.0000, 0.0040, 0.0069]),\n",
       " tensor([6.8265e-03, 1.9107e-02, 2.0365e-02, 7.1548e-03, 1.0141e-02, 3.6738e-03,\n",
       "         5.2894e-03, 4.9407e-03, 1.9948e-03, 2.9805e-03, 4.0691e-03, 7.5153e-04,\n",
       "         3.2139e-03, 1.0728e-02, 1.0297e-02, 4.9756e-03, 1.3372e-02, 3.4554e-03,\n",
       "         2.4099e-03, 0.0000e+00, 8.5951e-03, 2.5077e-03, 2.0599e-03, 1.6551e-02,\n",
       "         9.7893e-03, 2.7517e-02, 1.3261e-02, 1.1452e-02, 3.2736e-03, 5.0677e-03,\n",
       "         1.9895e-04, 4.8275e-03, 4.7360e-03, 6.3096e-03, 1.6724e-02, 7.7387e-03,\n",
       "         1.0888e-02, 5.1666e-03, 5.3330e-03, 6.5741e-03, 1.2257e-02, 1.0149e-02,\n",
       "         1.2792e-02, 8.2113e-03, 1.1958e-02, 3.5861e-03, 9.7188e-03, 1.0347e-02,\n",
       "         8.0617e-03, 0.0000e+00, 2.6718e-03, 1.7925e-02, 6.2277e-03, 0.0000e+00,\n",
       "         8.0931e-03, 1.0672e-02, 1.1524e-02, 4.3839e-03, 9.9779e-03, 1.4434e-02,\n",
       "         1.8902e-03, 2.4920e-04, 3.8408e-03, 2.2798e-02, 1.6175e-02, 8.5192e-03,\n",
       "         4.0751e-03, 8.4278e-03, 5.2893e-03, 5.9494e-03, 1.6624e-02, 9.2056e-03,\n",
       "         1.0825e-02, 5.3593e-03, 6.6461e-03, 7.6109e-03, 3.2329e-03, 8.6359e-03,\n",
       "         1.3612e-02, 3.9619e-03, 2.2714e-02, 8.5651e-03, 5.9905e-03, 8.5677e-03,\n",
       "         8.7614e-03, 1.1657e-02, 1.1403e-02, 1.7148e-03, 7.1333e-03, 1.5158e-03,\n",
       "         6.3717e-03, 1.0874e-02, 4.7815e-03, 2.0058e-02, 1.6940e-02, 1.8596e-02,\n",
       "         7.3533e-04, 1.6715e-02, 6.9973e-03, 7.3614e-03, 7.4051e-03, 1.1697e-02,\n",
       "         4.1266e-03, 2.5910e-03, 1.0252e-02, 1.8762e-02, 8.5603e-03, 1.3512e-02,\n",
       "         1.3129e-02, 1.2213e-02, 7.6154e-03, 1.6906e-03, 2.2921e-03, 1.2267e-02,\n",
       "         1.7784e-02, 8.2835e-03, 1.2768e-02, 0.0000e+00, 4.5224e-03, 1.0446e-02,\n",
       "         3.0835e-03, 0.0000e+00, 6.7249e-03, 3.5247e-03, 1.4796e-02, 1.0195e-02,\n",
       "         0.0000e+00, 1.8815e-03, 4.1028e-03, 8.4037e-04, 4.3394e-03, 1.6983e-02,\n",
       "         4.4760e-03, 0.0000e+00, 9.4389e-03, 1.6816e-02, 1.3354e-02, 1.0443e-02,\n",
       "         7.0179e-05, 3.8480e-03, 1.4792e-02, 9.8494e-03, 1.3721e-02, 1.0033e-02,\n",
       "         1.1651e-02, 8.9277e-03, 6.7998e-03, 1.2929e-02, 6.7069e-03, 4.4826e-03,\n",
       "         4.4955e-03, 5.5132e-04, 5.0275e-03, 1.0381e-02, 1.4177e-02, 1.8043e-03,\n",
       "         1.3384e-02, 1.2710e-03, 4.4773e-03, 1.3445e-02, 9.0644e-03, 1.1161e-02,\n",
       "         1.1646e-03, 1.5840e-02, 0.0000e+00, 4.7514e-03, 1.1151e-02, 9.1235e-03,\n",
       "         3.3435e-03, 0.0000e+00, 9.5748e-03, 6.8636e-03, 2.3428e-04, 1.7141e-03,\n",
       "         0.0000e+00, 7.6356e-03, 0.0000e+00, 7.5767e-03, 2.1281e-03, 6.4613e-03,\n",
       "         1.4636e-02, 2.6207e-03, 8.0820e-03, 1.0655e-02, 4.6693e-04, 8.9374e-03,\n",
       "         5.9753e-03, 8.2121e-03, 5.7222e-03, 7.2096e-03, 4.4005e-03, 1.8967e-02,\n",
       "         7.4047e-03, 0.0000e+00, 6.8430e-03, 1.3438e-02, 5.5326e-03, 5.0416e-03,\n",
       "         8.1571e-03, 6.8972e-03, 2.3168e-03, 1.9885e-02, 3.0754e-03, 6.8761e-03,\n",
       "         1.8924e-02, 3.1666e-03, 1.2366e-02, 1.9285e-03, 8.8954e-03, 2.1461e-02,\n",
       "         2.9946e-03, 1.6379e-02, 1.8140e-03, 7.5366e-03, 0.0000e+00, 1.6759e-03,\n",
       "         0.0000e+00, 5.9996e-03, 1.0075e-02, 5.6990e-03, 8.9607e-03, 1.7304e-02,\n",
       "         1.3345e-02, 1.3409e-02, 3.3663e-03, 1.3092e-02, 8.2146e-03, 6.7318e-03,\n",
       "         4.4808e-03, 1.3041e-03, 1.7003e-02, 1.0611e-02, 4.6484e-03, 3.2678e-02,\n",
       "         1.5065e-02, 1.7741e-02, 0.0000e+00, 1.3657e-02, 1.0563e-02, 5.1332e-03,\n",
       "         1.2979e-02, 0.0000e+00, 0.0000e+00, 1.8525e-02, 1.6590e-02, 5.3844e-03,\n",
       "         2.0987e-03, 1.3429e-02, 4.2009e-03, 7.1879e-03, 1.2785e-03, 7.3729e-03,\n",
       "         1.2315e-02, 9.6403e-04, 5.2654e-03, 9.5400e-03]),\n",
       " tensor([1.0000e-02, 5.3955e-03, 1.5066e-02, 2.2333e-03, 0.0000e+00, 2.3653e-04,\n",
       "         6.3034e-04, 0.0000e+00, 1.8418e-02, 7.3119e-03, 8.5516e-03, 1.0344e-02,\n",
       "         2.3011e-02, 4.0384e-03, 5.4473e-03, 8.1903e-03, 3.0958e-03, 9.8555e-03,\n",
       "         3.8267e-04, 5.8177e-03, 1.0184e-02, 0.0000e+00, 1.2324e-03, 0.0000e+00,\n",
       "         0.0000e+00, 2.8932e-02, 5.4313e-03, 0.0000e+00, 0.0000e+00, 7.1237e-04,\n",
       "         0.0000e+00, 2.2093e-03, 9.0267e-03, 2.1164e-02, 8.5946e-03, 2.4308e-03,\n",
       "         0.0000e+00, 0.0000e+00, 1.1880e-02, 2.8244e-02, 3.6482e-03, 7.5187e-03,\n",
       "         4.9127e-03, 5.2780e-03, 3.3403e-03, 1.9902e-03, 1.0217e-02, 2.5123e-03,\n",
       "         6.7612e-03, 2.9745e-03, 1.3752e-02, 5.0143e-03, 1.0464e-02, 2.3477e-02,\n",
       "         9.9839e-03, 1.1627e-02, 5.4256e-03, 1.4608e-02, 1.4846e-02, 5.8887e-03,\n",
       "         1.7056e-03, 0.0000e+00, 4.5152e-03, 9.2880e-03, 1.0203e-02, 4.9766e-03,\n",
       "         4.3272e-03, 6.9741e-03, 9.8591e-03, 7.8122e-03, 3.7626e-03, 3.8016e-03,\n",
       "         1.2049e-02, 1.1290e-02, 4.0457e-03, 1.6510e-02, 2.6595e-03, 4.9437e-02,\n",
       "         0.0000e+00, 3.4824e-04, 1.3140e-03, 8.0146e-03, 1.0802e-02, 1.3398e-02,\n",
       "         3.7547e-03, 4.1901e-03, 8.2813e-03, 0.0000e+00, 0.0000e+00, 7.2222e-03,\n",
       "         5.3892e-03, 0.0000e+00, 2.1071e-02, 1.3823e-03, 8.4054e-03, 0.0000e+00,\n",
       "         3.2446e-03, 0.0000e+00, 0.0000e+00, 1.4417e-02, 8.3029e-03, 1.0365e-02,\n",
       "         1.0755e-02, 1.3818e-03, 0.0000e+00, 4.5216e-03, 2.5762e-03, 1.4531e-02,\n",
       "         3.3420e-03, 3.8299e-03, 1.2321e-02, 3.6196e-03, 6.0987e-03, 3.5137e-03,\n",
       "         8.0298e-03, 1.3167e-03, 8.7197e-03, 4.7114e-03, 2.6281e-02, 7.0561e-03,\n",
       "         3.7097e-03, 1.1372e-02, 4.0084e-03, 0.0000e+00, 9.3349e-03, 1.8650e-02,\n",
       "         5.8778e-04, 0.0000e+00, 1.3068e-02, 1.1937e-02, 2.3966e-03, 6.4842e-03,\n",
       "         4.9591e-03, 8.7705e-04, 1.5283e-02, 6.1001e-03, 5.4535e-03, 1.5731e-02,\n",
       "         1.8443e-03, 3.8974e-03, 1.0400e-02, 4.1663e-04, 1.2552e-03, 1.5603e-02,\n",
       "         6.9542e-03, 5.0507e-03, 1.0035e-02, 0.0000e+00, 0.0000e+00, 5.2302e-05,\n",
       "         3.3405e-03, 0.0000e+00, 0.0000e+00, 2.7760e-03, 2.2125e-03, 7.2069e-04,\n",
       "         1.0885e-02, 5.5786e-03, 8.9529e-03, 7.7985e-03, 2.9217e-04, 1.6167e-02,\n",
       "         0.0000e+00, 0.0000e+00, 5.2923e-03, 8.3698e-03, 0.0000e+00, 7.8092e-04,\n",
       "         4.4814e-03, 0.0000e+00, 0.0000e+00, 3.7936e-03, 2.8603e-03, 1.5543e-02,\n",
       "         5.9146e-03, 1.0875e-02, 0.0000e+00, 0.0000e+00, 4.1157e-03, 0.0000e+00,\n",
       "         1.3821e-02, 8.8173e-03, 1.3068e-02, 1.1972e-02, 1.0245e-02, 2.0039e-02,\n",
       "         1.7036e-03, 6.3519e-03, 0.0000e+00, 6.6067e-03, 1.1686e-02, 8.6268e-03,\n",
       "         0.0000e+00, 5.3005e-03, 9.7681e-03, 1.0333e-02, 6.2995e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 6.2046e-04, 7.7993e-03, 5.5930e-03, 1.0458e-02,\n",
       "         2.9982e-03, 0.0000e+00, 1.3490e-02, 8.4428e-03, 7.5544e-03, 0.0000e+00,\n",
       "         9.7037e-03, 4.5409e-03, 1.0581e-02, 1.6098e-02, 6.8319e-03, 2.6942e-03,\n",
       "         7.5987e-03, 8.6431e-03, 3.1940e-03, 5.4828e-03, 8.0372e-03, 0.0000e+00,\n",
       "         5.7677e-03, 1.8136e-03, 1.1258e-02, 1.0123e-02, 1.2149e-02, 2.4987e-02,\n",
       "         1.2368e-03, 1.0611e-02, 3.9133e-03, 1.3394e-03, 1.3750e-02, 9.7178e-03,\n",
       "         0.0000e+00, 0.0000e+00, 1.0991e-02, 1.1265e-03, 1.3558e-02, 0.0000e+00,\n",
       "         3.2756e-03, 0.0000e+00, 6.2967e-03, 9.8570e-03, 2.6177e-02, 1.3929e-02,\n",
       "         1.0204e-02, 5.2671e-03, 2.8726e-03, 6.7789e-03, 1.0147e-02, 6.8460e-03,\n",
       "         1.5370e-04, 3.4385e-03, 1.4552e-03, 2.7245e-03]),\n",
       " tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2373e-02, 0.0000e+00, 2.3294e-03,\n",
       "         0.0000e+00, 6.3819e-03, 2.5778e-02, 2.8292e-03, 3.5928e-02, 0.0000e+00,\n",
       "         9.6853e-03, 1.8924e-03, 8.3895e-03, 6.2164e-03, 4.8330e-03, 2.1327e-02,\n",
       "         9.3467e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6112e-02, 1.6868e-02,\n",
       "         2.8852e-03, 0.0000e+00, 3.9625e-03, 1.4923e-02, 1.4037e-02, 0.0000e+00,\n",
       "         2.1140e-02, 2.7607e-03, 6.4838e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.7276e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2267e-02, 1.0814e-02,\n",
       "         3.9842e-03, 0.0000e+00, 1.8114e-03, 5.8317e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.3286e-02, 6.6556e-03, 2.3191e-03, 0.0000e+00,\n",
       "         0.0000e+00, 1.0742e-02, 1.5386e-02, 0.0000e+00, 8.9844e-03, 7.3421e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6239e-03, 2.7262e-03, 4.9079e-03,\n",
       "         2.2884e-02, 0.0000e+00, 5.7505e-03, 3.2883e-02, 5.8986e-03, 2.2366e-02,\n",
       "         2.0557e-03, 6.8594e-03, 7.7454e-03, 0.0000e+00, 0.0000e+00, 1.0024e-02,\n",
       "         7.7215e-03, 0.0000e+00, 1.2690e-02, 0.0000e+00, 0.0000e+00, 1.5719e-02,\n",
       "         2.0063e-02, 0.0000e+00, 0.0000e+00, 1.6067e-03, 7.3846e-04, 0.0000e+00,\n",
       "         7.6726e-04, 0.0000e+00, 2.4318e-02, 1.0942e-02, 7.4681e-03, 1.0516e-02,\n",
       "         1.4532e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4294e-03, 0.0000e+00,\n",
       "         1.9773e-02, 6.2098e-05, 6.4661e-03, 1.9898e-02, 0.0000e+00, 2.0310e-02,\n",
       "         2.6537e-02, 3.2158e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.4075e-03,\n",
       "         1.6271e-03, 1.0691e-02, 5.0394e-03, 2.0307e-02, 0.0000e+00, 4.2905e-04,\n",
       "         0.0000e+00, 7.2824e-05, 9.6643e-04, 0.0000e+00, 8.8891e-03, 1.9960e-02,\n",
       "         1.1795e-02, 9.4084e-03, 6.1229e-03, 8.4254e-03, 0.0000e+00, 4.7473e-03,\n",
       "         2.1524e-02, 2.2372e-03, 1.9420e-03, 1.6139e-02, 4.9867e-03, 1.0780e-02,\n",
       "         0.0000e+00, 1.4510e-02, 2.4245e-03, 2.5125e-02, 0.0000e+00, 4.1090e-03,\n",
       "         8.4065e-03, 0.0000e+00, 2.9864e-03, 8.4709e-03, 0.0000e+00, 8.7533e-03,\n",
       "         0.0000e+00, 6.4567e-03, 0.0000e+00, 1.4752e-02, 0.0000e+00, 3.0230e-04,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6752e-02, 0.0000e+00, 2.1188e-02,\n",
       "         2.5605e-03, 1.2352e-02, 0.0000e+00, 0.0000e+00, 3.0687e-02, 1.2150e-02,\n",
       "         3.5385e-03, 2.5338e-03, 5.1088e-03, 2.1915e-03, 2.3004e-02, 4.3924e-04,\n",
       "         2.3102e-03, 7.9917e-03, 0.0000e+00, 1.0738e-02, 2.2689e-03, 1.3967e-02,\n",
       "         0.0000e+00, 3.7075e-03, 0.0000e+00, 3.4328e-03, 7.0663e-03, 0.0000e+00,\n",
       "         9.0352e-03, 6.6646e-03, 7.5453e-03, 0.0000e+00, 8.4390e-03, 5.5187e-03,\n",
       "         0.0000e+00, 5.8841e-03, 0.0000e+00, 2.2254e-02, 2.2666e-03, 3.7283e-03,\n",
       "         0.0000e+00, 0.0000e+00, 1.1716e-02, 4.8630e-03, 3.7501e-02, 1.2103e-03,\n",
       "         3.9286e-03, 0.0000e+00, 0.0000e+00, 1.6158e-02, 3.9606e-04, 0.0000e+00,\n",
       "         9.2898e-03, 0.0000e+00, 1.4119e-02, 0.0000e+00, 4.5725e-03, 1.0988e-02,\n",
       "         0.0000e+00, 3.3336e-02, 0.0000e+00, 2.9445e-04, 6.6910e-03, 6.9769e-04,\n",
       "         8.4186e-03, 0.0000e+00, 3.7324e-03, 1.8472e-02, 3.5856e-02, 1.2122e-02,\n",
       "         0.0000e+00, 2.0965e-03, 0.0000e+00, 2.1259e-02, 5.3311e-03, 0.0000e+00,\n",
       "         0.0000e+00, 2.4329e-03, 2.2775e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.0946e-02, 3.0681e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2347e-02,\n",
       "         2.6033e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7202e-03, 2.0790e-02,\n",
       "         0.0000e+00, 1.5224e-02, 1.0037e-02, 5.4587e-03]),\n",
       " tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7124e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6326e-03, 0.0000e+00, 9.5687e-03,\n",
       "         0.0000e+00, 0.0000e+00, 4.6549e-03, 0.0000e+00, 1.4158e-02, 1.9715e-02,\n",
       "         3.2144e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6519e-03, 0.0000e+00,\n",
       "         0.0000e+00, 3.1032e-03, 0.0000e+00, 8.0699e-03, 0.0000e+00, 3.8495e-03,\n",
       "         1.5217e-02, 0.0000e+00, 2.4190e-02, 0.0000e+00, 0.0000e+00, 2.9349e-03,\n",
       "         0.0000e+00, 0.0000e+00, 7.5849e-03, 3.5529e-04, 2.5833e-04, 0.0000e+00,\n",
       "         6.2503e-05, 0.0000e+00, 5.1344e-03, 7.2084e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2846e-03,\n",
       "         0.0000e+00, 2.7051e-04, 1.9499e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 6.0203e-03, 3.5235e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 3.1718e-03, 0.0000e+00, 2.4542e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4692e-04, 0.0000e+00,\n",
       "         1.1805e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.9401e-04, 2.3116e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.3168e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.4397e-02, 8.8293e-04, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6577e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.9751e-03, 0.0000e+00, 0.0000e+00, 1.6604e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2271e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6054e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.3969e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.5635e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.0213e-03, 1.2524e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         8.3482e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0593e-03,\n",
       "         0.0000e+00, 0.0000e+00, 3.3403e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.3289e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0524e-02, 6.7015e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9624e-02, 1.1164e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.6700e-02, 0.0000e+00, 0.0000e+00, 8.1486e-03, 0.0000e+00,\n",
       "         0.0000e+00, 1.7262e-02, 0.0000e+00, 0.0000e+00, 1.4788e-03, 0.0000e+00,\n",
       "         9.9654e-03, 0.0000e+00, 1.2633e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 4.5569e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2452e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3670e-03,\n",
       "         0.0000e+00, 0.0000e+00, 7.3019e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.0309e-02, 0.0000e+00, 1.2633e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.4437e-02, 5.9293e-03, 0.0000e+00, 8.8647e-03, 9.0395e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.3511e-03, 2.6434e-03, 0.0000e+00, 0.0000e+00,\n",
       "         2.5273e-02, 2.5794e-02, 3.7616e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.0646e-03, 0.0000e+00, 0.0000e+00, 1.8189e-03,\n",
       "         0.0000e+00, 8.8351e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0978e-03,\n",
       "         0.0000e+00, 7.2480e-03, 2.0772e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7689e-03,\n",
       "         0.0000e+00, 0.0000e+00, 1.0185e-02, 0.0000e+00, 0.0000e+00, 1.9412e-03,\n",
       "         3.0168e-04, 9.8653e-04, 7.8414e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3565e-06,\n",
       "         0.0000e+00, 0.0000e+00, 1.0975e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3899e-02,\n",
       "         0.0000e+00, 4.9703e-03, 1.4908e-04, 1.0186e-02, 0.0000e+00, 1.7300e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6179e-03, 0.0000e+00,\n",
       "         0.0000e+00, 1.6455e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3442e-02,\n",
       "         0.0000e+00, 7.8970e-03, 2.0296e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8893e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 9.9471e-03, 0.0000e+00, 1.0875e-03, 0.0000e+00, 1.2340e-02,\n",
       "         0.0000e+00, 2.1458e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.3998e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9361e-03,\n",
       "         2.1855e-02, 0.0000e+00, 1.4918e-02, 0.0000e+00, 6.3792e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3298e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.7244e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.0075e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1105e-02,\n",
       "         0.0000e+00, 0.0000e+00, 5.5801e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5046e-03,\n",
       "         3.6790e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2219e-03,\n",
       "         0.0000e+00, 2.3465e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4597e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         2.3294e-03, 7.9727e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.0703e-03, 9.4171e-04, 5.4890e-03, 0.0000e+00,\n",
       "         0.0000e+00, 1.1829e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.8479e-03, 0.0000e+00, 1.3800e-02, 0.0000e+00, 6.5906e-03,\n",
       "         0.0000e+00, 0.0000e+00, 2.2554e-03, 0.0000e+00, 5.9750e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5553e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]),\n",
       " tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.3743e-02, 0.0000e+00, 0.0000e+00, 1.6423e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.8964e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.4087e-02, 0.0000e+00, 2.0016e-03, 0.0000e+00,\n",
       "         3.8790e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6117e-02, 0.0000e+00, 1.9099e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2283e-03, 0.0000e+00, 0.0000e+00,\n",
       "         8.4966e-03, 1.6189e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         3.0889e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.5914e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 6.4131e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.0238e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 8.2319e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0196e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1977e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.2447e-02, 6.5089e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.2520e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         8.9834e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0740e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.0706e-02, 0.0000e+00, 3.0455e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3274e-02, 1.1232e-02, 0.0000e+00,\n",
       "         1.4087e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.7546e-03, 0.0000e+00, 0.0000e+00, 9.4139e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1325e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4702e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4816e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.7454e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7997e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.9028e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7464e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5716e-03,\n",
       "         3.3643e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1671e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1064e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 5.6183e-04, 1.3550e-02, 0.0000e+00, 0.0000e+00, 3.8535e-04,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4278e-03, 1.8293e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1624e-04, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6723e-03,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.9885e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.1757e-02, 0.0000e+00, 0.0000e+00, 1.0567e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9972e-02,\n",
       "         2.0771e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         7.5563e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0290e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.8256e-02, 0.0000e+00, 0.0000e+00, 3.4830e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7005e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         1.0610e-05, 6.7745e-03, 0.0000e+00, 0.0000e+00, 6.9631e-04, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0731e-02,\n",
       "         0.0000e+00, 0.0000e+00, 1.9994e-03, 0.0000e+00, 0.0000e+00, 6.0360e-04,\n",
       "         0.0000e+00, 0.0000e+00, 3.6243e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.3552e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0094e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0871e-03, 0.0000e+00,\n",
       "         1.7932e-02, 0.0000e+00, 0.0000e+00, 2.0815e-03, 0.0000e+00, 1.2085e-02,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9253e-04,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 8.5932e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8332e-03, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4497e-02, 4.5213e-04, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2479e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7131e-02, 0.0000e+00,\n",
       "         0.0000e+00, 2.0042e-02])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_item = data1.clone()\n",
    "gamma_feature_item = torch.zeros(num_channel)\n",
    "gamma_feature_item_record = []\n",
    "\n",
    "index = 0\n",
    "for k, m in enumerate(model.feature):\n",
    "    with torch.no_grad():\n",
    "        one_item = m(one_item)\n",
    "        \n",
    "    if isinstance(m, nn.ReLU):\n",
    "        value = one_item.clone().squeeze(0)\n",
    "        gamma = activation_based_gamma(value)\n",
    "        gamma_feature_item_record.append(gamma)\n",
    "        \n",
    "        size = value.shape[0]\n",
    "        gamma_feature_item[index:(index+size)] = gamma.clone()\n",
    "        index += size\n",
    "    \n",
    "gamma_feature_item_record[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3852, tensor(0.0011))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruning_rate = 0.7\n",
    "y, i = torch.sort(gamma_feature_item)\n",
    "thre_idx = int(num_channel * pruning_rate)\n",
    "thresold_feature_item = y[thre_index]\n",
    "\n",
    "thre_index, thresold_feature_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 2\t shape: [64, 32, 32] \t total channel: 64\t remaining channel: 37\n",
      "layer index: 5\t shape: [64, 32, 32] \t total channel: 64\t remaining channel: 63\n",
      "layer index: 9\t shape: [128, 16, 16] \t total channel: 128\t remaining channel: 126\n",
      "layer index: 12\t shape: [128, 16, 16] \t total channel: 128\t remaining channel: 128\n",
      "layer index: 16\t shape: [256, 8, 8] \t total channel: 256\t remaining channel: 243\n",
      "layer index: 19\t shape: [256, 8, 8] \t total channel: 256\t remaining channel: 229\n",
      "layer index: 22\t shape: [256, 8, 8] \t total channel: 256\t remaining channel: 197\n",
      "layer index: 25\t shape: [256, 8, 8] \t total channel: 256\t remaining channel: 149\n",
      "layer index: 29\t shape: [512, 4, 4] \t total channel: 512\t remaining channel: 112\n",
      "layer index: 32\t shape: [512, 4, 4] \t total channel: 512\t remaining channel: 64\n",
      "layer index: 35\t shape: [512, 4, 4] \t total channel: 512\t remaining channel: 68\n",
      "layer index: 38\t shape: [512, 4, 4] \t total channel: 512\t remaining channel: 79\n",
      "layer index: 42\t shape: [512, 2, 2] \t total channel: 512\t remaining channel: 43\n",
      "layer index: 45\t shape: [512, 2, 2] \t total channel: 512\t remaining channel: 27\n",
      "layer index: 48\t shape: [512, 2, 2] \t total channel: 512\t remaining channel: 48\n",
      "layer index: 51\t shape: [512, 2, 2] \t total channel: 512\t remaining channel: 38\n",
      "Pruned ratio: 0.700036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_item = data1.clone()\n",
    "num_pruned = 0  # 裁剪个数\n",
    "cfg_num = []\n",
    "\n",
    "for k, m in enumerate(model.feature):\n",
    "    with torch.no_grad():\n",
    "        one_item = m(one_item)\n",
    "    \n",
    "    if isinstance(m, nn.ReLU):\n",
    "        value = one_item.clone().squeeze(0)\n",
    "        gamma = activation_based_gamma(value)\n",
    "        mask = gamma.gt(thresold_feature_item).float()\n",
    "        \n",
    "        num_remain = int(torch.sum(mask))\n",
    "        num_pruned += mask.shape[0] - num_remain\n",
    "        \n",
    "        cfg_num.append(num_remain)\n",
    "        \n",
    "        print('layer index: {:d}\\t shape: {} \\t total channel: {:d}\\t remaining channel: {:d}'.\n",
    "                      format(k, list(value.shape), mask.shape[0], num_remain))\n",
    "\n",
    "pruned_ratio = num_pruned / num_channel\n",
    "print('Pruned ratio: {:4f}\\r\\n'.format(pruned_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2 one batch average gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0024, 0.0010, 0.0000, 0.0011, 0.0012, 0.0000, 0.0000, 0.0026,\n",
       "         0.0000, 0.0000, 0.0028, 0.0000, 0.0000, 0.0033, 0.0015, 0.0011, 0.0000,\n",
       "         0.0000, 0.0000, 0.0017, 0.0034, 0.0000, 0.0048, 0.0000, 0.0000, 0.0049,\n",
       "         0.0029, 0.0036, 0.0007, 0.0024, 0.0004, 0.0027, 0.0000, 0.0040, 0.0024,\n",
       "         0.0016, 0.0009, 0.0000, 0.0003, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "         0.0014, 0.0023, 0.0012, 0.0000, 0.0000, 0.0020, 0.0033, 0.0000, 0.0000,\n",
       "         0.0027, 0.0050, 0.0000, 0.0030, 0.0007, 0.0031, 0.0000, 0.0066, 0.0009,\n",
       "         0.0000, 0.0016, 0.0057, 0.0037, 0.0027, 0.0015, 0.0057, 0.0069, 0.0033,\n",
       "         0.0037, 0.0032, 0.0029, 0.0046, 0.0033, 0.0027, 0.0028, 0.0014, 0.0047,\n",
       "         0.0064, 0.0023, 0.0021, 0.0048, 0.0072, 0.0035, 0.0051, 0.0081, 0.0019,\n",
       "         0.0025, 0.0031, 0.0039, 0.0072, 0.0067, 0.0038, 0.0027, 0.0065, 0.0027,\n",
       "         0.0075]),\n",
       " [tensor([0.0000, 0.0024, 0.0010, 0.0000, 0.0011, 0.0012, 0.0000, 0.0000, 0.0026,\n",
       "          0.0000, 0.0000, 0.0028, 0.0000, 0.0000, 0.0033, 0.0015, 0.0011, 0.0000,\n",
       "          0.0000, 0.0000, 0.0017, 0.0034, 0.0000, 0.0048, 0.0000, 0.0000, 0.0049,\n",
       "          0.0029, 0.0036, 0.0007, 0.0024, 0.0004, 0.0027, 0.0000, 0.0040, 0.0024,\n",
       "          0.0016, 0.0009, 0.0000, 0.0003, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "          0.0014, 0.0023, 0.0012, 0.0000, 0.0000, 0.0020, 0.0033, 0.0000, 0.0000,\n",
       "          0.0027, 0.0050, 0.0000, 0.0030, 0.0007, 0.0031, 0.0000, 0.0066, 0.0009,\n",
       "          0.0000]),\n",
       "  tensor([0.0016, 0.0057, 0.0037, 0.0027, 0.0015, 0.0057, 0.0069, 0.0033, 0.0037,\n",
       "          0.0032, 0.0029, 0.0046, 0.0033, 0.0027, 0.0028, 0.0014, 0.0047, 0.0064,\n",
       "          0.0023, 0.0021, 0.0048, 0.0072, 0.0035, 0.0051, 0.0081, 0.0019, 0.0025,\n",
       "          0.0031, 0.0039, 0.0072, 0.0067, 0.0038, 0.0027, 0.0065, 0.0027, 0.0075,\n",
       "          0.0032, 0.0026, 0.0019, 0.0023, 0.0027, 0.0026, 0.0013, 0.0018, 0.0034,\n",
       "          0.0039, 0.0054, 0.0253, 0.0062, 0.0061, 0.0040, 0.0030, 0.0029, 0.0107,\n",
       "          0.0021, 0.0046, 0.0031, 0.0053, 0.0037, 0.0135, 0.0028, 0.0019, 0.0020,\n",
       "          0.0052]),\n",
       "  tensor([0.0010, 0.0017, 0.0012, 0.0020, 0.0021, 0.0009, 0.0023, 0.0016, 0.0014,\n",
       "          0.0016, 0.0019, 0.0014, 0.0019, 0.0033, 0.0019, 0.0009, 0.0008, 0.0012,\n",
       "          0.0034, 0.0029, 0.0026, 0.0017, 0.0010, 0.0014, 0.0030, 0.0025, 0.0042,\n",
       "          0.0021, 0.0011, 0.0019, 0.0029, 0.0014, 0.0024, 0.0018, 0.0010, 0.0034,\n",
       "          0.0025, 0.0013, 0.0018, 0.0024, 0.0017, 0.0017, 0.0029, 0.0029, 0.0020,\n",
       "          0.0016, 0.0015, 0.0017, 0.0017, 0.0022, 0.0016, 0.0095, 0.0021, 0.0012,\n",
       "          0.0029, 0.0025, 0.0074, 0.0102, 0.0014, 0.0022, 0.0013, 0.0023, 0.0003,\n",
       "          0.0021, 0.0020, 0.0028, 0.0008, 0.0033, 0.0019, 0.0017, 0.0024, 0.0035,\n",
       "          0.0011, 0.0035, 0.0013, 0.0019, 0.0035, 0.0021, 0.0023, 0.0043, 0.0022,\n",
       "          0.0033, 0.0020, 0.0017, 0.0071, 0.0015, 0.0018, 0.0046, 0.0020, 0.0014,\n",
       "          0.0025, 0.0016, 0.0021, 0.0034, 0.0022, 0.0025, 0.0012, 0.0025, 0.0024,\n",
       "          0.0014, 0.0017, 0.0017, 0.0033, 0.0022, 0.0025, 0.0014, 0.0011, 0.0001,\n",
       "          0.0016, 0.0014, 0.0025, 0.0032, 0.0011, 0.0016, 0.0016, 0.0030, 0.0019,\n",
       "          0.0016, 0.0025, 0.0016, 0.0008, 0.0020, 0.0028, 0.0016, 0.0041, 0.0022,\n",
       "          0.0015, 0.0018]),\n",
       "  tensor([0.0035, 0.0021, 0.0027, 0.0019, 0.0024, 0.0020, 0.0018, 0.0025, 0.0051,\n",
       "          0.0021, 0.0013, 0.0063, 0.0034, 0.0047, 0.0016, 0.0030, 0.0048, 0.0018,\n",
       "          0.0020, 0.0011, 0.0035, 0.0016, 0.0014, 0.0023, 0.0035, 0.0042, 0.0023,\n",
       "          0.0047, 0.0059, 0.0036, 0.0023, 0.0085, 0.0041, 0.0075, 0.0035, 0.0020,\n",
       "          0.0015, 0.0059, 0.0074, 0.0028, 0.0022, 0.0023, 0.0024, 0.0039, 0.0020,\n",
       "          0.0020, 0.0015, 0.0020, 0.0047, 0.0029, 0.0029, 0.0098, 0.0016, 0.0025,\n",
       "          0.0016, 0.0021, 0.0051, 0.0043, 0.0051, 0.0022, 0.0057, 0.0076, 0.0025,\n",
       "          0.0064, 0.0076, 0.0021, 0.0033, 0.0057, 0.0023, 0.0033, 0.0016, 0.0015,\n",
       "          0.0016, 0.0015, 0.0012, 0.0035, 0.0020, 0.0019, 0.0049, 0.0045, 0.0027,\n",
       "          0.0030, 0.0017, 0.0026, 0.0019, 0.0014, 0.0032, 0.0024, 0.0015, 0.0014,\n",
       "          0.0024, 0.0042, 0.0022, 0.0028, 0.0027, 0.0043, 0.0016, 0.0034, 0.0012,\n",
       "          0.0025, 0.0020, 0.0031, 0.0104, 0.0037, 0.0019, 0.0022, 0.0118, 0.0028,\n",
       "          0.0018, 0.0023, 0.0013, 0.0045, 0.0033, 0.0060, 0.0021, 0.0034, 0.0017,\n",
       "          0.0021, 0.0018, 0.0027, 0.0047, 0.0042, 0.0011, 0.0018, 0.0015, 0.0020,\n",
       "          0.0033, 0.0032]),\n",
       "  tensor([0.0006, 0.0011, 0.0013, 0.0009, 0.0013, 0.0019, 0.0013, 0.0019, 0.0019,\n",
       "          0.0011, 0.0010, 0.0007, 0.0016, 0.0017, 0.0006, 0.0016, 0.0021, 0.0013,\n",
       "          0.0028, 0.0013, 0.0008, 0.0016, 0.0019, 0.0010, 0.0010, 0.0008, 0.0008,\n",
       "          0.0014, 0.0018, 0.0008, 0.0018, 0.0036, 0.0012, 0.0013, 0.0011, 0.0009,\n",
       "          0.0015, 0.0019, 0.0018, 0.0016, 0.0012, 0.0013, 0.0012, 0.0013, 0.0007,\n",
       "          0.0013, 0.0013, 0.0027, 0.0008, 0.0038, 0.0009, 0.0000, 0.0019, 0.0013,\n",
       "          0.0013, 0.0016, 0.0017, 0.0026, 0.0009, 0.0011, 0.0008, 0.0018, 0.0004,\n",
       "          0.0025, 0.0014, 0.0007, 0.0011, 0.0025, 0.0007, 0.0021, 0.0011, 0.0017,\n",
       "          0.0016, 0.0006, 0.0013, 0.0015, 0.0010, 0.0017, 0.0017, 0.0010, 0.0017,\n",
       "          0.0011, 0.0006, 0.0012, 0.0020, 0.0023, 0.0018, 0.0010, 0.0014, 0.0023,\n",
       "          0.0015, 0.0024, 0.0021, 0.0020, 0.0008, 0.0026, 0.0010, 0.0010, 0.0011,\n",
       "          0.0013, 0.0008, 0.0021, 0.0015, 0.0016, 0.0008, 0.0013, 0.0011, 0.0011,\n",
       "          0.0012, 0.0011, 0.0020, 0.0018, 0.0014, 0.0013, 0.0016, 0.0017, 0.0014,\n",
       "          0.0014, 0.0011, 0.0010, 0.0013, 0.0008, 0.0020, 0.0020, 0.0017, 0.0021,\n",
       "          0.0029, 0.0008, 0.0024, 0.0021, 0.0018, 0.0012, 0.0015, 0.0013, 0.0023,\n",
       "          0.0018, 0.0011, 0.0009, 0.0011, 0.0043, 0.0013, 0.0011, 0.0016, 0.0026,\n",
       "          0.0011, 0.0021, 0.0011, 0.0010, 0.0008, 0.0017, 0.0013, 0.0013, 0.0012,\n",
       "          0.0015, 0.0011, 0.0022, 0.0015, 0.0019, 0.0009, 0.0012, 0.0025, 0.0014,\n",
       "          0.0010, 0.0024, 0.0016, 0.0023, 0.0010, 0.0015, 0.0011, 0.0018, 0.0010,\n",
       "          0.0018, 0.0022, 0.0008, 0.0008, 0.0017, 0.0009, 0.0009, 0.0012, 0.0009,\n",
       "          0.0035, 0.0017, 0.0013, 0.0014, 0.0028, 0.0012, 0.0011, 0.0007, 0.0007,\n",
       "          0.0009, 0.0013, 0.0009, 0.0014, 0.0007, 0.0009, 0.0010, 0.0007, 0.0010,\n",
       "          0.0013, 0.0020, 0.0033, 0.0012, 0.0024, 0.0018, 0.0015, 0.0035, 0.0031,\n",
       "          0.0015, 0.0011, 0.0018, 0.0012, 0.0011, 0.0013, 0.0011, 0.0017, 0.0012,\n",
       "          0.0013, 0.0011, 0.0010, 0.0024, 0.0028, 0.0013, 0.0020, 0.0012, 0.0028,\n",
       "          0.0022, 0.0017, 0.0015, 0.0025, 0.0020, 0.0011, 0.0009, 0.0011, 0.0009,\n",
       "          0.0010, 0.0018, 0.0026, 0.0015, 0.0011, 0.0012, 0.0013, 0.0010, 0.0018,\n",
       "          0.0010, 0.0017, 0.0017, 0.0010, 0.0010, 0.0008, 0.0012, 0.0014, 0.0017,\n",
       "          0.0024, 0.0000, 0.0050, 0.0022]),\n",
       "  tensor([0.0035, 0.0048, 0.0022, 0.0023, 0.0012, 0.0030, 0.0019, 0.0012, 0.0008,\n",
       "          0.0009, 0.0032, 0.0020, 0.0023, 0.0019, 0.0010, 0.0012, 0.0009, 0.0018,\n",
       "          0.0017, 0.0009, 0.0046, 0.0012, 0.0035, 0.0014, 0.0020, 0.0017, 0.0013,\n",
       "          0.0035, 0.0022, 0.0028, 0.0014, 0.0014, 0.0018, 0.0019, 0.0058, 0.0029,\n",
       "          0.0047, 0.0026, 0.0010, 0.0020, 0.0023, 0.0012, 0.0014, 0.0027, 0.0022,\n",
       "          0.0010, 0.0014, 0.0017, 0.0015, 0.0000, 0.0015, 0.0016, 0.0015, 0.0012,\n",
       "          0.0012, 0.0016, 0.0029, 0.0017, 0.0013, 0.0014, 0.0013, 0.0008, 0.0020,\n",
       "          0.0013, 0.0019, 0.0020, 0.0026, 0.0016, 0.0009, 0.0017, 0.0015, 0.0013,\n",
       "          0.0016, 0.0010, 0.0009, 0.0014, 0.0013, 0.0034, 0.0019, 0.0013, 0.0010,\n",
       "          0.0030, 0.0018, 0.0017, 0.0027, 0.0044, 0.0013, 0.0021, 0.0017, 0.0008,\n",
       "          0.0029, 0.0013, 0.0023, 0.0014, 0.0021, 0.0014, 0.0018, 0.0015, 0.0015,\n",
       "          0.0018, 0.0013, 0.0013, 0.0008, 0.0013, 0.0028, 0.0014, 0.0012, 0.0012,\n",
       "          0.0006, 0.0028, 0.0015, 0.0022, 0.0019, 0.0019, 0.0016, 0.0021, 0.0025,\n",
       "          0.0020, 0.0016, 0.0022, 0.0008, 0.0000, 0.0020, 0.0015, 0.0011, 0.0006,\n",
       "          0.0009, 0.0026, 0.0015, 0.0012, 0.0012, 0.0013, 0.0011, 0.0009, 0.0011,\n",
       "          0.0022, 0.0018, 0.0027, 0.0011, 0.0007, 0.0015, 0.0015, 0.0026, 0.0025,\n",
       "          0.0016, 0.0023, 0.0016, 0.0027, 0.0020, 0.0033, 0.0018, 0.0011, 0.0012,\n",
       "          0.0018, 0.0015, 0.0020, 0.0029, 0.0011, 0.0013, 0.0011, 0.0022, 0.0007,\n",
       "          0.0017, 0.0020, 0.0007, 0.0011, 0.0018, 0.0032, 0.0020, 0.0015, 0.0017,\n",
       "          0.0032, 0.0016, 0.0012, 0.0000, 0.0014, 0.0010, 0.0011, 0.0010, 0.0016,\n",
       "          0.0024, 0.0022, 0.0013, 0.0016, 0.0027, 0.0024, 0.0016, 0.0021, 0.0023,\n",
       "          0.0016, 0.0020, 0.0026, 0.0014, 0.0000, 0.0014, 0.0020, 0.0008, 0.0016,\n",
       "          0.0025, 0.0020, 0.0007, 0.0014, 0.0019, 0.0014, 0.0018, 0.0015, 0.0039,\n",
       "          0.0029, 0.0013, 0.0014, 0.0013, 0.0021, 0.0012, 0.0013, 0.0011, 0.0010,\n",
       "          0.0018, 0.0015, 0.0014, 0.0016, 0.0015, 0.0026, 0.0021, 0.0025, 0.0013,\n",
       "          0.0030, 0.0027, 0.0031, 0.0038, 0.0006, 0.0021, 0.0012, 0.0024, 0.0015,\n",
       "          0.0016, 0.0007, 0.0005, 0.0007, 0.0018, 0.0015, 0.0076, 0.0000, 0.0000,\n",
       "          0.0017, 0.0049, 0.0006, 0.0010, 0.0052, 0.0012, 0.0009, 0.0043, 0.0026,\n",
       "          0.0019, 0.0022, 0.0009, 0.0023]),\n",
       "  tensor([0.0029, 0.0013, 0.0021, 0.0011, 0.0000, 0.0002, 0.0009, 0.0000, 0.0054,\n",
       "          0.0022, 0.0013, 0.0018, 0.0029, 0.0028, 0.0010, 0.0019, 0.0014, 0.0024,\n",
       "          0.0003, 0.0009, 0.0013, 0.0000, 0.0014, 0.0000, 0.0005, 0.0015, 0.0019,\n",
       "          0.0011, 0.0000, 0.0012, 0.0000, 0.0010, 0.0015, 0.0020, 0.0025, 0.0015,\n",
       "          0.0000, 0.0000, 0.0026, 0.0014, 0.0011, 0.0017, 0.0011, 0.0050, 0.0011,\n",
       "          0.0013, 0.0023, 0.0019, 0.0037, 0.0018, 0.0020, 0.0014, 0.0025, 0.0010,\n",
       "          0.0021, 0.0016, 0.0021, 0.0023, 0.0052, 0.0016, 0.0005, 0.0000, 0.0018,\n",
       "          0.0037, 0.0018, 0.0019, 0.0014, 0.0017, 0.0063, 0.0016, 0.0017, 0.0017,\n",
       "          0.0023, 0.0024, 0.0017, 0.0087, 0.0010, 0.0019, 0.0012, 0.0020, 0.0006,\n",
       "          0.0042, 0.0022, 0.0025, 0.0022, 0.0015, 0.0016, 0.0019, 0.0013, 0.0016,\n",
       "          0.0008, 0.0000, 0.0031, 0.0031, 0.0021, 0.0000, 0.0005, 0.0000, 0.0000,\n",
       "          0.0049, 0.0022, 0.0015, 0.0008, 0.0016, 0.0000, 0.0010, 0.0008, 0.0025,\n",
       "          0.0013, 0.0030, 0.0018, 0.0013, 0.0008, 0.0011, 0.0016, 0.0012, 0.0011,\n",
       "          0.0018, 0.0029, 0.0031, 0.0015, 0.0047, 0.0014, 0.0000, 0.0014, 0.0022,\n",
       "          0.0015, 0.0000, 0.0049, 0.0014, 0.0014, 0.0025, 0.0014, 0.0004, 0.0017,\n",
       "          0.0027, 0.0015, 0.0008, 0.0013, 0.0017, 0.0028, 0.0006, 0.0008, 0.0011,\n",
       "          0.0011, 0.0022, 0.0021, 0.0023, 0.0026, 0.0007, 0.0025, 0.0000, 0.0021,\n",
       "          0.0017, 0.0015, 0.0009, 0.0022, 0.0023, 0.0032, 0.0011, 0.0021, 0.0013,\n",
       "          0.0000, 0.0000, 0.0024, 0.0029, 0.0000, 0.0010, 0.0014, 0.0000, 0.0006,\n",
       "          0.0012, 0.0014, 0.0058, 0.0016, 0.0020, 0.0000, 0.0000, 0.0012, 0.0023,\n",
       "          0.0072, 0.0008, 0.0019, 0.0072, 0.0044, 0.0019, 0.0011, 0.0012, 0.0000,\n",
       "          0.0015, 0.0010, 0.0020, 0.0000, 0.0022, 0.0027, 0.0042, 0.0016, 0.0000,\n",
       "          0.0025, 0.0000, 0.0010, 0.0020, 0.0018, 0.0032, 0.0013, 0.0000, 0.0017,\n",
       "          0.0023, 0.0010, 0.0000, 0.0036, 0.0029, 0.0031, 0.0064, 0.0010, 0.0017,\n",
       "          0.0044, 0.0039, 0.0026, 0.0021, 0.0009, 0.0000, 0.0025, 0.0008, 0.0027,\n",
       "          0.0055, 0.0023, 0.0016, 0.0013, 0.0019, 0.0018, 0.0004, 0.0031, 0.0032,\n",
       "          0.0000, 0.0000, 0.0024, 0.0012, 0.0011, 0.0000, 0.0015, 0.0012, 0.0016,\n",
       "          0.0012, 0.0024, 0.0042, 0.0014, 0.0038, 0.0026, 0.0035, 0.0014, 0.0030,\n",
       "          0.0016, 0.0011, 0.0003, 0.0011]),\n",
       "  tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6674e-03, 2.0561e-03, 2.0817e-03,\n",
       "          2.8159e-03, 1.7325e-03, 3.8913e-03, 1.8469e-03, 2.6672e-03, 0.0000e+00,\n",
       "          3.2095e-03, 2.9651e-03, 1.9974e-03, 2.7430e-03, 4.3277e-03, 2.5483e-03,\n",
       "          2.2679e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7593e-03, 4.0651e-03,\n",
       "          2.4185e-03, 1.7788e-03, 4.0801e-03, 2.3274e-03, 2.9689e-03, 0.0000e+00,\n",
       "          9.7365e-04, 1.4266e-03, 2.8878e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.1452e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7105e-03, 2.2963e-03,\n",
       "          8.1694e-04, 0.0000e+00, 1.5345e-03, 3.3984e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 3.3277e-03, 4.6338e-03, 3.8775e-03, 4.0487e-03, 0.0000e+00,\n",
       "          0.0000e+00, 5.3741e-03, 9.6397e-04, 0.0000e+00, 1.7758e-03, 5.0453e-03,\n",
       "          0.0000e+00, 5.2938e-03, 0.0000e+00, 2.5087e-03, 3.2860e-03, 2.5035e-03,\n",
       "          1.0361e-02, 0.0000e+00, 1.9070e-03, 2.2893e-03, 4.5225e-03, 4.6147e-03,\n",
       "          2.4806e-03, 2.1776e-03, 3.1999e-03, 0.0000e+00, 0.0000e+00, 2.1290e-03,\n",
       "          5.5087e-03, 2.0679e-03, 3.2856e-03, 0.0000e+00, 0.0000e+00, 1.4761e-03,\n",
       "          7.1561e-03, 7.9252e-04, 0.0000e+00, 1.9465e-03, 6.7752e-04, 0.0000e+00,\n",
       "          3.3178e-03, 0.0000e+00, 3.8294e-03, 1.6249e-03, 6.2948e-03, 4.5046e-03,\n",
       "          6.4503e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9080e-03, 0.0000e+00,\n",
       "          3.9830e-03, 1.6405e-03, 3.6599e-03, 4.5595e-03, 0.0000e+00, 1.9124e-03,\n",
       "          9.5897e-03, 1.7782e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0026e-03,\n",
       "          1.3572e-03, 2.0629e-03, 3.3013e-03, 2.6164e-03, 0.0000e+00, 2.1781e-03,\n",
       "          0.0000e+00, 4.9111e-05, 3.2472e-03, 2.2851e-04, 1.5952e-03, 1.3609e-03,\n",
       "          2.5891e-03, 2.1569e-03, 3.2370e-03, 3.0394e-03, 0.0000e+00, 1.9013e-03,\n",
       "          3.5731e-03, 3.1352e-04, 3.4979e-03, 1.2136e-03, 3.0827e-03, 3.3750e-03,\n",
       "          0.0000e+00, 3.2377e-03, 1.9535e-03, 4.2645e-03, 0.0000e+00, 1.4873e-03,\n",
       "          2.8798e-03, 0.0000e+00, 2.5274e-03, 1.3144e-03, 0.0000e+00, 1.5338e-03,\n",
       "          0.0000e+00, 3.6235e-03, 0.0000e+00, 4.9837e-03, 0.0000e+00, 3.0415e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0989e-03, 0.0000e+00, 2.5852e-03,\n",
       "          1.4857e-03, 5.0276e-03, 0.0000e+00, 0.0000e+00, 9.1300e-03, 2.8274e-03,\n",
       "          3.6323e-03, 7.9160e-04, 1.3785e-03, 8.0643e-04, 5.0576e-03, 2.9600e-04,\n",
       "          3.4762e-03, 3.6608e-03, 0.0000e+00, 4.0273e-03, 4.0263e-03, 4.5346e-03,\n",
       "          0.0000e+00, 1.1878e-03, 1.6398e-03, 3.0726e-03, 4.6348e-03, 0.0000e+00,\n",
       "          4.8028e-03, 2.6524e-03, 2.0047e-03, 0.0000e+00, 2.8960e-03, 2.5387e-03,\n",
       "          0.0000e+00, 3.3968e-03, 0.0000e+00, 3.8294e-03, 2.0652e-03, 3.7343e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.0359e-02, 2.8115e-03, 4.2584e-03, 2.4833e-03,\n",
       "          2.6636e-03, 0.0000e+00, 0.0000e+00, 2.0457e-03, 1.5964e-03, 0.0000e+00,\n",
       "          1.4553e-03, 0.0000e+00, 3.6934e-03, 2.5801e-03, 3.3539e-03, 7.7582e-03,\n",
       "          0.0000e+00, 5.0913e-03, 0.0000e+00, 1.6758e-03, 2.2584e-03, 3.2742e-03,\n",
       "          7.2056e-04, 0.0000e+00, 1.5549e-03, 3.4866e-03, 3.5471e-03, 7.3036e-03,\n",
       "          0.0000e+00, 1.1585e-03, 3.5060e-03, 3.7069e-03, 3.4351e-03, 0.0000e+00,\n",
       "          0.0000e+00, 3.1184e-03, 4.4497e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.9304e-03, 2.0674e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6518e-03,\n",
       "          7.7206e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7192e-03, 1.9785e-03,\n",
       "          4.8829e-03, 3.9176e-03, 1.0240e-03, 1.4215e-03]),\n",
       "  tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3569e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.9447e-03, 0.0000e+00, 4.3898e-03, 0.0000e+00, 3.3843e-03,\n",
       "          0.0000e+00, 0.0000e+00, 9.1528e-03, 0.0000e+00, 2.1167e-03, 6.0629e-03,\n",
       "          6.3345e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9210e-03, 0.0000e+00,\n",
       "          0.0000e+00, 9.6748e-03, 0.0000e+00, 5.0494e-03, 0.0000e+00, 3.4806e-03,\n",
       "          5.5120e-03, 0.0000e+00, 2.5819e-03, 0.0000e+00, 0.0000e+00, 1.5731e-03,\n",
       "          0.0000e+00, 0.0000e+00, 4.4869e-03, 2.6360e-03, 2.5835e-03, 0.0000e+00,\n",
       "          4.3112e-05, 0.0000e+00, 4.4584e-03, 2.1335e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2273e-03,\n",
       "          0.0000e+00, 5.4724e-03, 2.0113e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.0832e-03, 7.3249e-04, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.7372e-03, 0.0000e+00, 2.5061e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2560e-03, 0.0000e+00,\n",
       "          2.8249e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.0148e-03, 3.4342e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.3672e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.5583e-03, 3.9451e-04, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5163e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.3797e-03, 0.0000e+00, 2.7144e-03, 0.0000e+00, 0.0000e+00, 4.9971e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2152e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1858e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.6547e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5162e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0032e-03, 2.0029e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          5.0554e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6247e-03,\n",
       "          0.0000e+00, 1.1931e-05, 1.8763e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.0176e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1964e-03, 2.1329e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.6480e-03, 3.3174e-03, 1.2071e-03, 0.0000e+00,\n",
       "          0.0000e+00, 5.5243e-03, 0.0000e+00, 0.0000e+00, 1.8217e-03, 0.0000e+00,\n",
       "          0.0000e+00, 4.8282e-03, 0.0000e+00, 0.0000e+00, 1.0445e-03, 0.0000e+00,\n",
       "          5.0016e-03, 0.0000e+00, 5.1743e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 8.2607e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5855e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.9447e-04,\n",
       "          0.0000e+00, 0.0000e+00, 2.3225e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          4.4606e-03, 0.0000e+00, 6.6365e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.7791e-03, 1.6663e-03, 0.0000e+00, 6.6308e-03, 1.5278e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 3.2424e-03, 2.4821e-03, 0.0000e+00, 0.0000e+00,\n",
       "          5.1379e-03, 3.1636e-03, 4.7414e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.8128e-03, 0.0000e+00, 0.0000e+00, 6.1605e-03,\n",
       "          0.0000e+00, 6.0012e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7362e-03,\n",
       "          0.0000e+00, 3.7890e-03, 2.3589e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6644e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.0521e-03, 0.0000e+00, 0.0000e+00, 1.7058e-03,\n",
       "          5.4224e-03, 3.0767e-03, 2.1525e-03, 0.0000e+00, 1.0384e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7693e-04,\n",
       "          0.0000e+00, 0.0000e+00, 3.6826e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7908e-03,\n",
       "          0.0000e+00, 3.9352e-03, 1.6338e-03, 7.2146e-03, 0.0000e+00, 5.2075e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5037e-03, 0.0000e+00,\n",
       "          0.0000e+00, 1.1229e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2294e-03,\n",
       "          0.0000e+00, 1.0062e-02, 1.0885e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3034e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 3.7577e-03, 0.0000e+00, 2.5389e-03, 0.0000e+00, 2.0238e-03,\n",
       "          0.0000e+00, 2.2205e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 7.2629e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8624e-03,\n",
       "          2.0246e-03, 0.0000e+00, 6.0138e-03, 0.0000e+00, 1.4964e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9304e-03,\n",
       "          0.0000e+00, 0.0000e+00, 8.8629e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 3.8807e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6609e-03,\n",
       "          0.0000e+00, 2.8843e-03, 2.2135e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1994e-03,\n",
       "          1.0630e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1973e-03,\n",
       "          0.0000e+00, 1.1094e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1034e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3352e-03, 0.0000e+00, 0.0000e+00,\n",
       "          2.5578e-03, 6.5678e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.6976e-03, 2.2762e-03, 9.0679e-04, 0.0000e+00,\n",
       "          0.0000e+00, 3.1326e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.7404e-03, 0.0000e+00, 3.2952e-03, 0.0000e+00, 5.6340e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.8871e-03, 0.0000e+00, 6.8703e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0756e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]),\n",
       "  tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.0818e-03, 0.0000e+00, 0.0000e+00, 1.5181e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 5.3763e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.1707e-03, 0.0000e+00, 1.6227e-03, 0.0000e+00,\n",
       "          8.2514e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1336e-03, 0.0000e+00, 1.6473e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0793e-03, 0.0000e+00, 0.0000e+00,\n",
       "          3.2124e-03, 2.9460e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.6579e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.8840e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 1.5809e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 8.9337e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 7.1041e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1486e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8815e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 2.4694e-03, 1.8109e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9653e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          7.8792e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3348e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 8.7251e-03, 0.0000e+00, 2.7476e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0267e-03, 2.2482e-03, 0.0000e+00,\n",
       "          1.2676e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.2008e-03, 0.0000e+00, 0.0000e+00, 2.7833e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6727e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0482e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1654e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.5448e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8700e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 6.8022e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1497e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1161e-03,\n",
       "          6.3555e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0063e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9886e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 4.7107e-04, 2.2987e-03, 0.0000e+00, 0.0000e+00, 3.6125e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2801e-03, 1.9819e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.6131e-04, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1829e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.9880e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.7827e-02, 0.0000e+00, 0.0000e+00, 2.0940e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1662e-03,\n",
       "          7.7237e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.3607e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1741e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.7802e-03, 0.0000e+00, 0.0000e+00, 4.5057e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1760e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          9.2489e-06, 1.6533e-03, 0.0000e+00, 0.0000e+00, 6.6513e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.2192e-03,\n",
       "          0.0000e+00, 0.0000e+00, 1.7610e-03, 0.0000e+00, 0.0000e+00, 5.4248e-04,\n",
       "          0.0000e+00, 0.0000e+00, 7.0760e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 4.4704e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 4.5656e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4635e-04, 0.0000e+00,\n",
       "          7.3150e-03, 0.0000e+00, 0.0000e+00, 1.9347e-03, 0.0000e+00, 3.2828e-03,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.8584e-04,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 1.2657e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8517e-03, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2481e-03, 3.9588e-04, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4151e-03, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7195e-03, 0.0000e+00,\n",
       "          0.0000e+00, 4.7400e-03])])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch = data_batch.clone()\n",
    "gamma_feature_batch = torch.zeros(num_channel)\n",
    "gamma_feature_batch_record = []\n",
    "\n",
    "index = 0\n",
    "for k, m in enumerate(model.feature):\n",
    "    with torch.no_grad():\n",
    "        one_batch = m(one_batch)\n",
    "\n",
    "    if isinstance(m, nn.ReLU):\n",
    "        value = one_batch.clone()\n",
    "        gamma = activation_based_gamma_batch(value)\n",
    "        gamma_feature_batch_record.append(gamma)\n",
    "        \n",
    "        size = value.shape[1]\n",
    "        gamma_feature_batch[index:(index+size)] = gamma.clone()\n",
    "        index += size\n",
    "\n",
    "gamma_feature_batch[:100], gamma_feature_batch_record[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3852, tensor(0.0008))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prunning_rate = 0.7\n",
    "y, i = torch.sort(gamma_feature_batch)\n",
    "thre_idx = int(num_channel * pruning_rate)\n",
    "threshold_feature_batch = y[thre_idx]\n",
    "\n",
    "thre_idx, threshold_feature_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index: 2 shape: [256, 64, 32, 32] total channel: 64 \t remaining channel: 33\n",
      "layer index: 5 shape: [256, 64, 32, 32] total channel: 64 \t remaining channel: 64\n",
      "layer index: 9 shape: [256, 128, 16, 16] total channel: 128 \t remaining channel: 124\n",
      "layer index: 12 shape: [256, 128, 16, 16] total channel: 128 \t remaining channel: 128\n",
      "layer index: 16 shape: [256, 256, 8, 8] total channel: 256 \t remaining channel: 236\n",
      "layer index: 19 shape: [256, 256, 8, 8] total channel: 256 \t remaining channel: 237\n",
      "layer index: 22 shape: [256, 256, 8, 8] total channel: 256 \t remaining channel: 205\n",
      "layer index: 25 shape: [256, 256, 8, 8] total channel: 256 \t remaining channel: 163\n",
      "layer index: 29 shape: [256, 512, 4, 4] total channel: 512 \t remaining channel: 125\n",
      "layer index: 32 shape: [256, 512, 4, 4] total channel: 512 \t remaining channel: 64\n",
      "layer index: 35 shape: [256, 512, 4, 4] total channel: 512 \t remaining channel: 65\n",
      "layer index: 38 shape: [256, 512, 4, 4] total channel: 512 \t remaining channel: 79\n",
      "layer index: 42 shape: [256, 512, 2, 2] total channel: 512 \t remaining channel: 19\n",
      "layer index: 45 shape: [256, 512, 2, 2] total channel: 512 \t remaining channel: 21\n",
      "layer index: 48 shape: [256, 512, 2, 2] total channel: 512 \t remaining channel: 40\n",
      "layer index: 51 shape: [256, 512, 2, 2] total channel: 512 \t remaining channel: 48\n",
      "Pruned ratio: 0.700036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_batch = data_batch.clone()\n",
    "num_pruned = 0  # 裁剪个数\n",
    "cfg_num = []\n",
    "\n",
    "for k, m in enumerate(model.feature):\n",
    "    with torch.no_grad():\n",
    "        one_batch = m(one_batch)\n",
    "    \n",
    "    if isinstance(m, nn.ReLU):\n",
    "        value = one_batch.clone()\n",
    "        gamma = activation_based_gamma_batch(value)\n",
    "        mask = gamma.gt(threshold_feature_batch).float()\n",
    "        \n",
    "        num_remain = int(torch.sum(mask))\n",
    "        num_pruned += mask.shape[0] - num_remain\n",
    "        \n",
    "        cfg_num.append(num_remain)\n",
    "        \n",
    "        print('layer index: {:d} shape: {} total channel: {:d} \\t remaining channel: {:d}'.\n",
    "              format(k, list(value.shape), mask.shape[0], num_remain))\n",
    "\n",
    "pruned_ratio = num_pruned / num_channel\n",
    "print('Pruned ratio: {:4f}\\r\\n'.format(pruned_ratio))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python37] *",
   "language": "python",
   "name": "conda-env-python37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
